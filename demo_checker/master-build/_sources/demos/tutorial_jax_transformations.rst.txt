
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "demos/tutorial_jax_transformations.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_demos_tutorial_jax_transformations.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_demos_tutorial_jax_transformations.py:


Using JAX with PennyLane
========================

.. meta::
    :property="og:description": Learn how to use JAX with PennyLane.
    :property="og:image": https://pennylane.ai/qml/_images/jax.png

.. related::

    tutorial_qubit_rotation Basic tutorial: qubit rotation
    tutorial_vqe A brief overview of VQE
    tutorial_vqt Variational Quantum Thermalizer

*Author: Chase Roberts â€” Posted: 12 April 2021. Last updated: 12 April 2021.*

JAX is an incredibly powerful scientific computing library that has been gaining traction in
both the physics and deep learning communities. While JAX was originally designed for 
classical machine learning (ML), many of its transformations are also useful 
for quantum machine learning (QML), and can be used directly with PennyLane.

.. GENERATED FROM PYTHON SOURCE LINES 23-44

.. figure:: ../demonstrations/jax_logo/jax.png
    :width: 50%
    :align: center

In this tutorial, we'll go over a number of JAX transformations and show how you can
use them to build and optimize quantum circuits. We'll show examples of how to 
do gradient descent with ``jax.grad``, run quantum circuits in parallel
using ``jax.vmap``, compile and optimize simulations with ``jax.jit``,
and control and seed the random nature of quantum computer simulations
with ``jax.random``. By the end of this tutorial you should feel just as comfortable
transforming quantum computing programs with JAX as you do transforming your 
neural networks.

If this is your first time reading PennyLane code, we recommend going through
the :doc:`basic tutorial </demos/tutorial_qubit_rotation>`
first. It's all in vanilla NumPy, so you should be able to 
easily transfer what you learn to JAX when you come back.

With that said, we begin by importing PennyLane, JAX,  the JAX-provided version of NumPy and
set up a two-qubit device for computations. We'll be using the ``default.qubit`` device
for the first part of this tutorial.

.. GENERATED FROM PYTHON SOURCE LINES 44-55

.. code-block:: default


    # Added to silence some warnings.
















.. GENERATED FROM PYTHON SOURCE LINES 56-58

Let's start with a simple example circuit that generates a two-qubit entangled state,
then evaluates the expectation value of the Pauli-Z operator on the first wire.

.. GENERATED FROM PYTHON SOURCE LINES 71-72

We can now execute the circuit just like any other python function.

.. GENERATED FROM PYTHON SOURCE LINES 75-81

Notice that the output of the circuit is a JAX ``DeviceArray``.
In fact, when we use the ``default.qubit`` device, the entire computation 
is done in JAX, so we can use all of the JAX tools out of the box!

Now let's move on to an example of a transformation. The code we wrote above is entirely 
differentiable, so let's calculate its gradient with ``jax.grad``.

.. GENERATED FROM PYTHON SOURCE LINES 81-103

.. code-block:: default




    # We use jax.grad here to transform our circuit method into one
    # that calcuates the gradient of the output relative to the input.




    # We can then use this grad_circuit function to optimize the parameter value
    # via gradient descent.


















.. GENERATED FROM PYTHON SOURCE LINES 104-113

And that's QML in a nutshell! If you've done classical machine learning before,
the above training loop should feel very familiar to you. The only difference is
that we used a quantum computer (or rather, a simulation of one) as part of our
model and cost calculation. In the end, almost all QML problems involve tuning some
parameters and minimizing some cost function, just like classical ML.
While classical ML focuses on learning classical systems like language or vision,
QML is most useful for learning about quantum systems. For example, 
:doc:`finding chemical ground states </demos/tutorial_vqe>` 
or learning to :doc:`sample thermal energy states </demos/tutorial_vqt>`.

.. GENERATED FROM PYTHON SOURCE LINES 116-130

Batching and Evolutionary Strategies
-------------------------------------

.. figure:: ../demonstrations/jax_logo/jaxvmap.png
    :width: 50%
    :align: center

We just showed how we can use gradient methods to learn a parameter value, 
but on real quantum computing hardware, calculating gradients can be really expensive and noisy.
Another approach is to use `evolutionary strategies <https://arxiv.org/abs/2012.00101>`__
(ES) to learn these parameters.
Here, we will be using the ``jax.vmap`` `transform <https://jax.readthedocs.io/en/latest/jax.html#jax.vmap>`__
to make running batches of circuits much easier. ``vmap`` essentially transforms a single quantum computer into
multiple running in parallel!

.. GENERATED FROM PYTHON SOURCE LINES 130-145

.. code-block:: default





    # Create a vectorized version of our original circuit.


    # Now, we call the ``vcircuit`` with multiple parameters at once and get back a
    # batch of expectations.
    # This examples runs 3 quantum circuits in parallel.












.. GENERATED FROM PYTHON SOURCE LINES 146-150

Let's now set up our ES training loop. The idea is pretty simple. First, we
calculate the expected values of each of our parameters. The cost values
then determine the "weight" of that example. The lower the cost, the larger the weight.
These batches are then used to generate a new set of parameters. 

.. GENERATED FROM PYTHON SOURCE LINES 150-182

.. code-block:: default


    # Needed to do randomness with JAX.
    # For more info on how JAX handles randomness, see the documentation.
    # https://jax.readthedocs.io/en/latest/jax.random.html


    # Generate our first set of samples.
































.. GENERATED FROM PYTHON SOURCE LINES 183-200

How to use jax.jit: Compiling Circuit Execution
-----------------------------------------------
.. figure:: ../demonstrations/jax_logo/jaxjit.png
    :width: 50%
    :align: center

JAX is built on top of `XLA <https://www.tensorflow.org/xla>`__, a powerful 
numerics library that can optimize and cross compile computations to different hardware, 
including CPUs, GPUs, etc. JAX can compile its computation to XLA via the ``jax.jit`` 
`transform. <https://jax.readthedocs.io/en/latest/jax.html?highlight=jit#jax.jit>`__

When compiling an XLA program, the compiler will do several rounds of optimization
passes to enhance the performance of the computation. Because of this compilation overhead,
you'll generally find the first time calling the function to be slow, but all subsequent
calls are much, much faster. You'll likely want to do it if you're running
the same circuit over and over but with different parameters, like you would find in almost
all variational quantum algorithms.

.. GENERATED FROM PYTHON SOURCE LINES 200-244

.. code-block:: default












    # Compiling your circuit with JAX is very easy, just add jax.jit!




    # No jit.

    # JAX runs async, so .block_until_ready() blocks until the computation
    # is actually finished. You'll only need to use this if you're doing benchmarking.



    # First call with jit.




    # Second call with jit.






    # Compilation overhead will make the first call slower than without jit...

    # ... but the second run time is >100x faster than the first!



    # You can see that for the cost of some compilation overhead, we can
    # greatly increase our performance of our simulation by orders of magnitude. 








.. GENERATED FROM PYTHON SOURCE LINES 245-263

Shots and Sampling with JAX
----------------------------

JAX was designed to enable experiments to be as repeatable as possible. Because of this,
JAX requires us to seed all randomly generated values (as you saw in the above
batching example). Sadly, the universe doesn't allow us to seed real quantum computers,
so if we want our JAX to mimic a real device, we'll have to handle randomness ourselves.

To learn more about how JAX handles randomness, visit their
`documentation site. <https://jax.readthedocs.io/en/latest/jax.random.html>`__

.. note:: 
    This example only applies if you are using ``jax.jit``. Otherwise, PennyLane 
    automatically seeds and resets the random-number-generator for you on each call.

To set the random number generating key, you'll have to pass the ``jax.random.PRNGKey``
when constructing the device. Because of this, if you want to use ``jax.jit`` with randomness,
the device construction will have to happen within that jitted method.

.. GENERATED FROM PYTHON SOURCE LINES 263-292

.. code-block:: default





    # Let's create our circuit with randomness and compile it with jax.jit.

















    # Notice that the first two runs return exactly the same results,



    # The second run has different results.









.. GENERATED FROM PYTHON SOURCE LINES 293-305

Closing Remarks
----------------
By now, using JAX with PennyLane should feel very natural. They 
complement each other very nicely; JAX with its powerful transforms, and PennyLane 
with its easy access to quantum computers. We're still in early days of 
development, but we hope to continue to grow our ecosystem around JAX,
and by extension, grow JAX into quantum computing and quantum machine learning.
The future looks bright for this field, and we're excited to see what you build!


About the author
----------------
.. include:: ../_static/authors/chase_roberts.txt


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.002 seconds)


.. _sphx_glr_download_demos_tutorial_jax_transformations.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tutorial_jax_transformations.py <tutorial_jax_transformations.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tutorial_jax_transformations.ipynb <tutorial_jax_transformations.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
