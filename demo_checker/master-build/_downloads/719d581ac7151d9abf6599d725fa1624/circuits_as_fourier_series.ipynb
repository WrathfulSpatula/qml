{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Circuits as Fourier series\n==========================\n\n```{=html}\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.1.9/p5.js\"></script>\n```\n::: {.meta}\n:property=\\\"og:description\\\": Learn interactively how we can view\ncircuits as Fourier series. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/thumbnail_tutorial_Fourier.png>\n:::\n\nIn this demo, we\\'re going to give an interactive, code-free\nintroduction to the idea of quantum circuits as Fourier series. We\\'ll\nalso discuss one of the main applications, called the parameter-shift\nrule. Concepts will be embodied in visualizations you can play with!\nWe\\'ll assume some familiarity with the basics of quantum circuits. The\nlast part of the demonstration is more advanced, and intended to give\nresearchers a different way to think of the material. But beginners are\nalso welcome!\n\nSingle-qubit gates\n------------------\n\nConsider a single-qubit Hermitian operator $G$. It acts on a\ntwo-dimensional space, and therefore has two eigenvalues:\n$\\kappa \\pm \\gamma$, where $\\kappa$ is the average and $2\\gamma$ the\ndifference between them. If we exponentiate $G$, with a parameter\n$\\theta$, we obtain a unitary gate:\n\n$$U(\\theta) = e^{i\\theta G}.$$\n\nUp to an overall phase of $e^{i\\theta\\kappa}$, which we can ignore\nbecause it is not measurable, the unitary $U$ has eigenvalues $e^\n{\\pm i \\theta\\gamma}$. In the eigenbasis (i.e., the basis of\neigenvectors of $G$, or equivalently $U$), it is diagonal. We will draw\nthis diagonal matrix as a magenta box:\n\n```{=html}\n<script src=\"../_static/demos/circuits_as_fourier_series/sketch.js\"></script>\n<figure>\n  <center>\n  <div id=\"sketch0_1\"></div>\n  </center>\n</figure>\n```\nWe will work on the eigenbasis of $U$ from now on. This means that a\ncolumn vector $[1, 0]^T$ is the eigenvector associated with $-\\gamma$,\nand $[0, 1]^T$ is associated with $+\\gamma$:\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick the image to toggle between eigenvectors.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch0_2\"></div>\n  </center>\n</figure>\n```\nWe\\'ve written the matrix as a box to suggest a different way to think\nof it: a *gate* in a quantum circuit. Instead of column vectors, we can\nuse bra-ket notation, with basis states $\\vert0\\rangle = [1, 0]^T$ and\n$\\vert 1\\rangle = [0, 1]^T$. We will also add some horizontal lines\nthrough the gate to suggest that the states are \\\"piped\\\" through and\npick up the corresponding phase.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick to toggle between basis states $\\vert 0\\rangle$ and\n$\\vert 1\\rangle$. While the mouse is in the magenta box, its vertical\nposition controls $\\gamma$, with the top of the box corresponding to\nhigh frequencies, and the bottom of the box to low frequencies.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch1\"></div>\n  </center>\n</figure>\n```\nFrequency components\n--------------------\n\nYou may wonder why we have introduced the parameter $\\theta$. The idea\nis that, if we have $U(\\theta)$ in our circuit, we can treat $\\theta$ as\na parameter we can tune to improve the results of the circuit for, say,\napproximating a state of interest. Viewed as functions of this tunable\nparameter $\\theta$, the purples phases are *exponentials* of frequency\n$\\omega = \\pm \\gamma$. Below, we plot the real and imaginary parts of\nthese frequencies.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick to toggle between basis states $\\vert 0\\rangle$ and\n$\\vert 1\\rangle$. While the mouse is in the magenta box, its vertical\nposition controls $\\gamma$.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch2\"></div>\n  </center>\n</figure>\n```\nUsually, we start a wire in the $\\vert 0\\rangle$ basis state. We can\n\\\"split\\\" this into a combination of $\\vert 0\\rangle$ and\n$\\vert1\\rangle$ by applying a gate $W$. We picture $W$ as a blue gate\nbelow. The state $\\vert\\psi(\\theta)\\rangle =U(\\theta)W\\vert 0\\rangle$\nwill then be a superposition of $e^{-i\\theta\\gamma}\\vert0\\rangle$ and\n$e^{+i\\theta\\gamma}\\vert1\\rangle$. Again, viewed as a function of\n$\\theta$, it has both frequency components, storing them in the\ncoefficient of the corresponding eigenstate.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nVertical mouse position in the blue box controls the relative weight of\n$\\vert0\\rangle$ and $\\vert 1\\rangle$. Hovering towards the top places\nthe most weight on $\\vert0\\rangle$, and towards the bottom on\n$\\vert1\\rangle$. Clicking the magenta box toggles between them, with\nvertical mouse position controlling $\\gamma$.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch3\" style></div>\n  </center>\n</figure>\n```\nOnce we\\'ve prepared a state $\\vert\\psi(\\theta)\\rangle$, we can measure\nit with some Hermitian operator $M$. In the context of variational\ncircuits, the result of a measurement can be used to optimize the\nparameter $\\theta$. To this end, let\\'s define the expectation value as\na function of $\\theta$,\n\n$$f(\\theta) = \\langle \\psi(\\theta)\\vert M \\vert\\psi(\\theta)\\rangle.$$\n\nWe represent the measurement $M$ as a yellow box, sandwiched between a\ncircuit on the left preparing the ket $\\vert\\psi(\\theta)\\rangle$ and an\nadjoint circuit preparing the bra $\\langle \\psi(\\theta)\\vert$.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nNote that taking the adjoint swaps $\\pm\\gamma$, and the order of\nelements in the circuit is inverted compared to the expression for\n$f(\\theta)$.\n:::\n\nWe can expand the bra and ket in terms of frequency components\n$e^{\\pm i\\theta\\gamma}$, so $f(\\theta)$ will be a sum of products of\nthese terms. We show this below.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick on magenta boxes to toggle between frequency components\ncontributing to $f(\\theta)$. Vertical position in the yellow box\ncontrols the constant terms $\\langle 0 \\vert M \\vert0\\rangle$ and\n$\\langle 1 \\vert M \\vert 1\\rangle$.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch4\" style></div>\n  </center>\n</figure>\n```\nTo recap what we\\'ve learned so far: the ket $\\vert\\psi\n(\\theta)\\rangle$ is a linear combination of the terms $e^\n{-i\\theta\\gamma}\\vert0\\rangle$ and $e^\n{+i\\theta\\gamma}\\vert1\\rangle$. If we measure this state, the\nexpectation value $f(\\theta) = \\langle\\psi(\\theta)\\vert M\\vert\\psi\n(\\theta)\\rangle$ is a linear combination of products of the frequency\nterms $e^{\\pm i\\theta\\gamma}$. More formally, we can write\n\n$$f(\\theta) = c_{(-2)} e^{-i2\\gamma\\theta} + c_{(0)} + c_{(+2)}\ne^{+i2\\gamma\\theta}$$\n\nfor some coefficients $c_{(-2)}, c_{(0)}, c_{(+2)}$. General expressions\nof this form---sums of exponential terms with evenly spaced\nfrequencies---are called *Fourier series*. This turns out to be a useful\nway to look at parameterized circuits!\n\nLarger circuits\n---------------\n\nWe can embed this structure, with a single occurrence of $U\n(\\theta)$, into a larger circuit. Instead of a linear combination of\n$\\vert0\\rangle$ and $\\vert 1\\rangle$, it will be a linear combination of\nthe form\n\n$$\\alpha_0 \\vert 0\\rangle \\otimes \\vert \\psi_0\\rangle + \\alpha_1 \\vert\n1\\rangle \\otimes\\vert \\psi_1\\rangle,$$\n\nfor some states $\\vert \\psi_0\\rangle$ and $\\vert \\psi_1\\rangle$ on the\nrest of the circuit, up to a reordering of wires. (This follows by\nfactorizing with respect to the tensor product.)\n\nAfter applying $U(\\theta)$, the overall state will become\n\n$$e^{-i\\theta\\gamma}\\alpha_0 \\vert 0\\rangle \\otimes\\vert \\psi_0\\rangle + e^{+i\\theta\\gamma}\\alpha_1 \\vert 1\\rangle \\otimes\\vert \\psi_1\\rangle.$$\n\nWhatever subsequent gates we apply, as long as there is only one\noccurrence of $U(\\theta)$, this expansion in frequencies $e^\n{\\pm i\\theta\\gamma}$ is the same. We illustrate how a single copy of $U\n(\\theta)$ acts on the larger circuit below.\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch5\" style></div>\n  </center>\n</figure>\n```\nIf there are multiple copies of $U(\\theta)$ in the circuit, we simply\nget more frequencies. Each box will multiply existing coefficients by\n$e^{-i\\gamma\\theta}$ or $e^{+i\\gamma\\theta}$, depending on the state it\nencounters. This will iteratively build up a state of the following\nform:\n\n$$\\vert \\psi(\\theta)\\rangle = \\alpha_{0\\cdots 00}e^{-in\\gamma\\theta}\\vert\n\\psi_{0\\cdots 00}\\rangle + \\alpha_{0\\cdots 01}e^{-i(n-2)\\gamma\\theta}\\vert\n\\psi_{0\\cdots 01}\\rangle + \\cdots + \\alpha_{1\\cdots 11}e^{+in\\gamma\\theta}\\vert\n\\psi_{1\\cdots 11}\\rangle,$$\n\nwhere the first term corresponds to choosing $e^{-i\\gamma \\theta}$ in\neach box, and the last term $e^{+i\\gamma \\theta}$ in each box. Note\nthat, for intermediate frequencies, many different choices yield the\nsame final result! We illustrate for $n = 2$ below, where the total\nstate at the end of the circuit is\n\n$$\\vert \\psi(\\theta)\\rangle = \\alpha_{00}e^{-i2\\theta\\gamma} \\vert \\psi_{00}\\rangle + \\alpha_{01}e^{i0\\gamma}\\vert\n\\psi_{01}\\rangle + \\alpha_{10}e^{i0\\gamma}\\vert \\psi_{10}\\rangle + \\alpha_{1}e^{+i2\\theta\\gamma} \\vert \\psi_{11}\\rangle.$$\n\nHere, there are two ways to obtain a frequency of zero.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick on the magenta boxes terms to choose $e^{\\pm i\\gamma\\theta}$ in\neach box. The final frequency is shown on the right. Although we are\nplacing the boxes in parallel, the result is the same for series.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch6\" style></div>\n  </center>\n</figure>\n```\nAs usual, we can form the expectation value of a measurement\n$f(\\theta) = \\langle \\psi(\\theta)\\vert M \\vert \\psi\n(\\theta)\\rangle$. This will be a linear combination of overlaps of the\nstates above. For instance, when $n =2$, it will be built from overlaps\nof the states (including the associated phases)\n\n$$e^{-i2\\theta\\gamma}\\vert \\psi_{00}\\rangle, \\quad e^{i0\\gamma}\\vert \\psi_{01}\\rangle, \\quad e^{i0\\gamma}\\vert\n\\psi_{10}\\rangle, \\quad e^{+i2\\theta\\gamma}\\vert \\psi_{11}\\rangle.$$\n\nThus, the expectation is a Fourier series, with terms arising from all\nthe different ways of combining these frequencies. For general $n$, this\nis\n\n$$f(\\theta) = c_{(-2n)}e^{-2in\\gamma\\theta} +\n \\cdots + c_{(0)} + \\cdots +\n c_{(2n)}e^{2in\\gamma\\theta}.$$\n\nBelow, we illustrate for $n = 2$, where the Fourier series consists of\nfive terms:\n\n$$f(\\theta) = c_{(-4)}e^{-4i\\gamma\\theta} + c_{(-2)}e^{-2i\\gamma\\theta} +\nc_{(0)} + c_{(+2)}e^{+2i\\gamma\\theta} +c_{(+4)}e^{+4i\\gamma\\theta}.$$\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick on the magenta boxes terms to choose $e^{\\pm i\\gamma\\theta}$ in\neach box, contributing to the final frequency. This frequency is shown\nbelow the circuit.\n:::\n\n```{=html}\n<figure>\n  <center>\n  <div id=\"sketch7\" style></div>\n  </center>\n</figure>\n```\nCoefficient vectors\n-------------------\n\nSo far, we\\'ve focused on the $\\theta$-dependent \\\"pure frequency\\\"\nterms $e^{i\\omega\\theta\\gamma}$ appearing in the Fourier series.\nHowever, the *coefficients* $c_w$ also have an important role to play.\nIt turns out that, for a given set of frequencies in the Fourier series,\nthe coefficients *uniquely* characterize $f(\\theta)$. We\\'ll redo the\n$n = 2$ example but highlight the coefficients instead; these depend on\nthe structure of the circuit and the choice of measurement.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick on the magenta boxes terms to choose $e^{\\pm i\\gamma\\theta}$ in\neach box, contributing to the final frequency. The coefficient of this\nfinal frequency is shown below the circuit.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch8\" style></div>\n    </center>\n</figure>\n```\nThe set of Fourier sums of fixed degree form a vector space, and the\npure frequencies $e^{i\\omega \\theta\\gamma}$ are a *basis*. We can\nassemble these coefficients into a column vector, $\\vec{c}_f$, which\ncontains the same information as the function $f(\\theta)$:\n\n$$\\begin{aligned}\nf(\\theta) = c_{(-2n)} e^{-i2n\\theta\\gamma} + \\cdots + c_{(+2n)} e^{+i2n\\theta\\gamma} \\quad\n\\Leftrightarrow \\quad \\vec{c}_f =\n\\begin{bmatrix}\nc_{(-2n)} \\\\\n\\vdots \\\\\nc_{(2n)}\n\\end{bmatrix}.\n\\end{aligned}$$\n\nIf an operation on the function $f(\\theta)$ preserves the structure of\nthe function, i.e., it only modifies the coefficients, then we can think\nof it as an operation on vectors instead! Our first example is\ndifferentiation. This simply pulls down a constant term from the\nexponent of each pure frequency:\n\n$$f'(\\theta) = (-i2n\\gamma)c_{-2n} e^{-i2n\\theta\\gamma} + \\cdots +\n(+i2n\\gamma)c_{+2n} e^{+i2n\\theta\\gamma}.$$\n\nIn terms of the coefficient vectors, however, it is just a diagonal\nmatrix $D$:\n\n$$\\begin{aligned}\n\\vec{c}_{f'} =\n\\begin{bmatrix}\n(-i2n\\gamma)c_{-2n} \\\\\n\\vdots \\\\\n(+i2n\\gamma)c_{2n}\n\\end{bmatrix} =\n\\begin{bmatrix}\n-i2n\\gamma & & \\\\\n& \\ddots & \\\\\n&& +i2n\\gamma\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{-2n} \\\\\n\\vdots \\\\ c_{2n}\n\\end{bmatrix} = D\\vec{c}.\n\\end{aligned}$$\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nThe derivative of the expectation $f(\\theta)$ as a coefficient vector.\nClick on the magenta boxes to choose terms contributing to the final\nfrequency.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch9\" style></div>\n    </center>\n</figure>\n```\nAnother particularly simple operation on $f(\\theta)$ is to shift the\nparameter $\\theta$ by some constant amount $s$, giving a new function\n$f_s(\\theta) = f(\\theta + s)$, also called a *parameter shift*. From\nindex laws, this adds an exponential factor to each coefficient:\n\n$$f(\\theta + s) = c_{(-2n)} e^{-i2n(\\theta + s)\\gamma} + \\cdots +\nc_{(+2n)} e^{+i2n(\\theta + s)\\gamma} = e^{-i2ns\\gamma}c_{(-2n)} e^{-i2n\\theta\\gamma} + \\cdots\n+ e^{+i2ns\\gamma}c_{(+2n)} e^{+i2n\\theta\\gamma}.$$\n\nOnce again, this can be viewed as a diagonal matrix $T_s$ acting on the\ncoefficient vector:\n\n$$\\begin{aligned}\n\\vec{c}_{f_s} =\n\\begin{bmatrix}\ne^{-i2ns\\gamma}c_{-2n} \\\\\n\\vdots \\\\\ne^{+i2ns\\gamma}c_{2n}\n\\end{bmatrix} =\n\\begin{bmatrix}\ne^{-i2ns\\gamma} & & \\\\\n& \\ddots & \\\\\n&& e^{+i2ns\\gamma}\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{-2n} \\\\\n\\vdots \\\\ c_{2n}\n\\end{bmatrix} = T_s\\vec{c}.\n\\end{aligned}$$\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nThe parameter shifted expectation $f(\\theta + s)$ as a coefficient\nvector. Click on the magenta boxes to choose terms contributing to the\nfinal frequency.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch10\" style></div>\n    </center>\n</figure>\n```\nThe two-term parameter-shift rule\n---------------------------------\n\nOur original motivation for introducing $\\theta$ was to *optimize* the\nmeasurement result $f(\\theta)$. If we can differentiate $f\n(\\theta)$, we can use tools from classical machine learning such as\n*gradient descent*. The problem is that circuits are black boxes; all we\ncan do is set some parameters, pull a lever, and out pops a measurement\noutcome. It\\'s a bit like a toaster. How do you differentiate a toaster?\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick the measurement button for toasty measurement outcomes.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch11\" style></div>\n    </center>\n</figure>\n```\nLuckily, the magic of Fourier series and coefficient vectors come to the\nrescue. The basic idea is to write the differentiation matrix $D$ as a\nlinear combination of shift matrices $T_s$. Although we can\\'t\ndifferentiate directly, we \\_[can]() change parameters! Let\\'s\nillustrate with the simple example of $n = 1$. In this case, the\nmatrices are\n\n$$\\begin{aligned}\nD =\n\\begin{bmatrix}-2i\\gamma && \\\\ & 0 & \\\\ & & +2i\\gamma \\end\n{bmatrix}, \\quad T_s = \\begin{bmatrix}e^{-2i\\gamma s} && \\\\ &1& \\\\ && e^\n{+2i\\gamma s} \\end{bmatrix}.\n\\end{aligned}$$\n\nIt\\'s easy to check that for any $s$,\n\n$$D = \\frac{2\\gamma}{\\sin(\\gamma s)}(T_s - T_{-s}).$$\n\nTranslating back into statements about functions, we learn that\n\n$$f'(\\theta) = \\frac{2\\gamma}{\\sin(\\gamma s)}[f(\\theta + s) - f\n(\\theta - s)].$$\n\nThis is called *two-term parameter-shift rule*.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nClick anywhere for gratuitous toast.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch11_5\" style></div>\n    </center>\n</figure>\n```\nThe two-term rule has a simple geometric interpretation. Changing\n$\\theta$ takes us around a circle of fixed radius\n$r = \\vert c_{\\pm2}\\vert$ at speed $\\gamma/2\\pi$.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nNote that parameter shifts add phases and don\\'t change $\\vert c_\n{(\\pm 2)}\\vert$. The radius is given by either, since the reality of\nmeasurement outcomes implies $c_{(+2)} = \\overline{c_{(-2)}}$.\n:::\n\nThe derivative $f'(\\theta)$ is a tangent vector of length $r\\gamma$. We\ncan choose Cartesian coordinates where this tangent vector is vertical,\nwith components\n\n$$f'(\\theta) = (0, r\\gamma).$$\n\nThe parameter shifts $f(\\theta \\pm s)$, on the other hand, have\ncomponents\n\n$$f(\\theta \\pm s) = (r \\cos(\\gamma s), \\pm r\\sin(\\gamma s)).$$\n\nIt follows immediately that\n\n$$f'(\\theta) = \\frac{2\\gamma}{\\sin(\\gamma s)} [f(\\theta + s) - f(\\theta - s)].$$\n\nWe picture the geometry below. On the right, tangent to the circle, is\n$f'$ as a coefficient vector in the $c_{\\pm2}$ plane. We display\n$f(\\theta + s)$ and $-f(\\theta - s)$ as light magenta lines. Their\nvector sum $\\Delta f$ is a dark magenta line.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nHorizontal mouse position controls $s$.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch12\" style></div>\n    </center>\n</figure>\n```\nThe general parameter-shift rule\n--------------------------------\n\nFor $n > 1$, parameter shift rules don\\'t have a simple geometric\ninterpretation. The problem is that each pair of coefficients\n$c_{\\pm \\omega}$ is associated with a circle governing two components of\nthe coefficient vector. To find the derivative, we need to understand\neach pair of components separately, but parameter shifts\n*simultaneously* wind us around all the circles, at different speeds!\nGeometrically speaking, putting all these circles together gives a\n*higher-dimensional donut*, which parameter shifts wind us around. This\nsounds complicated!\n\nIt also looks complicated, as we illustrate for $n = 3$ below. We set\n$\\gamma = 1$ so that, on the circle in the $c_{\\pm\n\\omega}$ plane, we execute $\\omega$ revolutions for a single cycle of\n$s$.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nHorizontal mouse position controls $s$. For instance, if we place it in\nthe middle of interval, $s = \\pi$. This translates into an angle\n$\\pm \\pi\\gamma$ on the circle labelled $c_{(\\pm \\gamma)}$.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch13\" style></div>\n    </center>\n</figure>\n```\nPerhaps surprisingly, the coefficient vector perspective and a few\ntricks let us derive the general parameter-shift rule straighforwardly.\nWe start with the observation that $f'(\\theta)$ has a coefficient vector\nwith $2n$ nonzero components, since the constant term always vanishes.\nThus, $2n$ linearly independent parameter shifts $f\n(\\theta + s_k)$ should be sufficient to reconstruct the derivative, with\n\n$$f'(\\theta) = \\sum_{k=1}^{2n} \\beta_k f(\\theta + s_k) \\tag{1}$$\n\nfor some coefficients $\\beta_k$. The problem is how to find the shifts\nand coefficients! We can invoke linear algebra to find coefficients, but\nonly once we choose shifts, and it\\'s not obvious how to get them to be\nindependent. We can see the problem for $n = 3$, where we choose six\nrandom shifts. Are they independent or not? It seems hard to tell. Is\nthere a better approach?\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nHorizontal mouse position controls $s$, which is now \\\"quantized\\\" with\nrandom shifts.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch14\" style></div>\n    </center>\n</figure>\n```\nThankfully, there is! We can solve two problems at once by introducing\nan *inner product*, i.e., a way to find the scalar overlap of two\nvectors. This will let us identify orthogonal and hence independent\nshifts $s_k$. Since they are orthogonal, we can also easily determine\nthe coefficients $\\beta_k$. The idea is straightforward: since the\nmatrices of interest are diagonal,\n\n$$D = \\mbox{diag}(-2in\\gamma, \\ldots, +2in\\gamma), \\quad T_s = \\mbox{diag}\n(e^{-2in\\gamma s}, \\ldots, e^{+2in\\gamma s}),$$\n\nwe can just pluck out the vector of diagonal entries and define a\ncomplex inner product in the usual way. Technically, this is the\n[Frobenius inner\nproduct](https://en.wikipedia.org/wiki/Frobenius_inner_product) for\nmatrices:\n\n$$\\langle A, B\\rangle = \\mbox{Tr}[A^\\dagger B].$$\n\nConsider two shifts $s, t \\in [0, 2\\pi/\\gamma)$, and define\n$\\omega = e^{2\\pi i\\gamma(t-s)}$. The inner product of diagonal shift\nmatrices $T_s, T_t$ is\n\n$$\\langle T_s, T_t\\rangle = \\sum_{j=-n}^n \\omega^j = \\omega^{-n}\\sum_{j=0}^\n{2n} \\omega^j= \\frac{\\omega^{-n}(1 - \\omega^{2n+1})}{1 - \\omega} \\tag{2}$$\n\nusing the [geometric\nseries](https://en.wikipedia.org/wiki/Geometric_series). Before moving\non, let\\'s visualize what these inner products look like for $n = 3$.\nThe expression (2) is a sum of phases, which we can add top-to-tail on\nthe complex plane. We\\'ve added a big $\\mathbb{C}$ to distinguish this\nfrom other planes we\\'ve been looking at.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nWe display the inner product $\\langle T_s, T_t\\rangle$ below. Horizontal\nmouse position controls $s$. The choice of $t$ is \\\"quantized\\\" to the\nrandom shifts from above; click to the left to set it. The phases summed\nare light magenta, and the total is dark magenta.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch14_5\" style></div>\n    </center>\n</figure>\n```\nAs expected, our random shifts are not orthogonal. But with (2) in hand,\nit\\'s easy to choose orthogonal vectors! We simply select our shifts\n$s_j$ so that the numerator of (2) vanishes:\n\n$$\\omega^{2n + 1} = e^{2\\pi i \\gamma(2n+1)(s_k - s_j)} = 1.$$\n\nThis occurs if $\\gamma(2n + 1)(s_k - s_j)$ is always an integer. A\nnatural choice is\n\n$$s_k = \\frac{k}{\\gamma(2n+1)},$$\n\nfor $k = 1, \\ldots, 2n$. In this case, we have the orthogonality\nrelation\n\n$$\\langle T_{s_k}, T_{s_k}\\rangle = (2n+1)\\delta_{jk}.$$\n\nThus, spacing shifts equally around the $s$ circle gives us an\northogonal set of shifts. We picture these equally spaced shifts, and\ncheck visually they are orthogonal, for $n=3$ below. Select any of the\nequally spaced points, and you can see that its inner product with\nanother of the equally spaced points vanishes.\n\n::: {.tip}\n::: {.title}\nTip\n:::\n\nHorizontal mouse position controls $s$.\n:::\n\n```{=html}\n<figure>\n    <center>\n    <div id=\"sketch15\" style></div>\n    </center>\n</figure>\n```\nOrthogonality makes finding the coefficients $\\beta_k$ easy: we simply\ntake the inner product between $T_{s_k}$ and the left-hand side of (1),\nexpressed in terms of the differentiation matrix $D$. This gives\n\n$$\\begin{align*}\n\\beta_k & = \\frac{2i\\gamma}{2n+1}\\sum_{j=-n}^n j \\omega_k^j\n\\end{align*}$$\n\nfor $\\omega_k = e^{-2\\pi i k/(2n+1)}$. This looks tricky, but we can\nstart with a geometric series, *differentiate* with respect to $\\omega$,\nand multiply by $\\omega_k$, so we get what we want:\n\n$$\\omega_k \\partial_{\\omega_k} \\sum_{j=-n}^n \\omega_k^j = \\sum_\n {j=-n}^n j\\omega_k^j.$$\n\nWe already computed the geometric series in (2). Plugging that back in,\ndifferentiating, and using the fact that $\\omega_k^{2n+1} = 1$, we\nfinally get\n\n$$\\begin{align*}\n\\beta_k & =\n\\frac{2i\\gamma}{2n+1}\\cdot\\frac{\\omega_k^{-n}(2n+1)(\\omega_k -\n1)}{(\\omega_k - 1)^2} = \\frac{2 i\\gamma \\omega_k^{-n}}{\\omega_k-1}.\n\\end{align*}$$\n\nPutting these together with our shifts, we have our general\nparameter-shift rule:\n\n$$f'(\\theta) = \\sum_{k=1}^{2n}\\frac{2 i\\gamma \\omega_k^{-n}}\n{\\omega_k-1}f\\left(\\theta + \\frac{k}{\\gamma(2n+1)}\\right).$$\n\nThe approach outlined here only works when the frequencies in the\nproblem are evenly spaced. However, there are ways to generalize\nfurther. Even without orthogonality, we can find independent shifts and\nsolve the linear algebra problem (1) for the coefficients.\nAlternatively, we can use randomization to obtain shifts that are\northogonal on average, leading to the *stochastic parameter-shift rule*.\n\nConclusion\n----------\n\nWe\\'ve seen that, by viewing a parameterized gate as an opportunity to\nchoose eigenvalues, we naturally expand the expectations of the circuit\nas a Fourier series, that is, a sum of frequency terms, corresponding to\nthe eigenvalues we chose along the way, with coefficients depending on\nthe circuit. By thinking about the structure of coefficients themselves,\nwe derived parameter-shift rules, allowing us to evaluate the derivative\nof a circuit as a linear combination of its expectation value at\ndifferent parameter values. This is a key part of the machinery of\nperforming machine learning with quantum circuits, since we cannot\nperform the derivatives directly.\n\nIf you\\'d like to learn more, there are many papers on the topic. Two\nparticularly clear references:\n\n-   [General parameter-shift rules for quantum\n    gradients](https://arxiv.org/abs/2107.12390) (2021). David Wierichs,\n    Josh Izaac, Cody Wang, Cedric Yen-Yu Lin.\n-   [The effect of data encoding on the expressive power of variational\n    quantum machine learning models](https://arxiv.org/abs/2008.08605)\n    (2020). Maria Schuld, Ryan Sweke, Johannes Jakob Meyer.\n\nHandily, both references have an associated PennyLane demo!\n\n-   [Quantum models as Fourier\n    series](https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series)\n    (2020). Maria Schuld, Ryan Sweke, Johannes Jakob Meyer.\n-   [Generalized parameter-shift\n    rules](https://pennylane.ai/qml/demos/tutorial_general_parshift)\n    (2021). David Wierichs.\n\nFinally, PennyLane is jam-packed with tools for analyzing circuits as\nFourier series. Check out the documentation on the [Fourier\nmodule](https://docs.pennylane.ai/en/stable/code/qml_fourier.html) to\nlearn more!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "About the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}