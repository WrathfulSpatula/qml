{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An equivariant graph embedding\n==============================\n\n::: {.meta}\n:property=\\\"og:description\\\": Find out more about how to embedd graphs\ninto quantum states. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/thumbnail_tutorial_equivariant_graph_embedding.png>\n:::\n\n::: {.related}\ntutorial\\_geometric\\_qml Geometric quantum machine learning\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A notorious problem when data comes in the form of graphs \\-- think of\nmolecules or social media networks \\-- is that the numerical\nrepresentation of a graph in a computer is not unique. For example, if\nwe describe a graph via an [adjacency\nmatrix](https://en.wikipedia.org/wiki/Adjacency_matrix) whose entries\ncontain the edge weights as off-diagonals and node weights on the\ndiagonal, any simultaneous permutation of rows and columns of this\nmatrix refer to the same graph.\n\n![](../demonstrations/equivariant_graph_embedding/adjacency-matrices.png){.align-center\nwidth=\"60.0%\"}\n\nFor example, the graph in the image above is represented by each of the\ntwo equivalent adjacency matrices. The top matrix can be transformed\ninto the bottom matrix by swapping the first row with the third row,\nthen swapping the third column with the third column, then the new first\nrow with the second, and finally the first colum with the second.\n\nBut the number of such permutations grows factorially with the number of\nnodes in the graph, which is even worse than an exponential growth!\n\nIf we want computers to learn from graph data, we usually want our\nmodels to \\\"know\\\" that all these permuted adjacency matrices refer to\nthe same object, so we do not waste resources on learning this property.\nIn mathematical terms, this means that the model should be in- or\nequivariant (more about this distinction below) with respect to\npermutations. This is the basic motivation of [Geometric Deep\nLearning](https://geometricdeeplearning.com/), ideas of which have found\ntheir way into quantum machine learning.\n\nThis tutorial shows how to implement an example of a trainable\npermutation equivariant graph embedding as proposed in [Skolik et al.\n(2022)](https://arxiv.org/pdf/2205.06109.pdf). The embedding maps the\nadjacency matrix of an undirected graph with edge and node weights to a\nquantum state, such that permutations of an adjacency matrix get mapped\nto the same states *if only we also permute the qubit registers in the\nsame fashion*.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThe tutorial is meant for beginners and does not contain the\nmathematical details of the rich theory of equivariance. Have a look [at\nthis demo](https://pennylane.ai/qml/demos/tutorial_geometric_qml.html)\nif you want to know more.\n:::\n\nPermuted adjacency matrices describe the same graph\n===================================================\n\nLet us first verify that permuted adjacency matrices really describe one\nand the same graph. We also gain some useful data generation functions\nfor later.\n\nFirst we create random adjacency matrices. The entry $a_{ij}$ of this\nmatrix corresponds to the weight of the edge between nodes $i$ and $j$\nin the graph. We assume that graphs have no self-loops; instead, the\ndiagonal elements of the adjacency matrix are interpreted as node\nweights (or \\\"node attributes\\\").\n\nTaking the example of a Twitter user retweet network, the nodes would be\nusers, edge weights indicate how often two users retweet each other and\nnode attributes could indicate the follower count of a user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndef create_data_point(n):\n    \"\"\"\n    Returns a random undirected adjacency matrix of dimension (n,n). \n    The diagonal elements are interpreted as node attributes.\n    \"\"\"\n    mat = np.random.rand(n, n)\n    A = (mat + np.transpose(mat))/2    \n    return np.round(A, decimals=2)\n\nA = create_data_point(3)\nprint(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s also write a function to generate permuted versions of this\nadjacency matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def permute(A, permutation):\n    \"\"\"\n    Returns a copy of A with rows and columns swapped according to permutation. \n    For example, the permutation [1, 2, 0] swaps 0->1, 1->2, 2->0.\n    \"\"\"\n    \n    P = np.zeros((len(A), len(A)))\n    for i,j in enumerate(permutation):\n        P[i,j] = 1\n\n    return P @ A @ np.transpose(P)\n\nA_perm = permute(A, [1, 2, 0])\nprint(A_perm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we create [networkx]{.title-ref} graphs from both adjacency matrices\nand plot them, we see that they are identical as claimed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n\n# interpret diagonal of matrix as node attributes\nnode_labels = {n: A[n,n] for n in range(len(A))} \nnp.fill_diagonal(A, np.zeros(len(A))) \n\nG1 = nx.Graph(A)\npos1=nx.spring_layout(G1)\nnx.draw(G1, pos1, labels=node_labels, ax=ax1, node_size = 800, node_color = \"#ACE3FF\")\nedge_labels = nx.get_edge_attributes(G1,'weight')\nnx.draw_networkx_edge_labels(G1,pos1,edge_labels=edge_labels, ax=ax1)\n\n# interpret diagonal of permuted matrix as node attributes\nnode_labels = {n: A_perm[n,n] for n in range(len(A_perm))}\nnp.fill_diagonal(A_perm, np.zeros(len(A)))\n\nG2 = nx.Graph(A_perm)\npos2=nx.spring_layout(G2)\nnx.draw(G2, pos2, labels=node_labels, ax=ax2, node_size = 800, node_color = \"#ACE3FF\")\nedge_labels = nx.get_edge_attributes(G2,'weight')\nnx.draw_networkx_edge_labels(G2,pos2,edge_labels=edge_labels, ax=ax2)\n\nax1.set_xlim([1.2*x for x in ax1.get_xlim()])\nax2.set_xlim([1.2*x for x in ax2.get_xlim()])\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.title}\nNote\n:::\n\nThe issue of non-unique numerical representations of graphs ultimately\nstems from the fact that the nodes in a graph do not have an intrinsic\norder, and by labelling them in a numerical data structure like a matrix\nwe therefore impose an arbitrary order.\n:::\n\nPermutation equivariant embeddings\n==================================\n\nWhen we design a machine learning model that takes graph data, the first\nstep is to encode the adjacency matrix into a quantum state using an\nembedding or [quantum feature\nmap](https://pennylane.ai/qml/glossary/quantum_feature_map.html) $\\phi$:\n\n$$A \\rightarrow |\\phi(A)\\rangle .$$\n\nWe may want the resulting quantum state to be the same for all adjacency\nmatrices describing the same graph. In mathematical terms, this means\nthat $\\phi$ is an *invariant* embedding with respect to simultaneous row\nand column permutations $\\pi(A)$ of the adjacency matrix:\n\n$$|\\phi(A) \\rangle = |\\phi(\\pi(A))\\rangle \\;\\; \\text{ for all } \\pi .$$\n\nHowever, invariance is often too strong a constraint. Think for example\nof an encoding that associates each node in the graph with a qubit. We\nmight want permutations of the adjacency matrix to lead to the same\nstate *up to an equivalent permutation of the qubits* $P_{\\pi}$, where\n\n$$P_{\\pi} |q_1,...,q_n \\rangle = |q_{\\textit{perm}_{\\pi}(1)}, ... q_{\\textit{perm}_{\\pi}(n)} \\rangle .$$\n\nThe function $\\text{perm}_{\\pi}$ maps each index to the permuted index\naccording to $\\pi$.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThe operator $P_{\\pi}$ is implemented by PennyLane\\'s\n`~pennylane.Permute`{.interpreted-text role=\"class\"}.\n:::\n\nThis results in an *equivariant* embedding with respect to permutations\nof the adjacency matrix:\n\n$$|\\phi(A) \\rangle = P_{\\pi}|\\phi(\\pi(A))\\rangle \\;\\; \\text{ for all } \\pi .$$\n\nThis is exactly what the following quantum embedding is aiming to do!\nThe mathematical details behind these concepts use group theory and are\nbeautiful, but can be a bit daunting. Have a look at [this\npaper](https://arxiv.org/abs/2210.08566) if you want to learn more.\n\nImplementation in PennyLane\n===========================\n\nLet\\'s get our hands dirty with an example. As mentioned, we will\nimplement the permutation-equivariant embedding suggested in [Skolik et\nal. (2022)](https://arxiv.org/pdf/2205.06109.pdf) which has this\nstructure:\n\n![](../demonstrations/equivariant_graph_embedding/circuit.png){.align-center\nwidth=\"70.0%\"}\n\nThe image can be found in [Skolik et al.\n(2022)](https://arxiv.org/pdf/2205.06109.pdf) and shows one layer of the\ncircuit. The $\\epsilon$ are our edge weights while $\\alpha$ describe the\nnode weights, and the $\\beta$, $\\gamma$ are variational parameters.\n\nIn PennyLane this looks as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n\ndef perm_equivariant_embedding(A, betas, gammas):\n    \"\"\"\n    Ansatz to embedd a graph with node and edge weights into a quantum state.\n    \n    The adjacency matrix A contains the edge weights on the off-diagonal, \n    as well as the node attributes on the diagonal.\n    \n    The embedding contains trainable weights 'betas' and 'gammas'.\n    \"\"\"\n    n_nodes = len(A)\n    n_layers = len(betas) # infer the number of layers from the parameters\n    \n    # initialise in the plus state\n    for i in range(n_nodes):\n        qml.Hadamard(i)\n    \n    for l in range(n_layers):\n\n        for i in range(n_nodes):\n            for j in range(i):\n            \t# factor of 2 due to definition of gate\n                qml.IsingZZ(2*gammas[l]*A[i,j], wires=[i,j]) \n\n        for i in range(n_nodes):\n            qml.RX(A[i,i]*betas[l], wires=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this ansatz in a circuit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_qubits = 5\nn_layers = 2\n\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef eqc(adjacency_matrix, observable, trainable_betas, trainable_gammas):\n    \"\"\"Circuit that uses the permutation equivariant embedding\"\"\"\n    \n    perm_equivariant_embedding(adjacency_matrix, trainable_betas, trainable_gammas)\n    return qml.expval(observable)\n\n\nA = create_data_point(n_qubits)\nbetas = np.random.rand(n_layers)\ngammas = np.random.rand(n_layers)\nobservable = qml.PauliX(0) @ qml.PauliX(1) @ qml.PauliX(3)\n\nqml.draw_mpl(eqc, decimals=2)(A, observable, betas, gammas)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating the equivariance\n===========================\n\nLet\\'s now check if the circuit is really equivariant!\n\nThis is the expectation value we get using the original adjacency matrix\nas an input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_A = eqc(A, observable, betas, gammas)\nprint(\"Model output for A:\", result_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we permute the adjacency matrix, this is what we get:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perm = [2, 3, 0, 1, 4]\nA_perm = permute(A, perm)\nresult_Aperm = eqc(A_perm, observable, betas, gammas)\nprint(\"Model output for permutation of A: \", result_Aperm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why are the two values different? Well, we constructed an *equivariant*\nansatz, not an *invariant* one! Remember, an *invariant* ansatz means\nthat embedding a permutation of the adjacency matrix leads to the same\nstate as an embedding of the original matrix. An *equivariant* ansatz\nembeds the permuted adjacency matrix into a state where the qubits are\npermuted as well.\n\nAs a result, the final state before measurement is only the same if we\npermute the qubits in the same manner that we permute the input\nadjacency matrix. We could insert a permutation operator\n`qml.Permute(perm)` to achieve this, or we simply permute the wires of\nthe observables!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "observable_perm = qml.PauliX(perm[0]) @ qml.PauliX(perm[1]) @ qml.PauliX(perm[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now everything should work out!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_Aperm = eqc(A_perm, observable_perm, betas, gammas)\nprint(\"Model output for permutation of A, and with permuted observable: \", result_Aperm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et voil\u00e0!\n\nConclusion\n==========\n\nEquivariant graph embeddings can be combined with other equivariant\nparts of a quantum machine learning pipeline (like measurements and the\ncost function). [Skolik et al.\n(2022)](https://arxiv.org/pdf/2205.06109.pdf), for example, use such a\npipeline as part of a reinforcement learning scheme that finds heuristic\nsolutions for the traveling salesman problem. Their simulations compare\na fully equivariant model to circuits that break permutation\nequivariance and show that it performs better, confirming that if we\nknow about structure in our data, we should try to use this knowledge in\nmachine learning.\n\nReferences\n==========\n\n1.  Andrea Skolik, Michele Cattelan, Sheir Yarkoni,Thomas Baeck and\n    Vedran Dunjko (2022). Equivariant quantum circuits for learning on\n    weighted graphs.\n    [arXiv:2205.06109](https://arxiv.org/abs/2205.06109)\n2.  Quynh T. Nguyen, Louis Schatzki, Paolo Braccia, Michael Ragone,\n    Patrick J. Coles, Fr\u00e9d\u00e9ric Sauvage, Mart\u00edn Larocca and Marco Cerezo\n    (2022). Theory for Equivariant Quantum Neural Networks.\n    [arXiv:2210.08566](https://arxiv.org/abs/2210.08566)\n\nAbout the author\n================\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}