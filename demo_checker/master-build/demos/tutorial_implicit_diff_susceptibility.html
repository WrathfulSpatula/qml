
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta content="Implicitly differentiating the the solution of a VQA in PennyLane." property="og:description" />
<meta content="https://pennylane.ai/qml/_images/descartes.png" property="og:image" />

  <link rel="icon" type="image/x-icon" href="../_static/favicon.ico">
  <link rel="shortcut icon" type="image/x-icon" href="../_static/favicon.ico">
  


  <meta property="og:title" content="Implicit differentiation of variational quantum algorithms &#8212; PennyLane">
  <meta property="og:url" content="https://pennylane.ai/qml/demos/tutorial_implicit_diff_susceptibility.html">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary_large_image">

  
  
  <meta content="Implicitly differentiating the the solution of a VQA in PennyLane." property="og:description" />
  

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&display=swap">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css">
  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/css/bootstrap.min.css">
  <!-- Material Design Bootstrap -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.5.14/css/mdb.min.css">
  <!-- NanoScroller -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.nanoscroller/0.8.7/css/nanoscroller.min.css">
  <!-- Syntax Highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/tomorrow-night.min.css">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
       TeX: {
         Macros: {
           pr : ['|\#1\\rangle\\langle\#1|',1],
           ket: ['\\left| \#1\\right\\rangle',1],
           bra: ['\\left\\langle \#1\\right|',1],
           xket: ['\\left| \#1\\right\\rangle_x',1],
           xbra: ['\\left\\langle \#1\\right|_x',1],
           braket: ['\\langle \#1 \\rangle',1],
           braketD: ['\\langle \#1 \\mid \#2 \\rangle',2],
           braketT: ['\\langle \#1 \\mid \#2 \\mid \#3 \\rangle',3],
           ketbra: ['| #1 \\rangle \\langle #2 |',2],
           hc: ['\\text{h.c.}',0],
           cc: ['\\text{c.c.}',0],
           h: ['\\hat',0],
           nn: ['\\nonumber',0],
           di: ['\\frac{d}{d \#1}',1],
           uu: ['\\mathcal{U}',0],
           inn: ['\\text{in}',0],
           out: ['\\text{out}',0],
           vac: ['\\text{vac}',0],
           I: ['\\hat{\\mathbf{1}}',0],
           x: ['\\hat{x}',0],
           p: ['\\hat{p}',0],
           a: ['\\hat{a}',0],
           ad: ['\\hat{a}^\\dagger',0],
           n: ['\\hat{n}',0],
           nbar: ['\\overline{n}',0],
           sech: ['\\mathrm{sech~}',0],
           tanh: ['\\mathrm{tanh~}',0],
           re: ['\\text{Re}',0],
           im: ['\\text{Im}',0],
           tr: ['\\mathrm{Tr} #1',1],
           sign: ['\\text{sign}',0],
           overlr: ['\\overset\\leftrightarrow{\#1}',1],
           overl: ['\\overset\leftarrow{\#1}',1],
           overr: ['\\overset\rightarrow{\#1}',1],
           avg: ['\\left< \#1 \\right>',1],
           slashed: ['\\cancel{\#1}',1],
           bold: ['\\boldsymbol{\#1}',1],
           d: ['\\mathrm d',0],
           expect: ["\\langle #1 \\rangle",1],
           pde: ["\\frac{\\partial}{\\partial \#1}",1],
           R: ["\\mathbb{R}",0],
           C: ["\\mathbb{C}",0],
           Ad: ["\\text{Ad}",0],
           Var: ["\\text{Var}",0],
           bx: ["\\mathbf{x}", 0],
           bm: ["\\boldsymbol{\#1}",1],
           haf: ["\\mathrm{haf}",0],
           lhaf: ["\\mathrm{lhaf}",0]
         }
       }
     });
     </script>

  <!-- Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-130507810-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-130507810-1');
      </script>
  
    <title>Implicit differentiation of variational quantum algorithms &#8212; PennyLane  documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/xanadu.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/light-slider.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/hubs.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link rel="canonical" href="https://pennylane.ai/qml/demos/tutorial_implicit_diff_susceptibility.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Perturbative Gadgets for Variational Quantum Algorithms" href="tutorial_barren_gadgets.html" />
    <link rel="prev" title="Differentiating quantum error mitigation transforms" href="tutorial_diffable-mitigation.html" /> 
  </head><body><nav class="navbar navbar-expand-lg navbar-light white sticky-top">

<!-- Logo and Title -->









  



  <a class="navbar-brand nav-link" href="https://pennylane.ai">
    
  <img class="pr-1" src=" ../_static/logo.png" width="28px"></img>
  
    <img id="navbar-wordmark" src="../_static/pennylane.svg"></img>
  
  </a>


  <!-- [Mobile] Collapse Button -->
  <div class="row right">
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#basicExampleNav"
      aria-controls="basicExampleNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
  </div>

  <!-- [Mobile] Collapsible Content -->
  <div class="collapse navbar-collapse" id="basicExampleNav">

    <!-- Links on the Left -->
    <ul class="navbar-nav mr-auto">
      
        
          
            <li class="nav-item active">
              <a class="nav-link" href="https://pennylane.ai/qml/">
                
  
    Learn
  

              </a>
              <span class="sr-only">(current)</span>
            </li>
          

        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/qml/demonstrations.html">
                
  
    Demos
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/install.html">
                
  
    Install
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/plugins.html">
                
  
    Plugins
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://docs.pennylane.ai">
                
  
    Documentation
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/blog/">
                
  
    Blog
  

            </a>
          </li>
        
      
    </ul>

    <!-- Links on the Right -->
    <ul class="navbar-nav ml-auto nav-flex-icons">
      
        <li class="nav-item">
          <a class="nav-link" href="https://pennylane.ai/faq.html">
            <i class="fas fa-question pr-1"></i> FAQ
          </a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://discuss.pennylane.ai/">
            <i class="fab fa-discourse pr-1"></i> Support
          </a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/PennyLaneAI/pennylane">
            <i class="fab fa-github pr-1"></i> GitHub
          </a>
        </li>
      

    </ul>
  </div>

</nav>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="tutorial_barren_gadgets.html" title="Perturbative Gadgets for Variational Quantum Algorithms"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tutorial_diffable-mitigation.html" title="Differentiating quantum error mitigation transforms"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../quantum-computing.html" >Quantum Computing</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="../demos_optimization.html" accesskey="U">Optimization</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Implicit differentiation of variational quantum algorithms</a></li> 
      </ul>
    </div>
    <div class="container-wrapper">
        <div id="content">
          <div id="right-column">
            
            

            <div class="document clearer body">
              
    <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-demos-tutorial-implicit-diff-susceptibility-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="implicit-differentiation-of-variational-quantum-algorithms">
<span id="implicit-diff-susceptibility"></span><span id="sphx-glr-demos-tutorial-implicit-diff-susceptibility-py"></span><h1>Implicit differentiation of variational quantum algorithms<a class="headerlink" href="#implicit-differentiation-of-variational-quantum-algorithms" title="Permalink to this headline">¶</a></h1>
<p><script type="text/javascript">
    var related_tutorials = ["tutorial_backprop.html", "tutorial_jax_transformations.html"];
    var related_tutorials_titles = ['Quantum gradients with backpropagation', 'Using JAX with PennyLane'];
</script></p>
<p><em>Authors: Shahnawaz Ahmed and Juan Felipe Carrasquilla Álvarez — Posted: 28 November 2022. Last updated: 28 November 2022.</em></p>
<p>In 1638, René Descartes, intrigued by (then amateur) Pierre de Fermat’s method
of computing tangents, challenged Fermat to find the tangent to
a complicated curve — now called the folium of Descartes:</p>
<div class="math notranslate nohighlight">
\[x^3 + y^3 = 3axy.\]</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/descartes.png"><img alt="Representation of the folium of Descartes" src="../_images/descartes.png" style="width: 460.2px; height: 351.65000000000003px;" /></a>
</div>
<p>With its cubic terms, this curve represents an implicit equation which cannot be
written as a simple expression <span class="math notranslate nohighlight">\(y = f(x)\)</span>. Therefore the task of calculating
the tangent function seemed formidable for the method Descartes had then,
except at the vertex. Fermat successfully provided the tangents at not just the
vertex but at any other point on the curve, baffling Descartes and legitimizing
the intellectual superiority of Fermat. The technique used by Fermat was
<em>implicit differentiation</em> <a class="footnote-reference brackets" href="#paradis2004" id="id1">1</a>. In the above equation, we can begin
by take derivatives on both sides and re-arrange the terms to obtain <span class="math notranslate nohighlight">\(dy/dx\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{dy}{dx} = -\frac{x^2 - ay}{y^2 - ax}.\]</div>
<p>Implicit differentiation can be used to compute gradients of such functions that
cannot be written down explicitly using simple elementary operations. It is a
basic technique of calculus that has recently found many applications in machine
learning — from hyperparameter optimization to the training of neural ordinary
differential equations (ODEs), and it has even led to the definition of a whole new class of architectures,
called Deep Equilibrium Models (DEQs) <a class="footnote-reference brackets" href="#implicitlayers" id="id2">4</a>.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The idea of implicit differentiation can be applied in quantum physics to extend
the power of automatic differentiation to situations where we are not able to
explicitly write down the solution to a problem. As a concrete example, consider
a variational quantum algorithm (VQA) that computes the ground-state solution of
a parameterized Hamiltonian <span class="math notranslate nohighlight">\(H(a)\)</span> using a variational ansatz
<span class="math notranslate nohighlight">\(|\psi_{z}\rangle\)</span>, where <span class="math notranslate nohighlight">\(z\)</span> are the variational parameters. This leads to the solution</p>
<div class="math notranslate nohighlight">
\[z^{*}(a) = \arg\,\min_{z} \langle \psi_{z}|H(a)|\psi_z\rangle.\]</div>
<p>As we change <span class="math notranslate nohighlight">\(H(a)\)</span>, the solution also changes, but we do not obtain an
explicit function for <span class="math notranslate nohighlight">\(z^{*}(a)\)</span>. If we are interested in the properties of the
solution state, we could measure the expectation values of some operator
<span class="math notranslate nohighlight">\(A\)</span> as</p>
<div class="math notranslate nohighlight">
\[\langle A \rangle (a) = \langle \psi_{z^{*}(a)}| A | \psi_{z^{*}(a)}\rangle.\]</div>
<p>With a VQA, we can find a solution to the optimization for a fixed <span class="math notranslate nohighlight">\(H(a)\)</span>.
However, just like with the folium of Descartes, we do not have an explicit solution,
so the gradient <span class="math notranslate nohighlight">\(\partial_a \langle A \rangle (a)\)</span> is not easy to compute.
The solution is only implicitly defined.</p>
<p>Automatic differentiation techniques that construct an explicit computational
graph and differentiate through it by applying the chain rule for gradient
computation cannot be applied here easily. A brute-force application of automatic
differentiation that finds <span class="math notranslate nohighlight">\(z^{*}(a)\)</span> throughout the full optimization
would require us to keep track of all intermediate variables and steps in the optimization
and differentiate through them. This could quickly become computationally
expensive and memory-intensive for quantum algorithms. Implicit differentiation
provides an alternative way to efficiently compute such a gradient.</p>
<p>Similarly, there exist various other
interesting quantities that can be written as gradients of a ground-state solution,
e.g., nuclear forces in quantum chemistry, permanent electric dipolestatic
polarizability, the static hyperpolarizabilities of various orders,
fidelity susceptibilities, and geometric tensors. All such
quantities could possibly be computed using implicit differentiation on quantum
devices. In our recent work we present a unified way to implement such
computations and other applications of implicit differentiation through
variational quantum algorithms <a class="footnote-reference brackets" href="#ahmed2022" id="id3">2</a>.</p>
<p>In this demo we show how implicit gradients can be computed using a variational
algorithm written in <em>PennyLane</em> and powered by a modular implicit differentiation
implementation provided by the tool <em>JAXOpt</em> <a class="footnote-reference brackets" href="#blondel2021" id="id4">3</a>. We compute the generalized
susceptibility for a spin system by using a variational ansatz to obtain a
ground-state and implicitly differentiating through it. In order to compare
the implicit solution, we find the exact ground-state through eigendecomposition
and determine gradients using automatic differentiation. Even though
eigendecomposition may become unfeasible for larger systems, for a small number
of spins, it suffices for a comparison with our implicit differentiation approach.</p>
</div>
<div class="section" id="implicit-differentiation">
<h2>Implicit Differentiation<a class="headerlink" href="#implicit-differentiation" title="Permalink to this headline">¶</a></h2>
<p>We consider the differentiation of a solution of the root-finding problem, defined by</p>
<div class="math notranslate nohighlight">
\[f(z, a) = 0.\]</div>
<p>A function <span class="math notranslate nohighlight">\(z^{*}(a)\)</span> that satisfies <span class="math notranslate nohighlight">\(f(z^{*}(a), a) = 0\)</span> gives a
solution map for fixed values of <span class="math notranslate nohighlight">\(a\)</span>. An explicit analytical solution
is, however, difficult to obtain in general. This means that the direct differentiation of
<span class="math notranslate nohighlight">\(\partial_a z^{*}(a)\)</span> is not always possible. Despite that, some iterative
algorithms may be able to compute the solution by starting from an initial set of values
for <span class="math notranslate nohighlight">\(z\)</span>, e.g., using a fixed-point solver. The optimality condition
<span class="math notranslate nohighlight">\(f(z^{*}(a), a) = 0\)</span> tells the solver when a solution is found.</p>
<p>Implicit differentiation can be used to compute <span class="math notranslate nohighlight">\(\partial_a z^{*}(a)\)</span>
more efficiently than brute-force automatic differentiation, using only the
solution <span class="math notranslate nohighlight">\(z^{*}(a)\)</span> and partial derivatives at the solution point. We do
not have to care about how the solution is obtained and, therefore, do not need
to differentiate through the solution-finding algorithm <a class="footnote-reference brackets" href="#blondel2021" id="id5">3</a>.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Implicit_function_theorem">Implicit function theorem</a>
is a statement about how the set of zeros of a system of equations is locally
given by the graph of a function under some conditions. It can be extended to
the complex domain and we state the theorem (informally) below <a class="footnote-reference brackets" href="#chang2003" id="id6">6</a>.</p>
<div class="topic">
<p class="topic-title">Implicit function theorem (IFT) (informal)</p>
<p>If <span class="math notranslate nohighlight">\(f(z, a)\)</span> is some analytic function where in a local neighbourhood
around <span class="math notranslate nohighlight">\((z_0, a_0)\)</span> we have <span class="math notranslate nohighlight">\(f(z_0, a_0) = 0\)</span>, there exists an
analytic solution <span class="math notranslate nohighlight">\(z^{*}(a)\)</span> that satisfies <span class="math notranslate nohighlight">\(f(z^{*}(a), a) = 0\)</span>.</p>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/implicit_diff.png"><img alt="Graph of the implicit function with its solution." src="../_images/implicit_diff.png" style="width: 451.1px; height: 208.0px;" /></a>
</div>
<p>In the figure above we can see solutions to the optimality condition
<span class="math notranslate nohighlight">\(f(z, a) = 0 \)</span>.
According to the IFT, the solution function is analytic, which means it can be
differentiated at the solution points by simply differentiating
the above equation with respect to <span class="math notranslate nohighlight">\(a\)</span>, as</p>
<div class="math notranslate nohighlight">
\[\partial_a f(z_0, a_0) + \partial_{z} f(z_0, a_0) \partial_{a} z^{*}(a) = 0,\]</div>
<p>which leads to</p>
<div class="math notranslate nohighlight">
\[\partial_{a} z^{*}(a) = - (\partial_{z} f(z_0, a_0) )^{-1}\partial_a f(z_0, a_0).\]</div>
<p>This shows that implicit differentiation can be used in situations where we can
phrase our optimization problem in terms of an optimality condition
or a fixed point equation that can be solved. In case of optimization tasks,
such an optimality condition would be that, at the minima, the gradient of the
cost function is zero — i.e.,</p>
<div class="math notranslate nohighlight">
\[f(z, a) = \partial_z g(z, a) = 0.\]</div>
<p>Then, as long as we have the solution, <span class="math notranslate nohighlight">\(z^{*}(a)\)</span>, and the partial derivatives at the solution (in this case the Hessian of the
cost function <span class="math notranslate nohighlight">\(g(z, a)\)</span>), <span class="math notranslate nohighlight">\((\partial_a f, \partial_z f)\)</span>, we can compute implicit gradients. Note that,
for a multivariate function, the inversion
<span class="math notranslate nohighlight">\((\partial_{z} f(z_0, a_0) )^{-1}\)</span> needs to be defined and easy to
compute. It is possible to approximate this inversion in a clever way by constructing
a linear problem that can be solved approximately <a class="footnote-reference brackets" href="#blondel2021" id="id7">3</a>, <a class="footnote-reference brackets" href="#implicitlayers" id="id8">4</a>.</p>
</div>
<div class="section" id="implicit-differentiation-through-a-variational-quantum-algorithm">
<h2>Implicit differentiation through a variational quantum algorithm<a class="headerlink" href="#implicit-differentiation-through-a-variational-quantum-algorithm" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/VQA.png"><img alt="Application of implicit differentiation in variational quantum algorithm." src="../_images/VQA.png" style="width: 439.40000000000003px; height: 356.85px;" /></a>
</div>
<p>We now discuss how the idea of implicit differentiation can be applied to
variational quantum algorithms. Let us take a parameterized Hamiltonian
<span class="math notranslate nohighlight">\(H(a)\)</span>, where <span class="math notranslate nohighlight">\(a\)</span> is a parameter that can be continuously varied.
If <span class="math notranslate nohighlight">\(|\psi_{z}\rangle\)</span> is a variational solution to the ground state of <span class="math notranslate nohighlight">\(H(a)\)</span>,
then we can find a <span class="math notranslate nohighlight">\(z^*(a)\)</span> that minimizes the ground state energy, i.e.,</p>
<div class="math notranslate nohighlight">
\[z^*(a) = \arg\, \min_{z} \langle \psi_{z}| H(a) | \psi_{z}\rangle = \arg \min_{z} E(z, a),\]</div>
<p>where <span class="math notranslate nohighlight">\(E(z, a)\)</span> is the energy function. We consider the following
Hamiltonian</p>
<div class="math notranslate nohighlight">
\[H(a) = -J \sum_{i} \sigma^{z}_i \sigma^{z}_{i+1} - \gamma \sum_{i} \sigma^{x}_i + \delta \sum_{i} \sigma^z_i - a A,\]</div>
<p>where <span class="math notranslate nohighlight">\(J\)</span> is the interaction, <span class="math notranslate nohighlight">\(\sigma^{x}\)</span> and <span class="math notranslate nohighlight">\(\sigma^{z}\)</span>
are the spin-<span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> operators and <span class="math notranslate nohighlight">\(\gamma\)</span> is the magnetic
field strength (which is taken to be the same for all spins).
The term <span class="math notranslate nohighlight">\(A = \frac{1}{N}\sum_i \sigma^{z}_i\)</span>
is the magnetization and a small non-zero magnetization <span class="math notranslate nohighlight">\(\delta\)</span> is added
for numerical stability. We have assumed a circular chain such that in the
interaction term the last spin (<span class="math notranslate nohighlight">\(i = N-1\)</span>) interacts with the first
(<span class="math notranslate nohighlight">\(i=0\)</span>).</p>
<p>Now we could find the ground state of this Hamiltonian by simply taking the
eigendecomposition and applying automatic differentiation through the
eigendecomposition to compute gradients. We will compare this exact computation
for a small system to the gradients given by implicit differentiation through a
variationally obtained solution.</p>
<p>We define the following optimality condition at the solution point:</p>
<div class="math notranslate nohighlight">
\[f(z, a) = \partial_z E(z, a) = 0.\]</div>
<p>In addition, if the conditions of the implicit function theorem
are also satisfied, i.e., <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable
with a non-singular Jacobian at the solution, then we can apply the chain rule
and determine the implicit gradients easily.</p>
<p>At this stage, any other complicated function that depends on the variational ground state
can be easily differentiated using automatic differentiation by plugging in the
value of <span class="math notranslate nohighlight">\(partial_a z^{*}(a)\)</span> where it is required. The
expectation value of the operator <span class="math notranslate nohighlight">\(A\)</span> for the ground state
is</p>
<div class="math notranslate nohighlight">
\[\langle A\rangle = \langle \psi_{z^*}| A| \psi_{z^*}\rangle.\]</div>
<p>In the case where <span class="math notranslate nohighlight">\(A\)</span> is just the energy, i.e., <span class="math notranslate nohighlight">\(A = H(a)\)</span>, the
Hellmann–Feynman theorem allows us to easily compute the gradient. However, for
a general operator we need the gradients <span class="math notranslate nohighlight">\(\partial_a z^{*}(a)\)</span>, which means that implicit differentiation is a very elegant way to go beyond the
Hellmann–Feynman theorem for arbitrary expectation values.</p>
<p>Let us now dive into the code and implementation.</p>
<p><em>Talk is cheap. Show me the code.</em> - Linus Torvalds</p>
</div>
<div class="section" id="implicit-differentiation-of-ground-states-in-pennylane">
<h2>Implicit differentiation of ground states in PennyLane<a class="headerlink" href="#implicit-differentiation-of-ground-states-in-pennylane" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>

<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax.config</span> <span class="kn">import</span> <span class="n">config</span>

<span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">jaxopt</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">jax</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s1">&#39;jax_platform_name&#39;</span><span class="p">,</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="c1"># Use double precision numbers</span>
<span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;jax_enable_x64&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="defining-the-hamiltonian-and-measurement-operator">
<h3>Defining the Hamiltonian and measurement operator<a class="headerlink" href="#defining-the-hamiltonian-and-measurement-operator" title="Permalink to this headline">¶</a></h3>
<p>We define the Hamiltonian by building the non-parametric part separately and
adding the parametric part to it as a separate term for efficiency. Note that, for
the example of generalized susceptibility, we are measuring expectation values
of the operator <span class="math notranslate nohighlight">\(A\)</span> that also defines the parametric part of the
Hamiltonian. However, this is not necessary and we could compute gradients for
any other operator using implicit differentiation, as we have access to the
gradients <span class="math notranslate nohighlight">\(\partial_a z^{*}(a)\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">J</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="k">def</span> <span class="nf">build_H0</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Builds the non-parametric part of the Hamiltonian of a spin system.</span>

<span class="sd">    Args:</span>
<span class="sd">        N (int): Number of qubits/spins.</span>
<span class="sd">        J (float): Interaction strength.</span>
<span class="sd">        gamma (float): Interaction strength.</span>

<span class="sd">    Returns:</span>
<span class="sd">        qml.Hamiltonian: The Hamiltonian of the system.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">H</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">Hamiltonian</span></a><span class="p">([],</span> <span class="p">[])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">H</span> <span class="o">+=</span> <span class="o">-</span><span class="n">J</span> <span class="o">*</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">@</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">H</span> <span class="o">+=</span> <span class="o">-</span><span class="n">J</span> <span class="o">*</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Transverse</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">H</span> <span class="o">+=</span> <span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliX.html#pennylane.PauliX" title="pennylane.PauliX" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliX</span></a><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="c1"># Small magnetization for numerical stability</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">H</span> <span class="o">+=</span> <span class="o">-</span><span class="mf">1e-1</span> <span class="o">*</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">H</span>

<a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">H0</span></a> <span class="o">=</span> <span class="n">build_H0</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">J</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
<span class="n">H0_matrix</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.op_transform.html#pennylane.op_transform" title="pennylane.op_transform" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">matrix</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">H0</span></a><span class="p">)</span>
<a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">A</span></a> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="n">add</span><span class="p">,</span> <span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span> <span class="o">*</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)))</span>
<span class="n">A_matrix</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.op_transform.html#pennylane.op_transform" title="pennylane.op_transform" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">matrix</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">A</span></a><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="computing-the-exact-ground-state-through-eigendecomposition">
<h3>Computing the exact ground state through eigendecomposition<a class="headerlink" href="#computing-the-exact-ground-state-through-eigendecomposition" title="Permalink to this headline">¶</a></h3>
<p>We now define a function that computes the exact ground state using
eigendecomposition. Ideally, we would like to take gradients of this function.
It is possible to simply apply automatic differentiation through this exact
ground-state computation. JAX has an implementation of differentiation
through eigendecomposition.</p>
<p>Note that we have some points in this plot that are <code class="docutils literal notranslate"><span class="pre">nan</span></code>, where the gradient
computation through the eigendecomposition does not work. We will see later that
the computation through the VQA is more stable.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">ground_state_solution_map_exact</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The ground state solution map that we want to differentiate</span>
<span class="sd">    through, computed from an eigendecomposition.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (float): The parameter in the Hamiltonian, H(a).</span>

<span class="sd">    Returns:</span>
<span class="sd">        jnp.array: The ground state solution for the H(a).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">H0_matrix</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="n">A_matrix</span>
    <span class="nb">eval</span><span class="p">,</span> <span class="n">eigenstates</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
    <span class="n">z_star</span> <span class="o">=</span> <span class="n">eigenstates</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">z_star</span>


<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="n">z_star_exact</span> <span class="o">=</span> <span class="n">ground_state_solution_map_exact</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="susceptibility-computation-through-the-ground-state-solution-map">
<h3>Susceptibility computation through the ground state solution map<a class="headerlink" href="#susceptibility-computation-through-the-ground-state-solution-map" title="Permalink to this headline">¶</a></h3>
<p>Let us now compute the susceptibility function by taking gradients of the
expectation value of our operator <span class="math notranslate nohighlight">\(A\)</span> w.r.t <cite>a</cite>. We can use <cite>jax.vmap</cite>
to vectorize the computation over different values of <cite>a</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">expval_A_exact</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expectation value of ``A`` as a function of ``a`` where we use the</span>
<span class="sd">    ``ground_state_solution_map_exact`` function to find the ground state.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (float): The parameter defining the Hamiltonian, H(a).</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The expectation value of A calculated using the variational state</span>
<span class="sd">               that should be the ground state of H(a).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z_star</span> <span class="o">=</span> <span class="n">ground_state_solution_map_exact</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">z_star</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">A_matrix</span> <span class="o">@</span> <span class="n">z_star</span>
    <span class="k">return</span> <span class="nb">eval</span><span class="o">.</span><span class="n">real</span>

<span class="c1"># the susceptibility is the gradient of the expectation value</span>
<span class="n">_susceptibility_exact</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">expval_A_exact</span><span class="p">)</span>
<span class="n">susceptibility_exact</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">vmap</span><span class="p">(</span><span class="n">_susceptibility_exact</span><span class="p">)</span>

<span class="n">alist</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">susvals_exact</span> <span class="o">=</span> <span class="n">susceptibility_exact</span><span class="p">(</span><span class="n">alist</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alist</span><span class="p">,</span> <span class="n">susvals_exact</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\partial_</span><span class="si">{a}</span><span class="s2">\langle A \rangle$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_tutorial_implicit_diff_susceptibility_001.png" srcset="../_images/sphx_glr_tutorial_implicit_diff_susceptibility_001.png" alt="tutorial implicit diff susceptibility" class = "sphx-glr-single-img"/></div>
<div class="section" id="computing-susceptibility-through-implicit-differentiation">
<h3>Computing susceptibility through implicit differentiation<a class="headerlink" href="#computing-susceptibility-through-implicit-differentiation" title="Permalink to this headline">¶</a></h3>
<p>We use PennyLane to find a variational ground state for the Hamiltonian
<span class="math notranslate nohighlight">\(H(a)\)</span> and compute implicit gradients through the variational
optimization procedure. We use the <code class="docutils literal notranslate"><span class="pre">jaxopt</span></code> library which contains an
implementation of gradient descent that automatically comes with implicit
differentiation capabilities. We are going to use that to obtain
susceptibility by taking gradients through the ground-state minimization.</p>
</div>
<div class="section" id="defining-the-variational-state">
<h3>Defining the variational state<a class="headerlink" href="#defining-the-variational-state" title="Permalink to this headline">¶</a></h3>
<p>In PennyLane, we can implement a variational state in different ways, by
defining a quantum circuit. There are also useful template circuits available, such as
<a class="reference external" href="https://docs.pennylane.ai/en/stable/code/api/pennylane.SimplifiedTwoDesign.html#pennylane.SimplifiedTwoDesign" title="(in PennyLane v0.30)"><code class="xref py py-class docutils literal notranslate"><span class="pre">SimplifiedTwoDesign</span></code></a>, which implements the <a class="reference internal" href="tutorial_unitary_designs.html"><span class="doc">two-design ansatz</span></a>.
The ansatz consists of layers of Pauli-Y rotations with
controlled-Z gates. In each layer there are <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">-</span> <span class="pre">1</span></code> parameters for the Pauli-Y gates.
Therefore, the ansatz is efficient as long as we have enough layers for it
so that is expressive enough to represent the ground-state.</p>
<p>We set <code class="docutils literal notranslate"><span class="pre">n_layers</span> <span class="pre">=</span> <span class="pre">5</span></code>, but you can redo this example with fewer layers to see
how a less expressive ansatz leads to error in the susceptibility computation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The setting <code class="docutils literal notranslate"><span class="pre">shots=None</span></code> makes for the computation of gradients using reverse-mode
autodifferentiation (backpropagation). It allows us to just-in-time (JIT)
compile the functions that compute expectation values and gradients.
In a real device we would use a finite number of shots and the gradients would be computed
using the parameter-shift rule. However, this may be slower.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.SimplifiedTwoDesign.html#pennylane.SimplifiedTwoDesign" title="pennylane.SimplifiedTwoDesign" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">variational_ansatz</span></a> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.SimplifiedTwoDesign.html#pennylane.SimplifiedTwoDesign" title="pennylane.SimplifiedTwoDesign" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">SimplifiedTwoDesign</span></a>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">weights_shape</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.SimplifiedTwoDesign.html#pennylane.SimplifiedTwoDesign" title="pennylane.SimplifiedTwoDesign" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">variational_ansatz</span></a><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.devices.default_qubit_jax.DefaultQubitJax.html#pennylane.devices.default_qubit_jax.DefaultQubitJax" title="pennylane.devices.default_qubit_jax.DefaultQubitJax" class="sphx-glr-backref-module-pennylane-devices-default_qubit_jax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dev</span></a> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.device.html#pennylane.device" title="pennylane.device" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;default.qubit.jax&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">shots</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.devices.default_qubit_jax.DefaultQubitJax.html#pennylane.devices.default_qubit_jax.DefaultQubitJax" title="pennylane.devices.default_qubit_jax.DefaultQubitJax" class="sphx-glr-backref-module-pennylane-devices-default_qubit_jax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dev</span></a><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="s2">&quot;jax&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">energy</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes the energy for a Hamiltonian H(a) using a measurement on the</span>
<span class="sd">    variational state U(z)|0&gt; with U(z) being any circuit ansatz.</span>

<span class="sd">    Args:</span>
<span class="sd">        z (jnp.array): The variational parameters for the ansatz (circuit)</span>
<span class="sd">        a (jnp.array): The Hamiltonian parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The expectation value (energy).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.SimplifiedTwoDesign.html#pennylane.SimplifiedTwoDesign" title="pennylane.SimplifiedTwoDesign" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">variational_ansatz</span></a><span class="p">(</span><span class="o">*</span><span class="n">z</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
    <span class="c1"># here we compute the Hamiltonian coefficients and operations</span>
    <span class="c1"># &#39;by hand&#39; because the qml.Hamiltonian class does not support</span>
    <span class="c1"># operator arithmetic with JAX device arrays.</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian.coeffs" title="pennylane.Hamiltonian.coeffs" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-attribute"><span class="n">H0</span><span class="o">.</span><span class="n">coeffs</span></a><span class="p">,</span> <span class="n">a</span> <span class="o">*</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian.coeffs" title="pennylane.Hamiltonian.coeffs" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-attribute"><span class="n">A</span><span class="o">.</span><span class="n">coeffs</span></a><span class="p">])</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.expval.html#pennylane.expval" title="pennylane.expval" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">expval</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">Hamiltonian</span></a><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian.ops" title="pennylane.Hamiltonian.ops" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-attribute"><span class="n">H0</span><span class="o">.</span><span class="n">ops</span></a> <span class="o">+</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian.ops" title="pennylane.Hamiltonian.ops" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-attribute"><span class="n">A</span><span class="o">.</span><span class="n">ops</span></a><span class="p">))</span>


<span class="n">z_init</span> <span class="o">=</span> <span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">weights_shape</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Energy&quot;</span><span class="p">,</span> <span class="n">energy</span><span class="p">(</span><span class="n">z_init</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Energy 0.06544676656241591
</pre></div>
</div>
</div>
<div class="section" id="computing-ground-states-using-a-variational-quantum-algorithm-vqa">
<h3>Computing ground states using a variational quantum algorithm (VQA)<a class="headerlink" href="#computing-ground-states-using-a-variational-quantum-algorithm-vqa" title="Permalink to this headline">¶</a></h3>
<p>We construct a loss function that defines a ground-state minimization
task. We are looking for variational parameters <code class="docutils literal notranslate"><span class="pre">z</span></code> that minimize the energy
function. Once we find a set of parameters <code class="docutils literal notranslate"><span class="pre">z</span></code>, we wish to compute the
gradient of any function of the ground state w.r.t. <code class="docutils literal notranslate"><span class="pre">a</span></code>.</p>
<p>For the implicit differentiation we will use the tool <code class="docutils literal notranslate"><span class="pre">jaxopt</span></code>, which implements
modular implicit differentiation for various cases; e.g., for fixed-point
functions or optimization. We can directly use <code class="docutils literal notranslate"><span class="pre">jaxopt</span></code> to optimize our loss
function and then compute implicit gradients through it.
It all works due to <a class="reference internal" href="tutorial_jax_transformations.html"><span class="doc">PennyLane’s integration with JAX</span></a>.</p>
<p>The implicit differentiation formulas can even be <a class="reference external" href="https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#implicit-function-differentiation-of-iterative-implementations">implemented manually with JAX</a>.
These formulas are implemented in a modular way, using the
<code class="docutils literal notranslate"><span class="pre">jaxopt.GradientDescent</span></code> optimizer with <code class="docutils literal notranslate"><span class="pre">implicit_diff=True</span></code>.
We use the seamless integration between PennyLane, JAX
and JAXOpt to compute the susceptibility.</p>
<p>Since everything is written in JAX, simply calling the
<code class="docutils literal notranslate"><span class="pre">jax.grad</span></code> function works as <code class="docutils literal notranslate"><span class="pre">jaxopt</span></code> computes the implicit gradients and
plugs it any computation used by <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>. We can also just-in-time (JIT)
compile all functions although the compilation may take some time as the
number of spins or variational ansatz becomes more complicated. Once compiled,
all computes run very fast for any parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ground_state_solution_map_variational</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">z_init</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The ground state solution map that we want to differentiate</span>
<span class="sd">    through.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (float): The parameter in the Hamiltonian, H(a).</span>
<span class="sd">        z_init [jnp.array(jnp.float)]: The initial guess for the variational</span>
<span class="sd">                                       parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        z_star (jnp.array [jnp.float]): The parameters that define the</span>
<span class="sd">                                        ground-state solution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loss function for the ground-state minimization with regularization.</span>

<span class="sd">        Args:</span>
<span class="sd">            z (jnp.array): The variational parameters for the ansatz (circuit)</span>
<span class="sd">            a (jnp.array): The Hamiltonian parameters.</span>

<span class="sd">        Returns:</span>
<span class="sd">            float: The loss value (energy + regularization)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">energy</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="p">)</span>
    <span class="n">gd</span> <span class="o">=</span> <span class="n">jaxopt</span><span class="o">.</span><span class="n">GradientDescent</span><span class="p">(</span>
        <span class="n">fun</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
        <span class="n">acceleration</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">maxiter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">implicit_diff</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">z_star</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">z_init</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">params</span>
    <span class="k">return</span> <span class="n">z_star</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>  <span class="c1"># A random ``a``</span>
<span class="n">z_star_variational</span> <span class="o">=</span> <span class="n">ground_state_solution_map_variational</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">z_init</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="computing-gradients-through-the-vqa-simply-by-calling-jax-grad">
<h3>Computing gradients through the VQA simply by calling <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code><a class="headerlink" href="#computing-gradients-through-the-vqa-simply-by-calling-jax-grad" title="Permalink to this headline">¶</a></h3>
<p>We can compute the susceptibility values by simply using <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code>. After the
first call, the function is compiled and subsequent calls become
much faster.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.devices.default_qubit_jax.DefaultQubitJax.html#pennylane.devices.default_qubit_jax.DefaultQubitJax" title="pennylane.devices.default_qubit_jax.DefaultQubitJax" class="sphx-glr-backref-module-pennylane-devices-default_qubit_jax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dev</span></a><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="s2">&quot;jax&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">expval_A_variational</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expectation value of $A$ as a function of $a$ where we use the</span>
<span class="sd">    a variational ground state solution map.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (float): The parameter in the Hamiltonian, H(a).</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The expectation value of M on the ground state of H(a)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.SimplifiedTwoDesign.html#pennylane.SimplifiedTwoDesign" title="pennylane.SimplifiedTwoDesign" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">variational_ansatz</span></a><span class="p">(</span><span class="o">*</span><span class="n">z</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.expval.html#pennylane.expval" title="pennylane.expval" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">expval</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hamiltonian.html#pennylane.Hamiltonian" title="pennylane.Hamiltonian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">A</span></a><span class="p">)</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">groundstate_expval_variational</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">z_init</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes ground state and calculates the expectation value of the operator M.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (float): The parameter in the Hamiltonian, H(a).</span>
<span class="sd">        z_init [jnp.array(jnp.float)]: The initial guess for the variational parameters.</span>
<span class="sd">        H0 (qml.Hamiltonian): The static part of the Hamiltonian</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z_star</span> <span class="o">=</span> <span class="n">ground_state_solution_map_variational</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">z_init</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">expval_A_variational</span><span class="p">(</span><span class="n">z_star</span><span class="p">)</span>

<span class="n">susceptibility_variational</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">groundstate_expval_variational</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">z_init</span> <span class="o">=</span> <span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">weights_shape</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Susceptibility&quot;</span><span class="p">,</span> <span class="n">susceptibility_variational</span><span class="p">(</span><span class="n">alist</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">z_init</span><span class="p">))</span>

<span class="n">susvals_variational</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alist</span><span class="p">)):</span>
    <span class="n">susvals_variational</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">susceptibility_variational</span><span class="p">(</span><span class="n">alist</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">z_init</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alist</span><span class="p">,</span> <span class="n">susvals_variational</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Implicit diff through VQA&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alist</span><span class="p">,</span> <span class="n">susvals_exact</span><span class="p">,</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Automatic diff through eigendecomposition&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\partial_</span><span class="si">{a}</span><span class="s2">\langle A \rangle$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_tutorial_implicit_diff_susceptibility_002.png" srcset="../_images/sphx_glr_tutorial_implicit_diff_susceptibility_002.png" alt="tutorial implicit diff susceptibility" class = "sphx-glr-single-img"/><p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Susceptibility -0.5149512273643617
</pre></div>
</div>
<p>PennyLane version and details</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.about.html#pennylane.about" title="pennylane.about" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">about</span></a><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Name: PennyLane
Version: 0.30.0
Summary: PennyLane is a Python quantum machine learning library by Xanadu Inc.
Home-page: https://github.com/XanaduAI/pennylane
Author:
Author-email:
License: Apache License 2.0
Location: /home/runner/work/qml/qml/venv/lib/python3.9/site-packages
Requires: appdirs, autograd, autoray, cachetools, networkx, numpy, pennylane-lightning, requests, rustworkx, scipy, semantic-version, toml
Required-by: PennyLane-Cirq, PennyLane-Lightning, PennyLane-qiskit, pennylane-qulacs

Platform info:           Linux-5.15.0-1039-azure-x86_64-with-glibc2.35
Python version:          3.9.17
Numpy version:           1.23.5
Scipy version:           1.10.1
Installed devices:
- lightning.qubit (PennyLane-Lightning-0.31.0.dev6)
- qiskit.aer (PennyLane-qiskit-0.30.1)
- qiskit.basicaer (PennyLane-qiskit-0.30.1)
- qiskit.ibmq (PennyLane-qiskit-0.30.1)
- qiskit.ibmq.circuit_runner (PennyLane-qiskit-0.30.1)
- qiskit.ibmq.sampler (PennyLane-qiskit-0.30.1)
- default.gaussian (PennyLane-0.30.0)
- default.mixed (PennyLane-0.30.0)
- default.qubit (PennyLane-0.30.0)
- default.qubit.autograd (PennyLane-0.30.0)
- default.qubit.jax (PennyLane-0.30.0)
- default.qubit.tf (PennyLane-0.30.0)
- default.qubit.torch (PennyLane-0.30.0)
- default.qutrit (PennyLane-0.30.0)
- null.qubit (PennyLane-0.30.0)
- qulacs.simulator (pennylane-qulacs-0.29.0)
- cirq.mixedsimulator (PennyLane-Cirq-0.29.0)
- cirq.pasqal (PennyLane-Cirq-0.29.0)
- cirq.qsim (PennyLane-Cirq-0.29.0)
- cirq.qsimh (PennyLane-Cirq-0.29.0)
- cirq.simulator (PennyLane-Cirq-0.29.0)
None
</pre></div>
</div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>We have shown how a combination of JAX, PennyLane and JAXOpt can be used to
compute implicit gradients through a VQA. The ability to compute such
gradients opens up new possibilities, e.g., the design of a Hamiltonian such that
its ground-state has certain properties. It is also possible to perhaps look
at this inverse-design of the Hamiltonian as a control problem. Implicit
differentiation in the classical setting allows defining a new type of
neural network layer — implicit layers such as neural ODEs. In a similar
way, we hope this demo the inspires creation of new architectures for quantum
neural networks, perhaps a quantum version of neural ODEs or quantum implicit
layers.</p>
<p>In future works, it would be important to assess the cost of running implicit
differentiation through an actual quantum computer and determine the quality
of such gradients as a function of noise as explored in a related recent work
<a class="footnote-reference brackets" href="#matteo2021" id="id9">5</a>.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="paradis2004"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Jaume Paradís, Josep Pla &amp; Pelegrí Viader
“Fermat and the Quadrature of the Folium of Descartes”
The American Mathematical Monthly, 111:3, 216-229
<a class="reference external" href="https://doi.org/10.1080/00029890.2004.11920067">10.1080/00029890.2004.11920067</a>, 2004.</p>
</dd>
<dt class="label" id="ahmed2022"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Shahnawaz Ahmed, Nathan Killoran, Juan Felipe Carrasquilla Álvarez
“Implicit differentiation of variational quantum algorithms
<a class="reference external" href="https://arxiv.org/abs/2211.13765">arXiv:2211.13765</a>, 2022.</p>
</dd>
<dt class="label" id="blondel2021"><span class="brackets">3</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>,<a href="#id7">3</a>)</span></dt>
<dd><p>Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-López, Fabian Pedregosa, Jean-Philippe Vert
“Efficient and modular implicit differentiation”
<a class="reference external" href="https://arxiv.org/abs/2105.15183">arXiv:2105.15183</a>, 2021.</p>
</dd>
<dt class="label" id="implicitlayers"><span class="brackets">4</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Zico Kolter, David Duvenaud, Matt Johnson.
“Deep Implicit Layers - Neural ODEs, Deep Equilibirum Models, and Beyond”
<a class="reference external" href="http://implicit-layers-tutorial.org">http://implicit-layers-tutorial.org</a>, 2021.</p>
</dd>
<dt class="label" id="matteo2021"><span class="brackets"><a class="fn-backref" href="#id9">5</a></span></dt>
<dd><p>Olivia Di Matteo, R. M. Woloshyn
“Quantum computing fidelity susceptibility using automatic differentiation”
<a class="reference external" href="https://arxiv.org/abs/2207.06526">arXiv:2207.06526</a>, 2022.</p>
</dd>
<dt class="label" id="chang2003"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Chang, Hung-Chieh, Wei He, and Nagabhushana Prabhu.
“The analytic domain in the implicit function theorem.”
<a class="reference external" href="http://emis.icm.edu.pl/journals/JIPAM/v4n1/061_02_www.pdf">JIPAM. J. Inequal. Pure Appl. Math 4.1</a>,  (2003).</p>
</dd>
</dl>
</div>
<div class="section" id="about-the-authors">
<h2>About the authors<a class="headerlink" href="#about-the-authors" title="Permalink to this headline">¶</a></h2>
<div class="bio" >
    <div class="photo" >
        <img class="photo__img" src="../_static/authors/shahnawaz_ahmed.png" alt="Shahnawaz Ahmed" >
    </div>
    <div class="bio-text">
        <h4 class="bio-text__author-name">Shahnawaz Ahmed</h4>
        <p class="bio-text__author-description">Shahnawaz works on developing useful machine learning techniques for scientific applications with a focus on quantum information and computing.</p>
    </div>
</div><div class="bio" >
    <div class="photo" >
        <img class="photo__img" src="../_static/authors/juan_felipe_carrasquilla_alvarez.png" alt="Juan Felipe Carrasquilla Alvarez" >
    </div>
    <div class="bio-text">
        <h4 class="bio-text__author-name">Juan Felipe Carrasquilla Alvarez</h4>
        <p class="bio-text__author-description">Juan is a physicist interested in ideas at the intersection of condensed matter theory, artificial intelligence, and quantum computing.</p>
    </div>
</div><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 5 minutes  28.919 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-demos-tutorial-implicit-diff-susceptibility-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/ff0c5c82fe46f3970a594c53c23ac6a6/tutorial_implicit_diff_susceptibility.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tutorial_implicit_diff_susceptibility.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/6d4281ed1e08834b65b1a2cbc570966f/tutorial_implicit_diff_susceptibility.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tutorial_implicit_diff_susceptibility.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


    <script type="text/javascript">
        // This script ensures that the active navbar entry switches
        // from 'QML' to 'Demos' for any webpage within the demos/ directory,
        // or for any of the demonstration landing pages
        // (e.g., demos_optimization).
        var pagename = document.location.href.match(/[^\/]+$/)[0];
        var dir = document.URL.substr(0,document.URL.lastIndexOf('/')).match(/[^\/]+$/)[0];

        if (pagename.includes("demos") || pagename.includes("demonstrations") || dir.includes("demos")) {

            $(".nav-item.active").removeClass("active");
            var demos_link = $('.navbar-nav a').filter(function(index) { return $(this).text() === "Demos"; })[0]
            $(demos_link).parent().addClass("active");
        }
    </script>

              <div id="bottom-dl" class="xanadu-call-to-action-links">
                <div id="tutorial-type">demos/tutorial_implicit_diff_susceptibility</div>
                <div class="download-python-link">
                  <i class="fab fa-python"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Python script</div>
                </div>
                <div class="download-notebook-link">
                  <i class="fas fa-download"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Notebook</div>
                </div>
                <div class="github-view-link">
                  <i class="fab fa-github"></i>&nbsp;
                  <div class="call-to-action-desktop-view">View on GitHub</div>
                </div>
              </div>

            </div>
            
          </div>
        
<div class="localtoc-container nano has-scrollbar">
  <div class="nano-content">
    <div id="localtoc">
        
          <h3>Contents</h3>
          <!-- Display the ToC for the current document if it is not empty. -->
          <ul class='current'>
<li class='current'><a class="reference internal" href="#">Implicit differentiation of variational quantum algorithms</a><ul class='current'>
<li class='current'><a class="reference internal" href="#introduction">Introduction</a></li>
<li class='current'><a class="reference internal" href="#implicit-differentiation">Implicit Differentiation</a></li>
<li class='current'><a class="reference internal" href="#implicit-differentiation-through-a-variational-quantum-algorithm">Implicit differentiation through a variational quantum algorithm</a></li>
<li class='current'><a class="reference internal" href="#implicit-differentiation-of-ground-states-in-pennylane">Implicit differentiation of ground states in PennyLane</a><ul class='current'>
<li class='current'><a class="reference internal" href="#defining-the-hamiltonian-and-measurement-operator">Defining the Hamiltonian and measurement operator</a></li>
<li class='current'><a class="reference internal" href="#computing-the-exact-ground-state-through-eigendecomposition">Computing the exact ground state through eigendecomposition</a></li>
<li class='current'><a class="reference internal" href="#susceptibility-computation-through-the-ground-state-solution-map">Susceptibility computation through the ground state solution map</a></li>
<li class='current'><a class="reference internal" href="#computing-susceptibility-through-implicit-differentiation">Computing susceptibility through implicit differentiation</a></li>
<li class='current'><a class="reference internal" href="#defining-the-variational-state">Defining the variational state</a></li>
<li class='current'><a class="reference internal" href="#computing-ground-states-using-a-variational-quantum-algorithm-vqa">Computing ground states using a variational quantum algorithm (VQA)</a></li>
<li class='current'><a class="reference internal" href="#computing-gradients-through-the-vqa-simply-by-calling-jax-grad">Computing gradients through the VQA simply by calling <code class="docutils literal notranslate"><span class="pre">jax.grad</span></code></a></li>
</ul>
</li>
<li class='current'><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class='current'><a class="reference internal" href="#references">References</a></li>
<li class='current'><a class="reference internal" href="#about-the-authors">About the authors</a></li>
</ul>
</li>
</ul>

        
    </div>

    <div class="xanadu-call-to-action-links">
        <h3>Downloads</h3>
        <div id="tutorial-type">demos/tutorial_implicit_diff_susceptibility</div>
        <div class="download-python-link">
            <i class="fab fa-python"></i>&nbsp;
            <div class="call-to-action-desktop-view">Download Python script</div>
        </div>
        <div class="download-notebook-link">
            <i class="fas fa-download"></i>&nbsp;
            <div class="call-to-action-desktop-view">Download Notebook</div>
        </div>
        <div class="github-view-link">
            <i class="fab fa-github"></i>&nbsp;
            <div class="call-to-action-desktop-view">View on GitHub</div>
        </div>
    </div>
    <div id="related-tutorials" class="mt-4">
      <h3> Related</h3>
    </div>
  </div>
</div>


    
          <div class="up-button">
            
              
                <a href="../demos_optimization.html"><i class="fas fa-angle-double-left"></i></a>
              
            
          </div>

          <div class="clearfix"></div>
        </div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="tutorial_barren_gadgets.html" title="Perturbative Gadgets for Variational Quantum Algorithms"
             >next</a> |</li>
        <li class="right" >
          <a href="tutorial_diffable-mitigation.html" title="Differentiating quantum error mitigation transforms"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../quantum-computing.html" >Quantum Computing</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="../demos_optimization.html" >Optimization</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Implicit differentiation of variational quantum algorithms</a></li> 
      </ul>
    </div>
  <script type="text/javascript">
    $("#mobile-toggle").click(function () {
      $("#left-column").slideToggle("slow");
    });
  </script>

  <!-- jQuery -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>
  <!-- MathJax -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- Bootstrap core JavaScript -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <!-- MDB core JavaScript -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.8.10/js/mdb.min.js"></script>
  <!-- NanoScroller -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery.nanoscroller/0.8.7/javascripts/jquery.nanoscroller.min.js"></script>
  <!-- Syntax Highlighting -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
  <script type="text/javascript">hljs.initHighlightingOnLoad();</script>

  <script type="text/javascript">
    $("a.reference.internal").each(function(){
      var link = $(this).attr("href");

      var hash = link.split("#")[1];
      var page = link.split("#")[0].split("/").slice(-1)[0].replace(".html", "");

      if (hash == page) {
        $(this).attr("href", link.split("#")[0]);
      }
    });

    $(".document > .section").removeClass("section");
    $("h1 ~ .section").removeClass("section");
    $(".localtoc-container .nano-content").css("height", $("#content").height());
    $(".localtoc-container").css("height", $("#content").height());
    $(".nano").nanoScroller();
  </script>

  <script type="text/javascript">
      $(window).scroll(function(){
        var scrollBottom = $(document).height() - $(window).height() - $(window).scrollTop();
        if (scrollBottom < 342) {
          $(".localtoc-container").css("height", "calc(100% - " + (342 - scrollBottom) + "px)");
          $(".localtoc-container .nano-content").css("height", "calc(100% - 119px)");
        }
      });
  </script>

  <script type="text/javascript">
    if ($(".current").length) {
      var target = $(".current")[0]
      var rect = target.getBoundingClientRect();
      if (rect.bottom > window.innerHeight) {
          $(".nano").nanoScroller({ scrollTo: $(".current") });
      } else {
          $(".nano").nanoScroller({ scrollTop: 0 });
      }
    }
    $(document).ready(function () {
        $(".css-transitions-only-after-page-load").each(function (index, element) {
            setTimeout(function () { $(element).removeClass("css-transitions-only-after-page-load") }, 10);
        });
        if (window.location.hash) {
          var target = $("[id='" + window.location.hash.substr(1) + "']");
          if (target.closest(".collapse").length) {
            target.closest(".collapse").addClass("show");
            target.closest(".collapse").prev().find(".rotate").addClass("up");
          }
        }
    });
  </script>

    <script type="text/javascript">
    var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
    if (downloadNote.length >= 1) {
      var tutorialUrlArray = $("#tutorial-type").text().split('/');

      if (tutorialUrlArray[0] == "demos") {
        tutorialUrlArray[0] = "demonstrations";
      }

      var githubLink = "https://github.com/" + "PennyLaneAI/qml" + "/blob/master/" + tutorialUrlArray.join("/") + ".py",
          pythonLink = $(".sphx-glr-download .reference.download")[0].href,
          notebookLink = $(".sphx-glr-download .reference.download")[1].href;

      $(".download-python-link").wrap("<a href=" + pythonLink + " data-behavior='call-to-action-event' data-response='Download Python script' download target='_blank'/>");
      $(".download-notebook-link").wrap("<a href=" + notebookLink + " data-behavior='call-to-action-event' data-response='Download Notebook' download target='_blank'/>");
      $(".github-view-link").wrap("<a href=" + githubLink + " data-behavior='call-to-action-event' data-response='View on Github' target='_blank'/>");
      $("#right-column").addClass("page-shadow");
    } else {
      $(".xanadu-call-to-action-links").hide();
      $("#bottom-dl").attr('style','display: none !important');
    }
    </script>

    <script type="text/javascript">
      function makeUL(urls, text) {
          var list = document.createElement('ul');

          for (var i = 0; i < urls.length; i++) {
              var item = document.createElement('li');
              var a = document.createElement('a');
              var linkText = document.createTextNode(text[i]);
              a.appendChild(linkText);
              a.href = urls[i];
              item.appendChild(a);
              list.appendChild(item);
          }
          return list;
      }

      if (typeof related_tutorials !== 'undefined') {
          document.getElementById('related-tutorials').appendChild(makeUL(related_tutorials, related_tutorials_titles));
          $("#related-tutorials ul li a").append(' <i class="fas fa-angle-double-right" style="font-size: smaller;"></i>')
          $("#related-tutorials").show();

    } else {
          $("#related-tutorials").hide();
    }
    </script>

  <!-- Account for MathJax when navigating to anchor tags. -->
  <script type="text/javascript">
    function scrollToElement(e) {
      // Scrolls to the given element, taking into account the navbar.
      MathJax.Hub.Queue(function() {
        // The following MUST be done asynchronously to take effect.
        setTimeout(function() {
          const navbar = document.querySelector("nav.navbar");
          const navbarHeight = navbar ? navbar.offsetHeight : 0;
          const scrollToY = e.offsetTop + e.offsetParent.offsetTop - navbarHeight;
          window.scrollTo(0, scrollToY);
        }, 0);
      });
    }

    function scrollToFragment(fragment) {
      // Scrolls to the position of the given URL fragment (which includes the "#").
      const elementID = fragment.replace(".", "\\.");
      if (elementID !== "") {
        const element = document.querySelector(elementID);
        if (element !== null) {
          scrollToElement(element);
        }
      }
    }

    $(document).ready(() => {
      scrollToFragment(window.location.hash);
      window.addEventListener("popstate", (_) => scrollToFragment(document.location.hash), false);
    });
  </script>

  <!-- Hide the rendering of :orphan: metadata. -->
  <script type="text/javascript">
    $(document).ready(() => {
      const elements = document.getElementsByClassName("field-odd");
      for (const element of elements) {
          if (element.innerHTML.trim() === "orphan") {
            element.style.display = "none";
          }
      }
    });
  </script>

  <script type="text/javascript">
    jQuery.noConflict(true);
  </script>

  

<footer class="page-footer text-md-left pt-4">

  <hr class="pb-0 mb-0">
  <div class="container-fluid">
    <div class="row justify-content-md-center">

      
      <!-- About -->
      <div class="col-md-4">
        <h5 class="mb-1 footer-heading">PennyLane</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <p>        PennyLane is an open-source software framework for quantum
        machine learning, quantum chemistry, and quantum computing, 
        with the ability to run on all hardware.
        Maintained with ❤️ by Xanadu.
        </p>
      </div>
      

      <!-- Links -->
      
      <div class="col-md-2 col-4">
        <h5 class="mb-1 footer-heading">PennyLane</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <ul class="list-unstyled">
          
          <li><a href="https://pennylane.ai/">Home</a></li>
          
          <li><a href="https://pennylane.ai/qml">Learn</a></li>
          
          <li><a href="https://pennylane.ai/qml/demonstrations.html">Demonstrations</a></li>
          
          <li><a href="https://docs.pennylane.ai/">Documentation</a></li>
          
          <li><a href="https://github.com/PennyLaneAI/pennylane">GitHub</a></li>
          
          <li><a href="https://twitter.com/pennylaneai">Twitter</a></li>
          
          <li><a href="https://pennylane.ai/blog">Blog</a></li>
          
        </ul>
      </div>
      
      <div class="col-md-2 col-4">
        <h5 class="mb-1 footer-heading">Xanadu</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <ul class="list-unstyled">
          
          <li><a href="https://xanadu.ai/">Home</a></li>
          
          <li><a href="https://xanadu.ai/about/">About</a></li>
          
          <li><a href="https://xanadu.ai/photonics">Hardware</a></li>
          
          <li><a href="https://xanadu.ai/careers/">Careers</a></li>
          
          <li><a href="https://cloud.xanadu.ai">Cloud</a></li>
          
          <li><a href="https://discuss.pennylane.ai/">Forum</a></li>
          
          <li><a href="https://xanadu.ai/blog">Blog</a></li>
          
        </ul>
      </div>
      

    </div>
  </div>
  <hr>

  <!-- Social -->
  <div class="social-section text-center">
      <ul class="list-unstyled list-inline mb-0">
          
          <li class="list-inline-item"><a class="btn-git" href="https://twitter.com/PennyLaneAI"><i class="fab fa-twitter"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://github.com/PennyLaneAI/pennylane"><i class="fab fa-github"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://linkedin.com/company/xanaduai/"><i class="fab fa-linkedin-in"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://discuss.pennylane.ai"><i class="fab fa-discourse"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://xanadu-quantum.slack.com/join/shared_invite/zt-nkwn25v9-H4hituCb_PUj4idG0MhSug#/shared-invite/email"><i class="fab fa-slack"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://pennylane.ai/blog/"><i class="fas fa-rss"> </i></a></li>
          
      </ul>
      
        
          <a href="https://xanadu.us17.list-manage.com/subscribe?u=725f07a1d1a4337416c3129fd&id=294b062630" style="font-size: initial;">
            Stay updated with our newsletter
          </a>
        
      
  </div>

  <!-- Copyright -->
  <div class="footer-copyright py-3 mt-0 text-center">
      <div class="container-fluid">
            Copyright &copy; 2022, Xanadu Quantum Technologies, Inc.

        
          <br>
          TensorFlow, the TensorFlow logo, and any related marks are trademarks of Google Inc.
        
      </div>
  </div>
</footer>
  </body>
</html>