
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta content="Train a quantum machine learning model based on the idea of quantum kernels." property="og:description" />
<meta content="https://pennylane.ai/qml/_images/kernel_based_scaling.png" property="og:image" />

  <link rel="icon" type="image/x-icon" href="../_static/favicon.ico">
  <link rel="shortcut icon" type="image/x-icon" href="../_static/favicon.ico">
  


  <meta property="og:title" content="Kernel-based training of quantum models with scikit-learn &#8212; PennyLane">
  <meta property="og:url" content="https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary_large_image">

  
  
  <meta content="Train a quantum machine learning model based on the idea of quantum kernels." property="og:description" />
  

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&display=swap">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css">
  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/css/bootstrap.min.css">
  <!-- Material Design Bootstrap -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.5.14/css/mdb.min.css">
  <!-- NanoScroller -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.nanoscroller/0.8.7/css/nanoscroller.min.css">
  <!-- Syntax Highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/tomorrow-night.min.css">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
       TeX: {
         Macros: {
           pr : ['|\#1\\rangle\\langle\#1|',1],
           ket: ['\\left| \#1\\right\\rangle',1],
           bra: ['\\left\\langle \#1\\right|',1],
           xket: ['\\left| \#1\\right\\rangle_x',1],
           xbra: ['\\left\\langle \#1\\right|_x',1],
           braket: ['\\langle \#1 \\rangle',1],
           braketD: ['\\langle \#1 \\mid \#2 \\rangle',2],
           braketT: ['\\langle \#1 \\mid \#2 \\mid \#3 \\rangle',3],
           ketbra: ['| #1 \\rangle \\langle #2 |',2],
           hc: ['\\text{h.c.}',0],
           cc: ['\\text{c.c.}',0],
           h: ['\\hat',0],
           nn: ['\\nonumber',0],
           di: ['\\frac{d}{d \#1}',1],
           uu: ['\\mathcal{U}',0],
           inn: ['\\text{in}',0],
           out: ['\\text{out}',0],
           vac: ['\\text{vac}',0],
           I: ['\\hat{\\mathbf{1}}',0],
           x: ['\\hat{x}',0],
           p: ['\\hat{p}',0],
           a: ['\\hat{a}',0],
           ad: ['\\hat{a}^\\dagger',0],
           n: ['\\hat{n}',0],
           nbar: ['\\overline{n}',0],
           sech: ['\\mathrm{sech~}',0],
           tanh: ['\\mathrm{tanh~}',0],
           re: ['\\text{Re}',0],
           im: ['\\text{Im}',0],
           tr: ['\\mathrm{Tr} #1',1],
           sign: ['\\text{sign}',0],
           overlr: ['\\overset\\leftrightarrow{\#1}',1],
           overl: ['\\overset\leftarrow{\#1}',1],
           overr: ['\\overset\rightarrow{\#1}',1],
           avg: ['\\left< \#1 \\right>',1],
           slashed: ['\\cancel{\#1}',1],
           bold: ['\\boldsymbol{\#1}',1],
           d: ['\\mathrm d',0],
           expect: ["\\langle #1 \\rangle",1],
           pde: ["\\frac{\\partial}{\\partial \#1}",1],
           R: ["\\mathbb{R}",0],
           C: ["\\mathbb{C}",0],
           Ad: ["\\text{Ad}",0],
           Var: ["\\text{Var}",0],
           bx: ["\\mathbf{x}", 0],
           bm: ["\\boldsymbol{\#1}",1],
           haf: ["\\mathrm{haf}",0],
           lhaf: ["\\mathrm{lhaf}",0]
         }
       }
     });
     </script>

  <!-- Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-130507810-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-130507810-1');
      </script>
  
    <title>Kernel-based training of quantum models with scikit-learn &#8212; PennyLane  documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/xanadu.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/light-slider.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/hubs.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link rel="canonical" href="https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Variational classifier" href="tutorial_variational_classifier.html" />
    <link rel="prev" title="Training and evaluating quantum kernels" href="tutorial_kernels_module.html" /> 
  </head><body><nav class="navbar navbar-expand-lg navbar-light white sticky-top">

<!-- Logo and Title -->









  



  <a class="navbar-brand nav-link" href="https://pennylane.ai">
    
  <img class="pr-1" src=" ../_static/logo.png" width="28px"></img>
  
    <img id="navbar-wordmark" src="../_static/pennylane.svg"></img>
  
  </a>


  <!-- [Mobile] Collapse Button -->
  <div class="row right">
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#basicExampleNav"
      aria-controls="basicExampleNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
  </div>

  <!-- [Mobile] Collapsible Content -->
  <div class="collapse navbar-collapse" id="basicExampleNav">

    <!-- Links on the Left -->
    <ul class="navbar-nav mr-auto">
      
        
          
            <li class="nav-item active">
              <a class="nav-link" href="https://pennylane.ai/qml/">
                
  
    Learn
  

              </a>
              <span class="sr-only">(current)</span>
            </li>
          

        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/qml/demonstrations.html">
                
  
    Demos
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/install.html">
                
  
    Install
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/plugins.html">
                
  
    Plugins
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://docs.pennylane.ai">
                
  
    Documentation
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/blog/">
                
  
    Blog
  

            </a>
          </li>
        
      
    </ul>

    <!-- Links on the Right -->
    <ul class="navbar-nav ml-auto nav-flex-icons">
      
        <li class="nav-item">
          <a class="nav-link" href="https://pennylane.ai/faq.html">
            <i class="fas fa-question pr-1"></i> FAQ
          </a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://discuss.pennylane.ai/">
            <i class="fab fa-discourse pr-1"></i> Support
          </a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/PennyLaneAI/pennylane">
            <i class="fab fa-github pr-1"></i> GitHub
          </a>
        </li>
      

    </ul>
  </div>

</nav>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="tutorial_variational_classifier.html" title="Variational classifier"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tutorial_kernels_module.html" title="Training and evaluating quantum kernels"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demos_qml.html" accesskey="U">Quantum machine learning</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Kernel-based training of quantum models with scikit-learn</a></li> 
      </ul>
    </div>
    <div class="container-wrapper">
        <div id="content">
          <div id="right-column">
            
            

            <div class="document clearer body">
              
    <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-demos-tutorial-kernel-based-training-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="kernel-based-training-of-quantum-models-with-scikit-learn">
<span id="kernel-based-training"></span><span id="sphx-glr-demos-tutorial-kernel-based-training-py"></span><h1>Kernel-based training of quantum models with scikit-learn<a class="headerlink" href="#kernel-based-training-of-quantum-models-with-scikit-learn" title="Permalink to this headline">¶</a></h1>
<p><script type="text/javascript">
    var related_tutorials = ["tutorial_variational_classifier.html"];
    var related_tutorials_titles = ['Variational classifier'];
</script></p>
<p><em>Author: Maria Schuld — Posted: 03 February 2021. Last updated: 3 February 2021.</em></p>
<p>Over the last few years, quantum machine learning research has provided a lot of insights on
how we can understand and train quantum circuits as machine learning models.
While many connections to neural networks have been made, it becomes increasingly clear that
their mathematical foundation is intimately related to so-called <em>kernel methods</em>, the most famous
of which is the <a class="reference external" href="https://en.wikipedia.org/wiki/Support-vector_machine">support vector machine (SVM)</a>
(see for example <a class="reference external" href="https://arxiv.org/abs/1803.07128">Schuld and Killoran (2018)</a>,
<a class="reference external" href="https://arxiv.org/abs/1804.11326">Havlicek et al. (2018)</a>,
<a class="reference external" href="https://arxiv.org/abs/2010.02174">Liu et al. (2020)</a>,
<a class="reference external" href="https://arxiv.org/pdf/2011.01938">Huang et al. (2020)</a>,
and, for a systematic summary which we will follow here,
<a class="reference external" href="https://arxiv.org/abs/2101.11020">Schuld (2021)</a>).</p>
<p>The link between quantum models and kernel methods has important practical implications:
we can replace the common <a class="reference external" href="https://pennylane.ai/qml/glossary/variational_circuit.html">variational approach</a>
to quantum machine learning with a classical kernel method where the kernel—a small building block
of the overall algorithm—is computed by a quantum device. In many situations there are
guarantees that we get better or at least equally good results.</p>
<p>This demonstration explores how kernel-based training compares with
<a class="reference external" href="https://pennylane.ai/qml/demos/tutorial_variational_classifier.html">variational training</a> in terms of the number of quantum
circuits that have to be evaluated. For this we train a quantum machine
learning model with a kernel-based approach using a combination of PennyLane
and the <a class="reference external" href="https://scikit-learn.org/">scikit-learn</a> machine
learning library. We compare this strategy with a variational
quantum circuit trained via stochastic gradient descent using
<a class="reference external" href="https://pennylane.readthedocs.io/en/stable/introduction/interfaces/torch.html">PyTorch</a>.</p>
<p>We will see that in a typical small-scale example, kernel-based training requires only a fraction of the number of
quantum circuit evaluations used by variational circuit training, while each
evaluation runs a much shorter circuit.
In general, the relative efficiency of kernel-based methods compared to variational circuits
depends on the number of parameters used in the variational model.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/scaling1.png"><img alt="Scaling of kernel-based vs. variational learning" src="../_images/scaling1.png" style="width: 504.0px; height: 360.0px;" /></a>
</div>
<p>If the number of variational parameters remains small, e.g., there is a square-root-like scaling with the number
of data samples (green line), variational circuits are almost as efficient as neural networks (blue line),
and require much fewer circuit evaluations
than the quadratic scaling of kernel methods (red line).
However, with current hardware-compatible training strategies,
kernel methods scale much better than variational circuits that require a number of parameters of the
order of the training set size (orange line).</p>
<p>In conclusion, <strong>for quantum machine learning applications with many parameters, kernel-based training can be a great
alternative to the variational approach to quantum machine learning</strong>.</p>
<p>After working through this demo, you will:</p>
<ul class="simple">
<li><p>be able to use a support vector machine with a quantum kernel computed with PennyLane, and</p></li>
<li><p>be able to compare the scaling of quantum circuit evaluations required in kernel-based versus
variational training.</p></li>
</ul>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>Let us consider a <em>quantum model</em> of the form</p>
<div class="math notranslate nohighlight">
\[f(x) = \langle \phi(x) | \mathcal{M} | \phi(x)\rangle,\]</div>
<p>where <span class="math notranslate nohighlight">\(| \phi(x)\rangle\)</span> is prepared
by a fixed <a class="reference external" href="https://pennylane.ai/qml/glossary/quantum_embedding.html">embedding
circuit</a> that
encodes data inputs <span class="math notranslate nohighlight">\(x\)</span>,
and <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> is an arbitrary observable. This model includes variational
quantum machine learning models, since the observable can
effectively be implemented by a simple measurement that is preceded by a
variational circuit:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/quantum_model.png"><img alt="quantum-model" src="../_images/quantum_model.png" style="width: 719.0px; height: 123.2px;" /></a>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>For example, applying a circuit <span class="math notranslate nohighlight">\(G(\theta)\)</span> and then
measuring the Pauli-Z observable <span class="math notranslate nohighlight">\(\sigma^0_z\)</span> of the first qubit
implements the trainable measurement
<span class="math notranslate nohighlight">\(\mathcal{M}(\theta) = G^{\dagger}(\theta) \sigma^0_z G(\theta)\)</span>.</p>
<p>The main practical consequence of approaching quantum machine learning with a
kernel approach is that instead of training <span class="math notranslate nohighlight">\(f\)</span> variationally,
we can often train an equivalent classical kernel method with a kernel executed on a
quantum device. This <em>quantum kernel</em>
is given by the mutual overlap of two data-encoding quantum states,</p>
<div class="math notranslate nohighlight">
\[\kappa(x, x') = | \langle \phi(x') | \phi(x)\rangle|^2.\]</div>
<p>Kernel-based training therefore bypasses the processing and measurement
parts of common variational circuits, and only depends on the
data encoding.</p>
<p>If the loss function <span class="math notranslate nohighlight">\(L\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">hinge
loss</a>, the kernel method
corresponds to a standard <a class="reference external" href="https://en.wikipedia.org/wiki/Support-vector_machine">support vector
machine</a> (SVM)
in the sense of a maximum-margin classifier. Other convex loss functions
lead to more general variations of support vector machines.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>More precisely, we can replace variational with kernel-based
training if the optimisation
problem can be written as minimizing a cost of the form</p>
<div class="math notranslate nohighlight">
\[\min_f  \lambda\;  \mathrm{tr}\{\mathcal{M}^2\} + \frac{1}{M}\sum_{m=1}^M L(f(x^m), y^m),\]</div>
<p>which is a regularized empirical risk with training data samples <span class="math notranslate nohighlight">\((x^m, y^m)_{m=1\dots M}\)</span>,
regularization strength <span class="math notranslate nohighlight">\(\lambda \in \mathbb{R}\)</span>, and loss function <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>Theory predicts that kernel-based training will always find better or equally good
minima of this risk. However, to show this here we would have
to either regularize the variational training by the trace of the squared observable, or switch off
regularization in the classical SVM, which removes a lot of its strength. The kernel-based and the variational
training in this demonstration therefore optimize slightly different cost
functions, and it is out of our scope to establish whether one training method finds a better minimum than
the other.</p>
</div>
</div>
<div class="section" id="id1">
<h2>Kernel-based training<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>First, we will turn to kernel-based training of quantum models.
As stated above, an example implementation is a standard support vector
machine with a kernel computed by a quantum circuit.</p>
<p>We begin by importing all sorts of useful methods:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">relu</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">from</span> <span class="nn">pennylane.templates</span> <span class="kn">import</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.AngleEmbedding.html#pennylane.AngleEmbedding" title="pennylane.AngleEmbedding" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">AngleEmbedding</span></a><span class="p">,</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.StronglyEntanglingLayers.html#pennylane.StronglyEntanglingLayers" title="pennylane.StronglyEntanglingLayers" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">StronglyEntanglingLayers</span></a>
<span class="kn">from</span> <span class="nn">pennylane.operation</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>The second step is to define a data set. Since the performance
of the models is not the focus of this demo, we can just use
the first two classes of the famous <a class="reference external" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris data set</a>.
Dating back to as far as 1936,
this toy data set consists of 100 samples of four features each,
and gives rise to a very simple classification problem.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># pick inputs and labels from the first two classes only,</span>
<span class="c1"># corresponding to the first 100 samples</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># scaling the inputs is important since the embedding we use is periodic</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># scaling the labels to -1, 1 is important for the SVM and the</span>
<span class="c1"># definition of a hinge loss</span>
<span class="n">y_scaled</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>We use the <a class="reference external" href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.templates.embeddings.AngleEmbedding.html">angle-embedding
template</a>
which needs as many qubits as there are features:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_qubits</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">n_qubits</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>4
</pre></div>
</div>
<p>To implement the kernel we could prepare the two states <span class="math notranslate nohighlight">\(| \phi(x) \rangle\)</span>, <span class="math notranslate nohighlight">\(| \phi(x') \rangle\)</span>
on different sets of qubits with angle-embedding routines <span class="math notranslate nohighlight">\(S(x), S(x')\)</span>,
and measure their overlap with a small routine called a <a class="reference external" href="https://en.wikipedia.org/wiki/Swap_test">SWAP test</a>.</p>
<p>However, we need only half the number of qubits if we prepare
<span class="math notranslate nohighlight">\(| \phi(x)\rangle\)</span> and then apply the inverse embedding
with <span class="math notranslate nohighlight">\(x'\)</span> on the same qubits. We then measure the projector onto
the initial state <span class="math notranslate nohighlight">\(|0..0\rangle \langle 0..0|\)</span>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/kernel_circuit.png"><img alt="Kernel evaluation circuit" src="../_images/kernel_circuit.png" style="width: 400.0px; height: 132.8px;" /></a>
</div>
<p>To verify that this gives us the kernel:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \langle 0..0 |S(x') S(x)^{\dagger} \mathcal{M} S(x')^{\dagger} S(x)  | 0..0\rangle &amp;= \langle 0..0 |S(x') S(x)^{\dagger} |0..0\rangle \langle 0..0| S(x')^{\dagger} S(x)  | 0..0\rangle  \\
    &amp;= |\langle 0..0| S(x')^{\dagger} S(x)  | 0..0\rangle |^2\\
    &amp;= | \langle \phi(x') | \phi(x)\rangle|^2 \\
    &amp;= \kappa(x, x').
\end{align*}\end{split}\]</div>
<p>Note that a projector <span class="math notranslate nohighlight">\(|0..0 \rangle \langle 0..0|\)</span> can be constructed
using the <code class="docutils literal notranslate"><span class="pre">qml.Hermitian</span></code> observable in PennyLane.</p>
<p>Altogether, we use the following quantum node as a <em>quantum kernel
evaluator</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dev_kernel</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.device.html#pennylane.device" title="pennylane.device" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;lightning.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">)</span>

<span class="n">projector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="n">n_qubits</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">n_qubits</span><span class="p">))</span>
<span class="n">projector</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev_kernel</span><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="s2">&quot;autograd&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The quantum kernel.&quot;&quot;&quot;</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.AngleEmbedding.html#pennylane.AngleEmbedding" title="pennylane.AngleEmbedding" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">AngleEmbedding</span></a><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.adjoint.html#pennylane.adjoint" title="pennylane.adjoint" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">adjoint</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.AngleEmbedding.html#pennylane.AngleEmbedding" title="pennylane.AngleEmbedding" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">AngleEmbedding</span></a><span class="p">)(</span><span class="n">x2</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.expval.html#pennylane.expval" title="pennylane.expval" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">expval</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hermitian.html#pennylane.Hermitian" title="pennylane.Hermitian" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">Hermitian</span></a><span class="p">(</span><span class="n">projector</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">)))</span>
</pre></div>
</div>
<p>A good sanity check is whether evaluating the kernel of a data point and
itself returns 1:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>array(1.)
</pre></div>
</div>
<p>The way an SVM with a custom kernel is implemented in scikit-learn
requires us to pass a function that computes a matrix of kernel
evaluations for samples in two different datasets A, B. If A=B,
this is the <a class="reference external" href="https://en.wikipedia.org/wiki/Gramian_matrix">Gram matrix</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kernel_matrix</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the matrix whose entries are the kernel</span>
<span class="sd">       evaluated on pairwise data from sets A and B.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">kernel</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">B</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">A</span><span class="p">])</span>
</pre></div>
</div>
<p>Training the SVM optimizes internal parameters that basically
weigh kernel functions.
It is a breeze in scikit-learn, which is designed
as a high-level machine learning library:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s compute the accuracy on the test set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
<p>The SVM predicted all test points correctly.
How many times was the quantum device evaluated?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.QubitDevice.html#pennylane.QubitDevice.num_executions" title="pennylane.QubitDevice.num_executions" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-attribute"><span class="n">dev_kernel</span><span class="o">.</span><span class="n">num_executions</span></a>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>7501
</pre></div>
</div>
<p>This number can be derived as follows: For <span class="math notranslate nohighlight">\(M\)</span> training samples,
the SVM must construct the <span class="math notranslate nohighlight">\(M \times M\)</span> dimensional kernel gram
matrix for training. To classify <span class="math notranslate nohighlight">\(M_{\rm pred}\)</span> new samples, the
SVM needs to evaluate the kernel at most <span class="math notranslate nohighlight">\(M_{\rm pred}M\)</span> times to get the
pairwise distances between training vectors and test samples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending on the implementation of the SVM, only <span class="math notranslate nohighlight">\(S \leq M_{\rm pred}\)</span>
<em>support vectors</em> are needed.</p>
</div>
<p>Let us formulate this as a function, which can be used at the end of the demo
to construct the scaling plot shown in the introduction.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">circuit_evals_kernel</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="n">split</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute how many circuit evaluations one needs for kernel-based</span>
<span class="sd">       training and prediction.&quot;&quot;&quot;</span>

    <span class="n">M</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">split</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">))</span>
    <span class="n">Mpred</span> <span class="o">=</span> <span class="n">n_data</span> <span class="o">-</span> <span class="n">M</span>

    <span class="n">n_training</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">M</span>
    <span class="n">n_prediction</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">Mpred</span>

    <span class="k">return</span> <span class="n">n_training</span> <span class="o">+</span> <span class="n">n_prediction</span>
</pre></div>
</div>
<p>With <span class="math notranslate nohighlight">\(M = 75\)</span> and <span class="math notranslate nohighlight">\(M_{\rm pred} = 25\)</span>, the number of kernel evaluations
can therefore be estimated as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">circuit_evals_kernel</span><span class="p">(</span><span class="n">n_data</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">split</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>7500
</pre></div>
</div>
<p>The single additional evaluation can be attributed to evaluating the kernel once above
as a sanity check.</p>
</div>
<div class="section" id="a-similar-example-using-variational-training">
<h2>A similar example using variational training<a class="headerlink" href="#a-similar-example-using-variational-training" title="Permalink to this headline">¶</a></h2>
<p>Using the variational principle of training, we can propose an <em>ansatz</em>
for the variational circuit and train it directly. By
increasing the number of layers of the ansatz, its expressivity
increases. Depending on the ansatz, we may only
search through a subspace of all measurements for the best
candidate.</p>
<p>Remember from above, the variational training does not optimize
<em>exactly</em> the same cost as the SVM, but we try to match them as closely
as possible. For this we use a bias term in the quantum model, and train
on the hinge loss.</p>
<p>We also explicitly use the <a class="reference external" href="https://pennylane.ai/qml/glossary/parameter_shift.html">parameter-shift</a>
differentiation method in the quantum node, since this is a method which works on hardware as well.
While <code class="docutils literal notranslate"><span class="pre">diff_method='backprop'</span></code> or <code class="docutils literal notranslate"><span class="pre">diff_method='adjoint'</span></code> would reduce the number of
circuit evaluations significantly, they are based on tricks that are only suitable for simulators,
and can therefore not scale to more than a few dozen qubits.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dev_var</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.device.html#pennylane.device" title="pennylane.device" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;lightning.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">)</span>

<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev_var</span><span class="p">,</span> <span class="n">interface</span><span class="o">=</span><span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="n">diff_method</span><span class="o">=</span><span class="s2">&quot;parameter-shift&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">quantum_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A variational quantum model.&quot;&quot;&quot;</span>

    <span class="c1"># embedding</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.AngleEmbedding.html#pennylane.AngleEmbedding" title="pennylane.AngleEmbedding" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">AngleEmbedding</span></a><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>

    <span class="c1"># trainable measurement</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.StronglyEntanglingLayers.html#pennylane.StronglyEntanglingLayers" title="pennylane.StronglyEntanglingLayers" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">StronglyEntanglingLayers</span></a><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">))</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.expval.html#pennylane.expval" title="pennylane.expval" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">expval</span></a><span class="p">(</span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.PauliZ.html#pennylane.PauliZ" title="pennylane.PauliZ" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class"><span class="n">qml</span><span class="o">.</span><span class="n">PauliZ</span></a><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">quantum_model_plus_bias</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adding a bias.&quot;&quot;&quot;</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.QNode.html#pennylane.QNode" title="pennylane.QNode" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">quantum_model</span></a><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span>

<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implements the hinge loss.&quot;&quot;&quot;</span>
    <span class="n">all_ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="n">hinge_loss</span> <span class="o">=</span> <span class="n">all_ones</span> <span class="o">-</span> <span class="n">predictions</span> <span class="o">*</span> <span class="n">targets</span>
    <span class="c1"># trick: since the max(0,x) function is not differentiable,</span>
    <span class="c1"># use the mathematically equivalent relu instead</span>
    <span class="n">hinge_loss</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">hinge_loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hinge_loss</span>
</pre></div>
</div>
<p>We now summarize the usual training and prediction steps into two
functions similar to scikit-learn’s <code class="docutils literal notranslate"><span class="pre">fit()</span></code> and <code class="docutils literal notranslate"><span class="pre">predict()</span></code>. While
it feels cumbersome compared to the one-liner used to train the kernel method,
PennyLane—like other differentiable programming libraries—provides a lot more
control over the particulars of training.</p>
<p>In our case, most of the work is to convert between numpy and torch,
which we need for the differentiable <code class="docutils literal notranslate"><span class="pre">relu</span></code> function used in the hinge loss.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">quantum_model_train</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Train the quantum model defined above.&quot;&quot;&quot;</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_qubits</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">params_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bias_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">params_torch</span><span class="p">,</span> <span class="n">bias_torch</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>

        <span class="n">batch_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">batch_ids</span><span class="p">]</span>

        <span class="n">X_batch_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_batch</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">y_batch_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y_batch</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="p">[</span><span class="n">quantum_model_plus_bias</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params_torch</span><span class="p">,</span> <span class="n">bias_torch</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_batch_torch</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y_batch_torch</span><span class="p">))</span>

            <span class="c1"># bookkeeping</span>
            <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">loss_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_loss</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;, loss&quot;</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">)</span>

            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params_torch</span><span class="p">,</span> <span class="n">bias_torch</span><span class="p">,</span> <span class="n">loss_history</span>


<span class="k">def</span> <span class="nf">quantum_model_predict</span><span class="p">(</span><span class="n">X_pred</span><span class="p">,</span> <span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Predict using the quantum model defined above.&quot;&quot;&quot;</span>

    <span class="n">p</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_pred</span><span class="p">:</span>

        <span class="n">x_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pred_torch</span> <span class="o">=</span> <span class="n">quantum_model_plus_bias</span><span class="p">(</span><span class="n">x_torch</span><span class="p">,</span> <span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred_torch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="n">p</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
</div>
<p>Let’s train the variational model and see how well we are doing on the
test set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">,</span> <span class="n">loss_history</span> <span class="o">=</span> <span class="n">quantum_model_train</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">pred_test</span> <span class="o">=</span> <span class="n">quantum_model_predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">trained_params</span><span class="p">,</span> <span class="n">trained_bias</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;accuracy on test set:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">pred_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_history</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;cost&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_tutorial_kernel_based_training_001.png" srcset="../_images/sphx_glr_tutorial_kernel_based_training_001.png" alt="tutorial kernel based training" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>step 0 , loss 1.2128428849025235
step 10 , loss 0.8582750956106431
step 20 , loss 0.43849890579633233
step 30 , loss 0.6458829274590641
step 40 , loss 0.5540116701446125
step 50 , loss 0.4132239145818266
step 60 , loss 0.5209433003814097
step 70 , loss 0.46941934231603816
step 80 , loss 0.48581457440211395
step 90 , loss 0.4196234621534023
accuracy on test set: 0.96
</pre></div>
</div>
<p>The variational circuit has a slightly lower
accuracy than the SVM—but this depends very much on the training settings
we used. Different random parameter initializations, more layers, or more steps may indeed get
perfect test accuracy.</p>
<p>How often was the device executed?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.QubitDevice.html#pennylane.QubitDevice.num_executions" title="pennylane.QubitDevice.num_executions" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-attribute"><span class="n">dev_var</span><span class="o">.</span><span class="n">num_executions</span></a>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>74025
</pre></div>
</div>
<p>That is a lot more than the kernel method took!</p>
<p>Let’s try to understand this value. In each optimization step, the variational
circuit needs to compute the partial derivative of all
trainable parameters for each sample in a batch. Using parameter-shift
rules, we require roughly two circuit
evaluations per partial derivative. Prediction uses only one circuit
evaluation per sample.</p>
<p>We can formulate this as another function that will be used in the scaling plot below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">circuit_evals_variational</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="n">n_params</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">shift_terms</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute how many circuit evaluations are needed for</span>
<span class="sd">       variational training and prediction.&quot;&quot;&quot;</span>

    <span class="n">M</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">split</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">))</span>
    <span class="n">Mpred</span> <span class="o">=</span> <span class="n">n_data</span> <span class="o">-</span> <span class="n">M</span>

    <span class="n">n_training</span> <span class="o">=</span> <span class="n">n_params</span> <span class="o">*</span> <span class="n">n_steps</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">shift_terms</span>
    <span class="n">n_prediction</span> <span class="o">=</span> <span class="n">Mpred</span>

    <span class="k">return</span> <span class="n">n_training</span> <span class="o">+</span> <span class="n">n_prediction</span>
</pre></div>
</div>
<p>This estimates the circuit evaluations in variational training as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">circuit_evals_variational</span><span class="p">(</span>
    <span class="n">n_data</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
    <span class="n">n_params</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">trained_params</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
    <span class="n">shift_terms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>96025
</pre></div>
</div>
<p>The estimate is a bit higher because it does not account for some optimizations
that PennyLane performs under the hood.</p>
<p>It is important to note that while they are trained in a similar manner,
the number of variational circuit evaluations differs from the number of
neural network model evaluations in classical machine learning, which would be given by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_evals_nn</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="n">n_params</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute how many model evaluations are needed for neural</span>
<span class="sd">       network training and prediction.&quot;&quot;&quot;</span>

    <span class="n">M</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">split</span> <span class="o">*</span> <span class="n">n_data</span><span class="p">))</span>
    <span class="n">Mpred</span> <span class="o">=</span> <span class="n">n_data</span> <span class="o">-</span> <span class="n">M</span>

    <span class="n">n_training</span> <span class="o">=</span> <span class="n">n_steps</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">n_prediction</span> <span class="o">=</span> <span class="n">Mpred</span>

    <span class="k">return</span> <span class="n">n_training</span> <span class="o">+</span> <span class="n">n_prediction</span>
</pre></div>
</div>
<p>In each step of neural network training, and due to the clever implementations of automatic differentiation,
the backpropagation algorithm can compute a
gradient for all parameters in (more-or-less) a single run.
For all we know at this stage, the no-cloning principle prevents variational circuits from using these tricks,
which leads to <code class="docutils literal notranslate"><span class="pre">n_training</span></code> in <code class="docutils literal notranslate"><span class="pre">circuit_evals_variational</span></code> depending on the number of parameters, but not in
<code class="docutils literal notranslate"><span class="pre">model_evals_nn</span></code>.</p>
<p>For the same example as used here, a neural network would therefore
have far fewer model evaluations than both variational and kernel-based training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_evals_nn</span><span class="p">(</span>
    <span class="n">n_data</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
    <span class="n">n_params</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">trained_params</span><span class="o">.</span><span class="n">flatten</span><span class="p">()),</span>
    <span class="n">n_steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>2025
</pre></div>
</div>
</div>
<div class="section" id="which-method-scales-best">
<h2>Which method scales best?<a class="headerlink" href="#which-method-scales-best" title="Permalink to this headline">¶</a></h2>
<p>The answer to this question depends on how the variational model
is set up, and we need to make a few assumptions:</p>
<ol class="arabic">
<li><p>Even if we use single-batch stochastic gradient descent, in which every training step uses
exactly one training sample, we would want to see every training sample at least once on average.
Therefore, the number of steps should scale at least linearly with the number of training data samples.</p></li>
<li><p>Modern neural networks often have many more parameters than training
samples. But we do not know yet whether variational circuits really need that many parameters as well.
We will therefore use two cases for comparison:</p>
<p>2a) the number of parameters grows linearly with the training data, or <code class="docutils literal notranslate"><span class="pre">n_params</span> <span class="pre">=</span> <span class="pre">M</span></code>,</p>
<p>2b) the number of parameters saturates at some point, which we model by setting <code class="docutils literal notranslate"><span class="pre">n_params</span> <span class="pre">=</span> <span class="pre">sqrt(M)</span></code>.</p>
</li>
</ol>
<p>Note that compared to the example above with 75 training samples and 24 parameters, a) overestimates the number of evaluations, while b)
underestimates it.</p>
<p>This is how the three methods compare:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">variational_training1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">variational_training2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">kernelbased_training</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">nn_training</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="n">x_axis</span><span class="p">:</span>

    <span class="n">var1</span> <span class="o">=</span> <span class="n">circuit_evals_variational</span><span class="p">(</span>
        <span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>  <span class="n">shift_terms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">variational_training1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var1</span><span class="p">)</span>

    <span class="n">var2</span> <span class="o">=</span> <span class="n">circuit_evals_variational</span><span class="p">(</span>
        <span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">M</span><span class="p">)),</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
        <span class="n">shift_terms</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">variational_training2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">var2</span><span class="p">)</span>

    <span class="n">kernel</span> <span class="o">=</span> <span class="n">circuit_evals_kernel</span><span class="p">(</span><span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
    <span class="n">kernelbased_training</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>

    <span class="n">nn</span> <span class="o">=</span> <span class="n">model_evals_nn</span><span class="p">(</span>
        <span class="n">n_data</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_params</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="n">M</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="n">nn_training</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">nn_training</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;neural net&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">variational_training1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;var. circuit (linear param scaling)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">variational_training2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;var. circuit (srqt param scaling)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">kernelbased_training</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;(quantum) kernel&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;size of data set&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;number of evaluations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_tutorial_kernel_based_training_002.png" srcset="../_images/sphx_glr_tutorial_kernel_based_training_002.png" alt="tutorial kernel based training" class = "sphx-glr-single-img"/><p>This is the plot we saw at the beginning.
With current hardware-compatible training methods, whether kernel-based training
requires more or fewer quantum circuit evaluations
than variational training depends on how many parameters the latter needs.
If variational circuits turn out to be as parameter-hungry as neural networks,
kernel-based training will outperform them for common machine learning tasks. However,
if variational learning only turns out to require few parameters (or if more efficient training methods are found),
variational circuits could in principle match the linear scaling of neural networks trained with backpropagation.</p>
<p>The practical take-away from this demo is that unless your variational circuit has significantly fewer
parameters than training data, kernel methods could be a much faster alternative!</p>
<p>Finally, it is important to note that fault-tolerant quantum computers may change the picture
for both quantum and classical machine learning.
As mentioned in <a class="reference external" href="https://arxiv.org/abs/2101.11020">Schuld (2021)</a>,
early results from the quantum machine learning literature show that
larger quantum computers will most likely enable us to reduce
the quadratic scaling of kernel methods to linear scaling, which may make classical as well as quantum kernel methods a
strong alternative to neural networks for big data processing one day.</p>
</div>
<div class="section" id="about-the-author">
<h2>About the author<a class="headerlink" href="#about-the-author" title="Permalink to this headline">¶</a></h2>
<div class="bio" >
    <div class="photo" >
        <img class="photo__img" src="../_static/authors/maria_schuld.jpg" alt="Maria Schuld" >
    </div>
    <div class="bio-text">
        <h4 class="bio-text__author-name">Maria Schuld</h4>
        <p class="bio-text__author-description">Maria leads Xanadu's quantum machine learning team and is a seasoned PennyLane developer.</p>
    </div>
</div><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 2 minutes  49.255 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-demos-tutorial-kernel-based-training-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/91517ef9f618b39f2571f85fafd0f785/tutorial_kernel_based_training.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tutorial_kernel_based_training.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4cf13e58de2a3117c8a79c5ce76688fb/tutorial_kernel_based_training.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tutorial_kernel_based_training.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


    <script type="text/javascript">
        // This script ensures that the active navbar entry switches
        // from 'QML' to 'Demos' for any webpage within the demos/ directory,
        // or for any of the demonstration landing pages
        // (e.g., demos_optimization).
        var pagename = document.location.href.match(/[^\/]+$/)[0];
        var dir = document.URL.substr(0,document.URL.lastIndexOf('/')).match(/[^\/]+$/)[0];

        if (pagename.includes("demos") || pagename.includes("demonstrations") || dir.includes("demos")) {

            $(".nav-item.active").removeClass("active");
            var demos_link = $('.navbar-nav a').filter(function(index) { return $(this).text() === "Demos"; })[0]
            $(demos_link).parent().addClass("active");
        }
    </script>

              <div id="bottom-dl" class="xanadu-call-to-action-links">
                <div id="tutorial-type">demos/tutorial_kernel_based_training</div>
                <div class="download-python-link">
                  <i class="fab fa-python"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Python script</div>
                </div>
                <div class="download-notebook-link">
                  <i class="fas fa-download"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Notebook</div>
                </div>
                <div class="github-view-link">
                  <i class="fab fa-github"></i>&nbsp;
                  <div class="call-to-action-desktop-view">View on GitHub</div>
                </div>
              </div>

            </div>
            
          </div>
        
<div class="localtoc-container nano has-scrollbar">
  <div class="nano-content">
    <div id="localtoc">
        
          <h3>Contents</h3>
          <!-- Display the ToC for the current document if it is not empty. -->
          <ul class='current'>
<li class='current'><a class="reference internal" href="#">Kernel-based training of quantum models with scikit-learn</a><ul class='current'>
<li class='current'><a class="reference internal" href="#background">Background</a></li>
<li class='current'><a class="reference internal" href="#id1">Kernel-based training</a></li>
<li class='current'><a class="reference internal" href="#a-similar-example-using-variational-training">A similar example using variational training</a></li>
<li class='current'><a class="reference internal" href="#which-method-scales-best">Which method scales best?</a></li>
<li class='current'><a class="reference internal" href="#about-the-author">About the author</a></li>
</ul>
</li>
</ul>

        
    </div>

    <div class="xanadu-call-to-action-links">
        <h3>Downloads</h3>
        <div id="tutorial-type">demos/tutorial_kernel_based_training</div>
        <div class="download-python-link">
            <i class="fab fa-python"></i>&nbsp;
            <div class="call-to-action-desktop-view">Download Python script</div>
        </div>
        <div class="download-notebook-link">
            <i class="fas fa-download"></i>&nbsp;
            <div class="call-to-action-desktop-view">Download Notebook</div>
        </div>
        <div class="github-view-link">
            <i class="fab fa-github"></i>&nbsp;
            <div class="call-to-action-desktop-view">View on GitHub</div>
        </div>
    </div>
    <div id="related-tutorials" class="mt-4">
      <h3> Related</h3>
    </div>
  </div>
</div>


    
          <div class="up-button">
            
              
                <a href="../demos_qml.html"><i class="fas fa-angle-double-left"></i></a>
              
            
          </div>

          <div class="clearfix"></div>
        </div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="tutorial_variational_classifier.html" title="Variational classifier"
             >next</a> |</li>
        <li class="right" >
          <a href="tutorial_kernels_module.html" title="Training and evaluating quantum kernels"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demos_qml.html" >Quantum machine learning</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Kernel-based training of quantum models with scikit-learn</a></li> 
      </ul>
    </div>
  <script type="text/javascript">
    $("#mobile-toggle").click(function () {
      $("#left-column").slideToggle("slow");
    });
  </script>

  <!-- jQuery -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>
  <!-- MathJax -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- Bootstrap core JavaScript -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <!-- MDB core JavaScript -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.8.10/js/mdb.min.js"></script>
  <!-- NanoScroller -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery.nanoscroller/0.8.7/javascripts/jquery.nanoscroller.min.js"></script>
  <!-- Syntax Highlighting -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
  <script type="text/javascript">hljs.initHighlightingOnLoad();</script>

  <script type="text/javascript">
    $("a.reference.internal").each(function(){
      var link = $(this).attr("href");

      var hash = link.split("#")[1];
      var page = link.split("#")[0].split("/").slice(-1)[0].replace(".html", "");

      if (hash == page) {
        $(this).attr("href", link.split("#")[0]);
      }
    });

    $(".document > .section").removeClass("section");
    $("h1 ~ .section").removeClass("section");
    $(".localtoc-container .nano-content").css("height", $("#content").height());
    $(".localtoc-container").css("height", $("#content").height());
    $(".nano").nanoScroller();
  </script>

  <script type="text/javascript">
      $(window).scroll(function(){
        var scrollBottom = $(document).height() - $(window).height() - $(window).scrollTop();
        if (scrollBottom < 342) {
          $(".localtoc-container").css("height", "calc(100% - " + (342 - scrollBottom) + "px)");
          $(".localtoc-container .nano-content").css("height", "calc(100% - 119px)");
        }
      });
  </script>

  <script type="text/javascript">
    if ($(".current").length) {
      var target = $(".current")[0]
      var rect = target.getBoundingClientRect();
      if (rect.bottom > window.innerHeight) {
          $(".nano").nanoScroller({ scrollTo: $(".current") });
      } else {
          $(".nano").nanoScroller({ scrollTop: 0 });
      }
    }
    $(document).ready(function () {
        $(".css-transitions-only-after-page-load").each(function (index, element) {
            setTimeout(function () { $(element).removeClass("css-transitions-only-after-page-load") }, 10);
        });
        if (window.location.hash) {
          var target = $("[id='" + window.location.hash.substr(1) + "']");
          if (target.closest(".collapse").length) {
            target.closest(".collapse").addClass("show");
            target.closest(".collapse").prev().find(".rotate").addClass("up");
          }
        }
    });
  </script>

    <script type="text/javascript">
    var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
    if (downloadNote.length >= 1) {
      var tutorialUrlArray = $("#tutorial-type").text().split('/');

      if (tutorialUrlArray[0] == "demos") {
        tutorialUrlArray[0] = "demonstrations";
      }

      var githubLink = "https://github.com/" + "PennyLaneAI/qml" + "/blob/master/" + tutorialUrlArray.join("/") + ".py",
          pythonLink = $(".sphx-glr-download .reference.download")[0].href,
          notebookLink = $(".sphx-glr-download .reference.download")[1].href;

      $(".download-python-link").wrap("<a href=" + pythonLink + " data-behavior='call-to-action-event' data-response='Download Python script' download target='_blank'/>");
      $(".download-notebook-link").wrap("<a href=" + notebookLink + " data-behavior='call-to-action-event' data-response='Download Notebook' download target='_blank'/>");
      $(".github-view-link").wrap("<a href=" + githubLink + " data-behavior='call-to-action-event' data-response='View on Github' target='_blank'/>");
      $("#right-column").addClass("page-shadow");
    } else {
      $(".xanadu-call-to-action-links").hide();
      $("#bottom-dl").attr('style','display: none !important');
    }
    </script>

    <script type="text/javascript">
      function makeUL(urls, text) {
          var list = document.createElement('ul');

          for (var i = 0; i < urls.length; i++) {
              var item = document.createElement('li');
              var a = document.createElement('a');
              var linkText = document.createTextNode(text[i]);
              a.appendChild(linkText);
              a.href = urls[i];
              item.appendChild(a);
              list.appendChild(item);
          }
          return list;
      }

      if (typeof related_tutorials !== 'undefined') {
          document.getElementById('related-tutorials').appendChild(makeUL(related_tutorials, related_tutorials_titles));
          $("#related-tutorials ul li a").append(' <i class="fas fa-angle-double-right" style="font-size: smaller;"></i>')
          $("#related-tutorials").show();

    } else {
          $("#related-tutorials").hide();
    }
    </script>

  <!-- Account for MathJax when navigating to anchor tags. -->
  <script type="text/javascript">
    function scrollToElement(e) {
      // Scrolls to the given element, taking into account the navbar.
      MathJax.Hub.Queue(function() {
        // The following MUST be done asynchronously to take effect.
        setTimeout(function() {
          const navbar = document.querySelector("nav.navbar");
          const navbarHeight = navbar ? navbar.offsetHeight : 0;
          const scrollToY = e.offsetTop + e.offsetParent.offsetTop - navbarHeight;
          window.scrollTo(0, scrollToY);
        }, 0);
      });
    }

    function scrollToFragment(fragment) {
      // Scrolls to the position of the given URL fragment (which includes the "#").
      const elementID = fragment.replace(".", "\\.");
      if (elementID !== "") {
        const element = document.querySelector(elementID);
        if (element !== null) {
          scrollToElement(element);
        }
      }
    }

    $(document).ready(() => {
      scrollToFragment(window.location.hash);
      window.addEventListener("popstate", (_) => scrollToFragment(document.location.hash), false);
    });
  </script>

  <!-- Hide the rendering of :orphan: metadata. -->
  <script type="text/javascript">
    $(document).ready(() => {
      const elements = document.getElementsByClassName("field-odd");
      for (const element of elements) {
          if (element.innerHTML.trim() === "orphan") {
            element.style.display = "none";
          }
      }
    });
  </script>

  <script type="text/javascript">
    jQuery.noConflict(true);
  </script>

  

<footer class="page-footer text-md-left pt-4">

  <hr class="pb-0 mb-0">
  <div class="container-fluid">
    <div class="row justify-content-md-center">

      
      <!-- About -->
      <div class="col-md-4">
        <h5 class="mb-1 footer-heading">PennyLane</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <p>        PennyLane is an open-source software framework for quantum
        machine learning, quantum chemistry, and quantum computing, 
        with the ability to run on all hardware.
        Maintained with ❤️ by Xanadu.
        </p>
      </div>
      

      <!-- Links -->
      
      <div class="col-md-2 col-4">
        <h5 class="mb-1 footer-heading">PennyLane</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <ul class="list-unstyled">
          
          <li><a href="https://pennylane.ai/">Home</a></li>
          
          <li><a href="https://pennylane.ai/qml">Learn</a></li>
          
          <li><a href="https://pennylane.ai/qml/demonstrations.html">Demonstrations</a></li>
          
          <li><a href="https://docs.pennylane.ai/">Documentation</a></li>
          
          <li><a href="https://github.com/PennyLaneAI/pennylane">GitHub</a></li>
          
          <li><a href="https://twitter.com/pennylaneai">Twitter</a></li>
          
          <li><a href="https://pennylane.ai/blog">Blog</a></li>
          
        </ul>
      </div>
      
      <div class="col-md-2 col-4">
        <h5 class="mb-1 footer-heading">Xanadu</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <ul class="list-unstyled">
          
          <li><a href="https://xanadu.ai/">Home</a></li>
          
          <li><a href="https://xanadu.ai/about/">About</a></li>
          
          <li><a href="https://xanadu.ai/photonics">Hardware</a></li>
          
          <li><a href="https://xanadu.ai/careers/">Careers</a></li>
          
          <li><a href="https://cloud.xanadu.ai">Cloud</a></li>
          
          <li><a href="https://discuss.pennylane.ai/">Forum</a></li>
          
          <li><a href="https://xanadu.ai/blog">Blog</a></li>
          
        </ul>
      </div>
      

    </div>
  </div>
  <hr>

  <!-- Social -->
  <div class="social-section text-center">
      <ul class="list-unstyled list-inline mb-0">
          
          <li class="list-inline-item"><a class="btn-git" href="https://twitter.com/PennyLaneAI"><i class="fab fa-twitter"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://github.com/PennyLaneAI/pennylane"><i class="fab fa-github"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://linkedin.com/company/xanaduai/"><i class="fab fa-linkedin-in"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://discuss.pennylane.ai"><i class="fab fa-discourse"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://xanadu-quantum.slack.com/join/shared_invite/zt-nkwn25v9-H4hituCb_PUj4idG0MhSug#/shared-invite/email"><i class="fab fa-slack"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://pennylane.ai/blog/"><i class="fas fa-rss"> </i></a></li>
          
      </ul>
      
        
          <a href="https://xanadu.us17.list-manage.com/subscribe?u=725f07a1d1a4337416c3129fd&id=294b062630" style="font-size: initial;">
            Stay updated with our newsletter
          </a>
        
      
  </div>

  <!-- Copyright -->
  <div class="footer-copyright py-3 mt-0 text-center">
      <div class="container-fluid">
            Copyright &copy; 2022, Xanadu Quantum Technologies, Inc.

        
          <br>
          TensorFlow, the TensorFlow logo, and any related marks are trademarks of Google Inc.
        
      </div>
  </div>
</footer>
  </body>
</html>