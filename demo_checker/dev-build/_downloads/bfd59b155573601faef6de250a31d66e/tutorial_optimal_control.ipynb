{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimal control for gate compilation\n====================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Optimize pulse programs to obtain digital\ngates. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/thumbnail_tutorial_optimal_control.png>\n:::\n\n::: {.related}\ntutorial\\_pulse\\_programming101 Introduction to pulse programming in\nPennyLane tutorial\\_neutral\\_atoms Introduction to neutral atom quantum\ncomputers ahs\\_aquila Pulse programming on Rydberg atom hardware\n:::\n\nToday, quantum computations are largely phrased in the language of\nquantum circuits, composed of digital quantum gates. However, most\nquantum hardware does not come with such digital gates as elementary\nnative operations. Instead, the hardware allows us to play sequences of\nanalog electromagnetic pulses, for example by shining laser pulses on\ntrapped ions or Rydberg atoms, or by sending microwave pulses onto\nsuperconducting qubit cavities. These pulses need to be calibrated to\nproduce the desired digital gates, and in this tutorial we will be\nconcerned with exactly this task.\n\nFor this, we will parametrize a pulse sequence, which leads to a whole\n*space* of possible sequences. Then we optimize the pulse parameters in\norder to find a configuration in this space that behaves as closely to\nthe target gate as possible. More concretely, we will optimize simple\npulse programs on two and three qubits to obtain a CNOT and a Toffoli\ngate. This training of control parameters to achieve a specific time\nevolution is a standard task in the field of *quantum optimal control*.\n\n| \n\n![](../demonstrations/optimal_control/OptimalControl_control_quantum_systems.png){.align-center\nwidth=\"100.0%\"}\n\n| \n\nFor an introduction, see\n`the demo on differentiable pulse programming </demos/tutorial_pulse_programming101>`{.interpreted-text\nrole=\"doc\"} in PennyLane. Instead of optimizing pulses to yield digital\nquantum gates, we may use them directly to solve optimization problems,\nas is also showcased in this introductory demo. If you are interested in\nspecific hardware pulses, take a look at\n`an introduction to neutral-atom quantum computing </demos/tutorial_neutral_atoms>`{.interpreted-text\nrole=\"doc\"} or\n`the tutorial on the QuEra Aquila device </demos/ahs_aquila>`{.interpreted-text\nrole=\"doc\"}, which treat pulse programming with Rydberg atoms.\n\nQuantum optimal control\n-----------------------\n\nThe overarching goal of quantum optimal control is to find the best way\nto steer a microscopical physical system such that its dynamics matches\na desired behaviour. The meaning of \\\"best\\\" and \\\"desired behaviour\\\"\nwill depend on the specific task, and it is important to specify the\nunderlying assumptions and constraints on the system controls in order\nto make the problem statement well-defined. Once we specified all these\ndetails, optimal control theory is concerned with questions like \\\"How\nclose can the system get to modelling the desired behaviour?\\\", \\\"How\ncan we find the best (sequence of) control parameters to obtain the\ndesired behaviour?\\\", or \\\"What is the shortest time in which the system\ncan reach a specific state, given some initial state?\\\" (controlling at\nthe so-called quantum speed limit).\n\nIn this tutorial, we consider the control of few-qubit systems through\npulse sequences, with the goal to produce a given target, namely a\ndigital gate, to the highest possible precision. To do this, we will\nchoose an ansatz for the pulse sequence that contains free parameters\nand define a profit function that quantifies the similarity between the\nqubit operation and the target gate. Then, we maximize this function by\noptimizing the pulse parameters until we find the desired gate to a\nsufficient precision\\--or can no longer improve on the approximation we\nfound. For the training phase, we will make use of fully-differentiable\nclassical simulations of the qubit dynamics, allowing us to make use of\nbackpropagation \\-- an efficient differentiation technique widely used\nin machine learning \\-- and gradient-based optimization. At the same\ntime we attempt to find pulse shapes and control parameters that are (to\nsome degree) realistically feasible, including bounded pulse amplitudes\nand rates of change of the amplitudes.\n\nTutorials that use other techniques are available, for example, for the\n[open-source quantum toolbox\nQuTiP](https://qutip.org/qutip-tutorials/#optimal-control).\n\nGate calibration via pulse programming\n--------------------------------------\n\nHere, we briefly discuss the general setup of pulse programs that we\nwill use for our optimal control application. For more details, you may\nperuse the related tutorials focusing on pulse programming.\n\nConsider a quantum system comprised of $n$ two-level systems, or qubits,\ndescribed by a Hamiltonian\n\n$$H(\\boldsymbol{p}, t) = H_d + \\sum_{i=1}^K f_i(\\boldsymbol{p_i}, t) H_i.$$\n\nAs we can see, $H$ depends on the time $t$ and on a set of control\nparameters $\\boldsymbol{p}$, which is composed of one parameter vector\n$\\boldsymbol{p_i}$ per term. Both $t$ and $\\boldsymbol{p}$ feed into\nfunctions $f_i$ that return scalar coefficients for the (constant)\nHamiltonian terms $H_i$. In addition, there is a constant drift\nHamiltonian $H_d$. We will assume that the Hamiltonian $H$ fully\ndescribes the system of interest and, in particular, we do not consider\nsources of noise in the system, such as leakage, dephasing, or\ncrosstalk, i.e. the accidental interaction with other parts of a larger,\nsurrounding system.\n\nThe time evolution of the state of our quantum system will be described\nby the Schr\u00f6dinger equation associated with $H$. However, for our\npurposes it will be more useful to consider the full unitary evolution\nthat the Hamiltonian causes, independently of the initial state. This\nway, we can compare it to the digital target gate without iterating over\ndifferent input and output states. The Schr\u00f6dinger equation dictates the\nbehaviour of the evolution operator $U$ to be\n\n$$\\frac{d}{dt} U(\\boldsymbol{p}, t) = -i H(\\boldsymbol{p}, t) U(\\boldsymbol{p}, t),$$\n\nwhere we implicitly fixed the initial time of the evolution to $t_0=0$.\nIt is possible to simulate the dynamics of sufficiently small quantum\nsystems on a classical computer by solving the ordinary differential\nequation (ODE) above numerically. For a fixed pulse duration $T$ and\ngiven control parameters $\\boldsymbol{p}$, a numerical ODE solver\ncomputes the matrix $U(\\boldsymbol{p}, T)$.\n\nHow can we tell whether the evolution of the qubit system is close to\nthe digital gate we aim to produce? We will need a measure of\nsimilarity, or fidelity.\n\nIn this tutorial we will describe the similarity of two unitary matrices\n$U$ and $V$ on $n$ qubits with a fidelity function:\n\n$$f(U,V) = \\frac{1}{2^n}\\big|\\operatorname{tr}(U^\\dagger V)\\big|.$$\n\nIt is similar to an overlap measure obtained from the [Frobenius\nnorm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) but it\nallows us to ignore differences in the global phase. Note that fidelity\nis often used to compare quantum states rather than gates, and that\nnoise often plays a role in this context. Here we only consider unitary\ngates.\n\nWe can maximize the fidelity function above to train the pulse\nparameters. For this purpose we write\n\n$$F(\\boldsymbol{p}) \\equiv f(U_\\text{target}, U(\\boldsymbol{p}, T)).$$\n\nHere $U_\\text{target}$ is the unitary matrix of the gate that we want to\ncompile. We consider the total duration $T$ as a fixed constraint to the\noptimization problem and therefore we do not denote it as a free\nparameter of $F$.\n\n| \n\n![](../demonstrations/optimal_control/OptimalControl_distance.png){.align-center\nwidth=\"100.0%\"}\n\n| \n\nWe can then maximize the fidelity $F$, for example, using gradient-based\noptimization algorithms like Adam. But how do we obtain the gradient of\na function that requires us to run an ODE solver to obtain its value? We\nare in luck! The implementation of pulse programming in PennyLane is\nfully differentiable via backpropagation thanks to its backend based on\nthe machine learning library\n[JAX](https://jax.readthedocs.io/en/latest/). This enables us to\noptimize the gate sequences using efficiently computed gradients\n(provided the target gate is not too large).\n\nBefore we climb mount fidelity for particular example gates, let\\'s\nbriefly talk about the pulse shape that we will use.\n\nSmooth rectangle pulses\n-----------------------\n\nLet\\'s look at a building block that we will use a lot: smoothened\nrectangular pulses. We start with a simple rectangular pulse\n\n$$R_\\infty(t, (\\Omega, t_0, t_1)) = \\Omega \\Theta(t-t_0) \\Theta(t_1-t)$$\n\nwhere $\\Omega$ is the amplitude, $t_0$ and $t_1$ are the start and end\ntimes of the pulse, and $\\Theta(t)$ is the [Heaviside step\nfunction](https://en.wikipedia.org/wiki/Heaviside_step_function) which\nis one for $t\\geq 0$ and zero otherwise. The trainable parameters of\nthis pulse are the amplitude and the start/end times.\n\nThere are two main issues with $R_\\infty$ for our purposes:\n\n1.  The Heaviside step function is not differentiable with respect to\n    the times $t_0$ and $t_1$ in the conventional sense (but only if we\n    were to consider distributions in addition to functions), and in\n    particular we cannot differentiate the resulting\n    $U(\\boldsymbol{p},T)$ within the automatic differentiation framework\n    provided by JAX.\n2.  The instantaneous change in the amplitude will not be realizable in\n    practice. In reality, the pulses describe some electromagnetic\n    control field that can only be changed at a bounded rate and in a\n    smooth manner. $R_\\infty$ is not only not smooth, it is not even\n    continuous. So we should consider smooth pulses with a bounded rate\n    of change instead.\n\nWe can solve both these issues by smoothening the rectangular pulse: We\nsimply replace the step functions above by a smooth variant, namely by\nsigmoid functions:\n\n$$\\begin{aligned}\nR_k(t, (\\Omega, t_0, t_1)) &= \\Omega S(t-t_0, k) S(t_1-t, k)\\\\\nS(t, k) &= (1+\\exp(-k t))^{-1}.\n\\end{aligned}$$\n\nWe introduced an additional parameter, $k$, that controls the steepness\nof the sigmoid functions and can be adapted to the constraints posed by\nhardware on the maximal rate of change. In contrast to $R_\\infty$, its\nsister $R_k$ is smooth in all three arguments $\\Omega$, $t_0$ and $t_1$,\nand training these three parameters with automatic differentiation will\nnot be a problem.\n\n| \n\n![](../demonstrations/optimal_control/OptimalControl_Smoother_Rectangles.png){.align-center\nwidth=\"100.0%\"}\n\n| \n\nLet\\'s implement the smooth rectangle function using JAX\\'s `numpy`. We\ndirectly implement the product of the two sigmoids in the function\n`sigmoid_rectangle`:\n\n$$R_k(t, (\\Omega, t_0, t_1), k)=\n\\Omega [1+\\exp(-k (t-t_0))+\\exp(-k (t_1-t))+\\exp(-k(t_1-t_0))]^{-1}.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import jax\nfrom jax import numpy as jnp\n\njax.config.update(\"jax_enable_x64\", True)  # Use float64 precision\njax.config.update(\"jax_platform_name\", \"cpu\")  # Disables a warning regarding device choice\n\n\ndef sigmoid_rectangle(t, Omega, t_0, t_1, k=1.0):\n    \"\"\"Smooth-rectangle pulse between t_0 and t_1, with amplitude Omega.\"\"\"\n    return Omega / (\n        1 + jnp.exp(-k * (t - t_0)) + jnp.exp(-k * (t_1 - t)) + jnp.exp(-k * (t_1 - t_0))\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s look at a rectangular pulse and its smoothened sister, for a\nnumber of different smoothness parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nt = jnp.linspace(0, 6, 1000)\nt_0, t_1 = (1.3, 5.4)\namplitude = 2.3\nks = [5, 10, 50]\nrect = amplitude * jnp.heaviside(t - t_0, 1.0) * jnp.heaviside(t_1 - t, 1.0)\n\nfor k in ks:\n    smooth = sigmoid_rectangle(t, amplitude, t_0, t_1, k)\n    plt.plot(t, smooth, label=f\"Smooth rectangle $R_k$, $k={k}$\")\nplt.plot(t, rect, label=\"Rectangle $R_{\\\\infty}$, $k\\\\to\\\\infty$\")\nplt.legend(bbox_to_anchor=(0.6, 0.05), loc=\"lower center\")\nplt.xlabel(\"time $t$\")\nplt.ylabel(\"Pulse function\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that for very large $k$, the smooth rectangle becomes practically\nindistinguishable from the original rectangle function $R_\\infty$. This\nmeans that we can consider the smooth $R_k$ a *generalization* of the\npulse shape, rather than a restriction.\n\nIn the examples below, we will use a pulse ansatz $S_k$ that sums\nmultiple smooth rectangles $R_k$ with the same value for $k$ but\nindividual start/end times $t_{0/1}$ and amplitudes $\\Omega$. With this\nnicely trainable pulse shape in our hands, we now turn to the first gate\ncalibration task.\n\nPulse ansatz for CNOT calibration\n=================================\n\nIn this first example we will tune a two-qubit pulse to produce a\nstandard CNOT gate.\n\nWe start by choosing a system Hamiltonian. It contains the drift term\n$H_d = Z_0 + Z_1$, i.e. a Pauli $Z$ operator acting on each qubit, with\na constant unit amplitude. The parametrized part uses five generating\nterms: Pauli $Z$ acting on the first qubit ($Z_0$), all three Pauli\noperators acting on the second qubit ($X_1, Y_1, Z_1$) and a single\ninteraction term $Z_0X_1$, resembling an abstract cross-resonance\ndriving term. For all coefficient functions we choose the same function,\n$f_i=S_k\\ \\forall i$ (see the section above), but with distinct\nparameters. That is, our Hamiltonian is\n\n$$H(\\boldsymbol{p}, t) = \\underset{H_d}{\\underbrace{Z_0 + Z_1}}\n+ S_k(\\boldsymbol{p_1}, t) Z_0\n+ S_k(\\boldsymbol{p_2}, t) X_1\n+ S_k(\\boldsymbol{p_3}, t) Y_1\n+ S_k(\\boldsymbol{p_4}, t) Z_1\n+ \\underset{\\text{interaction}}{\\underbrace{S_k(\\boldsymbol{p_5}, t) Z_0X_1}}$$\n\nDue to this choice, the $Z_0$ term commutes with all other terms,\nincluding the drift term, and can be considered a correction of the\ndrive term to obtain the correct action on the first qubit. Although the\ninteraction term was chosen to resemble a typical interaction in a\nsuperconducting cross resonance drive, this Hamiltonian remains a toy\nmodel. Realistic hardware Hamiltonians may impose additional constraints\nor provide fewer controls, and we do not consider the unit systems of\nsuch real-world systems here.\n\nThe idea behind using the sum of smooth rectangles function for the\nparametrization is the following: Many methods in quantum optimal\ncontrol work with discretized pulse shapes that keep the pulse envelope\nconstant for short time bins. This approach leads to a large number of\nparameters that need to be trained, and it requires us to manually\nenforce that the values do not differ by too much between neighbouring\ntime bins. The smooth rectangles introduced above have a limited rate of\nchange by design, and the number of parameters is much smaller than in\ngeneric discretization approaches. Each coefficient function $S_k$ sums\n$P$ smooth rectangles $R_k$ with individual amplitudes and start and end\ntimes. Overall, this leads to $n=5\\cdot 3\\cdot P=15P$ parameters in $H$.\nIn this and the next example, we chose $P$ heuristically.\n\nBefore we define this Hamiltonian, we implement the sum over multiple\n`sigmoid_rectangle` functions, including two normalization steps. First,\nwe normalize the start and end times of the rectangles to the interval\n$[\\epsilon, T-\\epsilon]$, which makes sure that the pulse amplitudes are\nclose to zero at $t=0$ and $t=T$. Without this step, we might be tuning\nthe pulses to be turned on (off) instantaneously at the beginning (end)\nof the sequence, negating our effort on the pulse shape itself not to\nvary too quickly. Second, we normalize the final output value to the\ninterval $(-\\Omega_\\text{max}, \\Omega_\\text{max})$, which allows us to\nbound the maximal amplitudes of the pulses to a realizable range while\nmaintaining differentiability.\n\nFor the normalization steps, we define a `sigmoid` and a `normalize`\nfunction. The first is a straightforward implementation of $R_k$ whereas\nthe second uses the `sigmoid` function to normalize real numbers to the\ninterval $(-1, 1)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sigmoid(t, k=1.0):\n    \"\"\"Sigmoid function with steepness parameter k.\"\"\"\n    return 1 / (1 + jnp.exp(-k * t))\n\n\ndef normalize(t, k=1.0):\n    \"\"\"Smoothly normalize a real input value to the interval (-1, 1) using 'sigmoid'\n    with steepness parameter k.\"\"\"\n    return 2 * sigmoid(t, k) - 1.0\n\n\ndef smooth_rectangles(params, t, k=2.0, max_amp=1.0, eps=0.0, T=1.0):\n    \"\"\"Compute the sum of P smooth-rectangle pulses and normalize their\n    starting and ending times, as well as the total output amplitude.\n\n    Args:\n        params (tensor_like): Amplitudes and start and end times for the rectangles,\n            in the order '[amp_1, ... amp_P, t_{1, 0}, t_{1, 1}, ... t_{P, 0}, t_{P, 1}]'.\n        t (float): Time at which to evaluate the pulse function.\n        k (float): Steepness of the sigmoid functions that delimit the rectangles\n        max_amp (float): Maximal amplitude of the rectangles. The output will be normalized\n            to the interval '(-max_amp, max_amp)'.\n        eps (float): Margin to the beginning and end of the pulse sequence within which the\n            start and end times of the individual rectangles need to lie.\n        T (float): Total duration of the pulse.\n\n    Returns:\n        float: Value of sum of smooth-rectangle pulses at 't' for the given parameters.\n    \"\"\"\n    P = len(params) // 3\n    # Split amplitudes from times\n    amps, times = jnp.split(params, [P])\n    # Normalize times to be sufficiently far away from 0 and T\n    times = sigmoid(times - T / 2, k=1.0) * (T - 2 * eps) + eps\n    # Extract the start and end times of single rectangles\n    times = jnp.reshape(times, (-1, 2))\n    # Sum products of sigmoids (unit rectangles), rescaled with the amplitudes\n    rectangles = [sigmoid_rectangle(t, amp, *ts, k) for amp, ts in zip(amps, times)]\n    value = jnp.sum(jnp.array([rectangles]))\n    # Normalize the output value to be in [-max_amp, max_amp] with standard steepness\n    return max_amp * normalize(value, k=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s look at this function for some example parameters, with the same\nsteepness parameter $k=20$ for all rectangles in the sum:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\n\nT = 2 * jnp.pi  # Total pulse sequence time\nk = 20.0  # Steepness parameter\nmax_amp = 1.0  # Maximal amplitude \\Omega_{max}\neps = 0.1 * T  # Margin for the start/end times of the rectangles\n# Bind hyperparameters to the smooth_rectangles function\nS_k = partial(smooth_rectangles, k=k, max_amp=max_amp, eps=eps, T=T)\n\n# Set some arbitrary amplitudes and times\namps = jnp.array([0.4, -0.2, 1.9, -2.0])  # Four amplitudes\ntimes = jnp.array([0.2, 0.6, 1.2, 1.8, 2.1, 3.7, 4.9, 5.9])  # Four pairs of start/end times\nparams = jnp.hstack([amps, times])  # Amplitudes and times form the trainable parameters\n\nplot_times = jnp.linspace(0, T, 300)\nplot_S_k = [S_k(params, t) for t in plot_times]\n\nplt.plot(plot_times, plot_S_k)\nax = plt.gca()\nax.set(xlabel=\"Time t\", ylabel=r\"Pulse function $S_k(p, t)$\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the rectangles are rather round for these generic parameters.\nThe optimized parameters in the training workflows below will lead to\nmore sharply defined pulses that resemble rectangles more closely. The\namplitude normalization step in `smooth_rectangles` enables us to\nproduce them in a differentiable manner, as was our goal with\nintroducing $R_k$. Also note that the normalization of the final output\nvalue is not a simple clipping step, but again a smooth function. As a\nconsequence, the amplitudes `1.9` and `-2.` in the example above, which\nare not in the interval `[-1, 1]`, are not set to `1` and `-1` but take\nsmaller absolute values. Finally, also note that the start and end times\nof the smooth rectangles are being normalized as well, in order to not\nend up too close to the boundaries of the total time interval. While\nthis makes the pulse times differ from the input times, our pulse\ntraining will automatically consider this normalization step so that it\nhas no major consequences for us.\n\nUsing this function, we now may build the parametrized pulse Hamiltonian\nand the fidelity function discussed above. We make use of just-in-time\n(JIT) compilation, which will make the first execution of `profit` and\n`grad` slower, but speed up the subsequent executions a lot. For\noptimization workflows of small-scale functions, this almost always pays\noff.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n\nX, Y, Z = qml.PauliX, qml.PauliY, qml.PauliZ\n\nnum_wires = 2\n# Hamiltonian terms of the drift and parametrized parts of H\nops_H_d = [Z(0), Z(1)]\nops_param = [Z(0), X(1), Y(1), Z(1), Z(0) @ X(1)]\n# Coefficients: 1 for drift Hamiltonian and smooth rectangles for parametrized part\ncoeffs = [1.0, 1.0] + [S_k for op in ops_param]\n# Build H\nH = qml.dot(coeffs, ops_H_d + ops_param)\n# Set tolerances for the ODE solver\natol = rtol = 1e-10\n\n# Target unitary is CNOT. We get its matrix and note that we do not need the dagger\n# because CNOT is Hermitian.\ntarget = qml.CNOT([0, 1]).matrix()\ntarget_name = \"CNOT\"\nprint(f\"Our target unitary is a {target_name} gate, with matrix\\n{target.astype('int')}\")\n\n\ndef pulse_matrix(params):\n    \"\"\"Compute the unitary time evolution matrix of the pulse for given parameters.\"\"\"\n    return qml.evolve(H, atol=atol, rtol=rtol)(params, T).matrix()\n\n\n@jax.jit\ndef profit(params):\n    \"\"\"Compute the fidelity function for given parameters.\"\"\"\n    # Compute the unitary time evolution of the pulse Hamiltonian\n    op_mat = pulse_matrix(params)\n    # Compute the fidelity between the target and the pulse evolution\n    return jnp.abs(jnp.trace(target.conj().T @ op_mat)) / 2**num_wires\n\n\ngrad = jax.jit(jax.grad(profit))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the arbitrary parameters from above, of course we get a rather\narbitrary unitary time evolution, which does not match the CNOT at all:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = [params] * len(ops_param)\narb_mat = jnp.round(pulse_matrix(params), 4)\narb_profit = profit(params)\nprint(\n    f\"The arbitrarily chosen parameters yield the unitary\\n{arb_mat}\\n\"\n    f\"which has a fidelity of {arb_profit:.6f}.\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can start the optimization, we require initial parameters. We\nset small alternating amplitudes and evenly distributed start and end\ntimes for $P=3$ smoothened rectangles. This choice leads to a total of\n$15P=45$ parameters in the pulse sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P = 3  # Number of rectangles P\n# Initial parameters for the start and end times of the rectangles\ntimes = [jnp.linspace(eps, T - eps, P * 2) for op in ops_param]\n# All initial parameters: small alternating amplitudes and times\nparams = [jnp.hstack([[0.1 * (-1) ** i for i in range(P)], time]) for time in times]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are all set up to train the parameters of the pulse sequence to\nproduce our target gate, the CNOT. We will use the Adam optimizer,\nimplemented in the [optax](https://optax.readthedocs.io/en/latest/)\nlibrary to our convenience. We keep track of the optimization via a list\nthat contains the parameters and fidelity values. Then we can plot the\nfidelity across the optimization. As we will run a second optimization\nlater on, we code up the optimizer run as a function. This function will\nreport on the optimization progress and duration, and it will plot the\ntrajectory of the profit function during the optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport optax\n\n\ndef run_adam(profit_fn, grad_fn, params, learning_rate, num_steps):\n    start_time = time.process_time()\n    # Initialize the Adam optimizer\n    optimizer = optax.adam(learning_rate, b1=0.97)\n    opt_state = optimizer.init(params)\n    # Initialize a memory buffer for the optimization\n    hist = [(params.copy(), profit_fn(params))]\n    for step in range(num_steps):\n        g = grad_fn(params)\n        updates, opt_state = optimizer.update(g, opt_state, params)\n\n        params = optax.apply_updates(params, updates)\n        hist.append([params, c := profit_fn(params)])\n        if (step + 1) % (num_steps // 10) == 0:\n            print(f\"Step {step+1:4d}: {c:.6f}\")\n    _, profit_hist = list(zip(*hist))\n    plt.plot(list(range(num_steps + 1)), profit_hist)\n    ax = plt.gca()\n    ax.set(xlabel=\"Iteration\", ylabel=f\"Fidelity $F(p)$\")\n    plt.show()\n    end_time = time.process_time()\n    print(f\"The optimization took {end_time-start_time:.1f} (CPU) seconds.\")\n    return hist\n\n\nlearning_rate = -0.2  # negative learning rate leads to maximization\nnum_steps = 500\nhist = run_adam(profit, grad, params, learning_rate, num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, Adam steadily increases the fidelity, bringing the pulse\nprogram closer and closer to the target unitary. On its way, the\noptimizer produces a mild oscillating behaviour. The precision to which\nthe optimization can produce the target unitary depends on the\nexpressivity of the pulses we use, but also on the precision with which\nwe run the ODE solver and the hyperparameters of the optimizer.\n\nLet\\'s pick those parameters with the largest fidelity we observed\nduring the training and take a look at the pulses we found. We again\nprepare a function that plots the pulse sequence, which we can reuse\nlater on. For the single-qubit terms, we encode their qubit in the color\nand the type of Pauli operator in the line style of the plotted line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "colors = {0: \"#70CEFF\", 1: \"#C756B2\", 2: \"#FDC357\"}\ndashes = {\"X\": [10, 0], \"Y\": [2, 2, 10, 2], \"Z\": [6, 2]}\n\n\ndef plot_optimal_pulses(hist, pulse_fn, ops, T, target_name):\n    _, profit_hist = list(zip(*hist))\n    fig, axs = plt.subplots(2, 1, figsize=(10, 9), gridspec_kw={\"hspace\": 0.0}, sharex=True)\n\n    # Pick optimal parameters from the buffer of all observed profit values\n    max_params, max_profit = hist[jnp.argmax(jnp.array(profit_hist))]\n    plot_times = jnp.linspace(0, T, 300)\n    # Iterate over pulse parameters and parametrized operators\n    for p, op in zip(max_params, ops):\n        # Create label, and pick correct axis\n        label = op.name\n        ax = axs[0] if isinstance(label, str) else axs[1]\n        # Convert the label into a concise string. This differs depending on\n        # whether the operator has a single or multiple Pauli terms. Pick the line style\n        if isinstance(label, str):\n            label = f\"${label[-1]}_{op.wires[0]}$\"\n            dash = dashes[label[1]]\n        else:\n            label = \"$\" + \" \".join([f\"{n[-1]}_{w}\" for w, n in zip(op.wires, label)]) + \"$\"\n            dash = [10, 0]\n\n        # Set color according to qubit the term acts on\n        col = colors[op.wires[0]]\n        # Plot the pulse\n        values = [pulse_fn(p, t) for t in plot_times]\n        ax.plot(plot_times, values, label=label, dashes=dash, color=col)\n    ax.legend()\n    # Set legends and axis descriptions\n    axs[0].legend(title=\"Single-qubit terms\", ncol=int(jnp.sqrt(len(ops))))\n    axs[1].legend(title=\"Two-qubit terms\")\n    title = f\"{target_name}, Fidelity={max_profit:.6f}\"\n    axs[0].set(ylabel=r\"Pulse function $f(p, t)$\", title=title)\n    axs[1].set(xlabel=\"Time $t$\", ylabel=r\"Pulse function $S_k(p, t)$\")\n    plt.show()\n\n\nplot_optimal_pulses(hist, S_k, ops_param, T, target_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that a single rectangular pulse is sufficient for some of the\ngenerating terms in the Hamiltonian whereas others end up at rather\nintricate pulse shapes. We see that their shape is closer to actual\nrectangles now, in particular for those with a saturated amplitude.\n\nThe final fidelity tells us that we achieved our goal of finding a pulse\nsequence that implements a unitary close to a CNOT gate. It could be\noptimized further, for example by running the optimization for more\ntraining iterations, by tuning the optimizer further to avoid\noscillations, or by increasing the precision with which we run the ODE\nsolver. This likely would also allow to reduce the total duration of the\npulse.\n\nPulse sequence for Toffoli\n==========================\n\nThe second example we consider is the compilation of a Toffoli\\--or\nCCNOT\\--gate. We reuse most of the workflow from above and only change\nthe pulse Hamiltonian as well as a few hyperparameters.\n\nIn particular, the Hamiltonian uses the drift term $H_d=Z_0+Z_1+Z_2$ and\nthe generators are all single-qubit Pauli operators on all three qubits,\ntogether with the interaction generators $Z_0X_1, Z_1X_2, Z_2X_0$.\nAgain, all parametrized terms use the coefficient function\n`smooth_rectangles`. We allow for a longer pulse duration of $3\\pi$ and\nfive smooth rectangles in each pulse shape.\n\nIn summary, we use nine single-qubit generators and three two-qubit\ngenerators, with five rectangles in each pulse shape and each rectangle\nbeing given by an amplitude and a start and end time. The pulse sequence\nthus has $(9+3)\\cdot 5\\cdot 3=180$ parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_wires = 3\n# New pulse hyperparameters\nT = 3 * jnp.pi  # Longer total duration\neps = 0.1 * T\nP = 5  # More rectangles in sum: P=5\nS_k = partial(smooth_rectangles, k=k, max_amp=max_amp, eps=eps, T=T)\n\n# Hamiltonian terms of the drift and parametrized parts of H\nops_H_d = [Z(0), Z(1), Z(2)]\nops_param = [pauli_op(w) for pauli_op in [X, Y, Z] for w in range(num_wires)]\nops_param += [Z(0) @ X(1), Z(1) @ X(2), Z(2) @ X(0)]\n\n# Coefficients: 1. for drift Hamiltonian and smooth rectangles for parametrized part\ncoeffs = [1.0, 1.0, 1.0] + [S_k for op in ops_param]\n# Build H\nH = qml.dot(coeffs, ops_H_d + ops_param)\n# Set tolerances for the ODE solver\natol = rtol = 1e-10\n\n# Target unitary is Toffoli. We get its matrix and note that we do not need the dagger\n# because Toffoli is Hermitian and unitary.\ntarget = qml.Toffoli([0, 1, 2]).matrix()\ntarget_name = \"Toffoli\"\nprint(f\"Our target unitary is a {target_name} gate, with matrix\\n{target.astype('int')}\")\n\n\ndef pulse_matrix(params):\n    \"\"\"Compute the unitary time evolution matrix of the pulse for given parameters.\"\"\"\n    return qml.evolve(H, atol=atol, rtol=rtol)(params, T).matrix()\n\n\n@jax.jit\ndef profit(params):\n    \"\"\"Compute the fidelity function for given parameters.\"\"\"\n    # Compute the unitary time evolution of the pulse Hamiltonian\n    op_mat = pulse_matrix(params)\n    # Compute the fidelity between the target and the pulse evolution\n    return jnp.abs(jnp.trace(target.conj().T @ op_mat)) / 2**num_wires\n\n\ngrad = jax.jit(jax.grad(profit))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create initial parameters similar to the above but allow for a larger\nnumber of $1200$ optimization steps and use a reduced learning rate (by\nabsolute value) in the optimization with Adam. Our `run_adam` function\nfrom above comes in handy and also provides an overview of the\noptimization process in the produced plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "times = [jnp.linspace(eps, T - eps, P * 2) for op in ops_param]\nparams = [jnp.hstack([[0.2 * (-1) ** i for i in range(P)], time]) for time in times]\n\nnum_steps = 1200\nlearning_rate = -2e-3\nhist = run_adam(profit, grad, params, learning_rate, num_steps)\n\nparams_hist, profit_hist = list(zip(*hist))\nmax_params = params_hist[jnp.argmax(jnp.array(profit_hist))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks promising: Adam maximized the fidelity successfully and we\nthus compiled a pulse sequence that implements a Toffoli gate! To\ninspect how close the compiled pulse sequence is to the Toffoli gate, we\ncan apply it to an exemplary quantum state, say $|110\\rangle$, and\ninvestigate the returned probabilities. A perfect Toffoli gate would\nflip the third qubit, returning a probability of one in the last entry\nand zeros elsewhere.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit.jax\", wires=3)\n\n\n@qml.qnode(dev, interface=\"jax\")\ndef node(params):\n    # Prepare |110>\n    qml.PauliX(0)\n    qml.PauliX(1)\n    # Apply pulse sequence\n    qml.evolve(H, atol=atol, rtol=rtol)(params, T)\n    # Return quantum state\n    return qml.probs()\n\n\nprobs = node(max_params)\nprint(f\"The state |110> is mapped to the probability vector\\n{jnp.round(probs, 6)}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the returned probabilities are close to the expected vector.\nThe last entry is close to one, the others are almost zero. However,\nthere are more possible inputs to the gate, and we hardly want to stare\nat eight probability vectors to understand the quality of the compiled\npulse sequence. Instead, let\\'s plot the transition amplitudes with\nwhich our compiled pulse sequence maps computational basis vectors to\neach other. We include the complex phase of the amplitudes in the color\nof the bars.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection=\"3d\")\n\ndim = 8\nx = jnp.tile(jnp.arange(dim), dim) - 0.27  # Input state indices\ny = jnp.repeat(jnp.arange(dim), dim) - 0.36  # Output state indices\nmat = pulse_matrix(max_params).ravel()  # Pulse matrix, reshaped to be a sequence of values\nphases = jnp.angle(mat)  # Complex phases\ncolor_norm = mpl.colors.Normalize(-jnp.pi, jnp.pi)\nbar_colors = mpl.cm.turbo(color_norm(phases))\n# Barplot with x, y positions, bottom, width, depth and height values for bars\nax.bar3d(x, y, 0.0, 0.6, 0.6, jnp.abs(mat).ravel(), shade=True, color=bar_colors)\n# Specify a few visual attributes of the axes object\nax.set(\n    xticks=list(range(dim)),\n    yticks=list(range(dim)),\n    xticklabels=[f\"|{bin(i)[2:].rjust(3, '0')}>\" for i in range(dim)],\n    yticklabels=[f\"|{bin(i)[2:].rjust(3, '0')}>\" for i in range(dim)],\n    zticks=[0.2 * i for i in range(6)],\n    xlabel=\"Input state\",\n    ylabel=\"Output state\",\n)\n# Add axes for the colorbar\ncax = plt.axes([0.85, 0.15, 0.02, 0.62])\nsc = mpl.cm.ScalarMappable(cmap=mpl.cm.turbo, norm=color_norm)\nsc.set_array([])\n# Plot colorbar\nplt.colorbar(sc, cax=cax, ax=ax)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transition amplitudes are as expected, except for very small\ndeviations. All computational basis states are mapped to themselves, but\nthe last two are swapped. The color of the entries close to one does not\ncorrespond to a phase of zero. However, the fact that they have the same\ncolor tells us that this deviation is a global phase, so that the pulse\nsequence is equivalent to the Toffoli gate. Let\\'s also look at the\npulse sequence itself:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_optimal_pulses(hist, S_k, ops_param, T, target_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the optimized smooth rectangles do not fill out the time\nat maximal amplitudes. This means that we probably can find shorter\npulse sequences with larger amplitudes that produce a Toffoli with the\nsame fidelity. If you are interested, take a shot at it and try to\noptimize the sequence with respect to the number of generators and pulse\nduration!\n\nConclusion\n==========\n\nIn this tutorial we calibrated a two-qubit and a three-qubit pulse\nsequence to obtain a CNOT and a Toffoli gate, respectively. For this, we\nused smooth rectangular pulse shapes together with toy pulse\nHamiltonians, and obtained very good approximations to the target gates.\nThanks to JAX, just-in-time (JIT) compiling and the PennyLane `pulse`\nmodule, training the pulse sequences was simple to implement and fast to\nrun.\n\nThere are many different techniques in quantum optimal control that can\nbe used to calibrate pulse sequences, some of which include\ngradient-based training. A widely-used technique called GRAPE[^1] makes\nuse of discretized pulses, which leads to a large number of free\nparameters to be optimized with gradient ascent. The technique shown\nhere reduces the parameter count significantly and provides smooth,\nbounded shapes by definition.\n\nYet another method that does *not* use gradient-based optimization is\nthe chopped random-basis quantum optimization (CRAB) algorithm[^2]. It\nuses a different parametrization altogether, exploiting randomized basis\nfunctions for the pulse envelopes.\n\nWhile setting up the application examples, we accommodated for some\nrequirements of realistic hardware, like smooth pulse shapes with\nbounded maximal amplitudes and bounded rates of change, and we tried to\nuse only few interaction terms between qubits. However, it is important\nto note that the shown optimization remains a toy model for calibration\nof quantum hardware. We did not take into account the interaction terms\nor pulse shapes available on realistic devices and their control\nelectronics. We also did not consider a unit system tied to real\ndevices, and we ignored noise, which plays a very important role in\ntoday\\'s quantum devices and in quantum optimal control. We leave the\nextension to real-world pulse Hamiltonians and noisy systems to a future\ntutorial\\--or maybe your work?\n\nReferences\n==========\n\nAbout the author\n================\n\n[^1]: N. Khaneja, T. Reiss, C. Kehlet, T. Schulte-Herbr\u00fcggen, S.J.\n    Glaser \\\"Optimal Control of Coupled Spin Dynamics: Design of NMR\n    Pulse Sequences by Gradient Ascent Algorithms\\\" [J. Magn. Reson.\n    172,\n    296-305](https://www.ch.nat.tum.de/fileadmin/w00bzu/ocnmr/pdf/94_GRAPE_JMR_05_.pdf),\n    2005\n\n[^2]: P. Doria, T. Calarco and S. Montangero \\\"Optimal Control Technique\n    for Many-Body Quantum Dynamics\\\" [Phys. Rev. Lett. 106,\n    190501](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.106.190501),\n    [arxiv:1003.3750](https://arxiv.org/abs/1003.3750), 2011\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}