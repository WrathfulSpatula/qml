{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contextuality and inductive bias in QML\n=======================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Train a tailored quantum model on a\ncontextuality-inspired dataset :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/contextuality_thumbnail.png>\n:::\n\n::: {.related}\ntutorial\\_geometric\\_qml\n:::\n\n*Author: Joseph Bowles --- Posted: 21 March 2023*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What machine learning problems are quantum computers likely to excel at?\n\nIn the article *Contextuality and inductive bias in quantum machine\nlearning* by Joseph Bowles, Victoria J Wright, M\u00e1t\u00e9 Farkas, Nathan\nKilloran and Maria Schuld, we look to contextuality for answers to this\nquestion.\n\nContextuality is a nonclassical phenomenon exhibited by quantum systems,\nand it is necessary for computational advantage relative to classical\nmachines. To be a little more specific, we focus on the framework of\n*generalized contextuality*, which was introduced by Robert Spekkens in\n2004. We find learning problems for which contextuality plays a key\nrole, and these problems may therefore be good areas where quantum\nmachine learning algorithms shine. In this demo we will:\n\n-   Describe a specific example of a contextuality-relevant problem that\n    is based on the well-known rock-paper-scissors game, and\n-   Construct and train a quantum model that is tailored to the\n    symmetries of the problem.\n\nThroughout the demo we will make use of JAX to vectorise and\njust-in-time compile certain functions, which will speed things up. For\nmore information on how to combine JAX and PennyLane, see the PennyLane\n[documentation](https://docs.pennylane.ai/en/stable/introduction/interfaces/jax.html).\n\n![](../demonstrations/contextuality/socialthumbnail_large_Contextuality.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generalized contextuality\n=========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we want to prepare the maximally mixed state of a single qubit,\nwith $\\rho = \\frac{1}{2}\\mathbb{I}$. Although this corresponds to a\nsingle density matrix, there are many ways we could prepare the state.\nFor example, we could mix the states $\\vert 0 \\rangle$ or\n$\\vert 1 \\rangle$ with equal probability. Alternatively, we could use\nthe $X$ basis, and mix the states $\\vert + \\rangle$ or\n$\\vert - \\rangle$. Even though this may not strike us as particularly\nstrange, a remarkable coincidence is in fact going on here: an\nexperimentalist can perform two physically distinct procedures (namely,\npreparing $\\rho$ in the $Z$ or $X$ basis), however it is impossible to\ndistinguish which procedure was performed, since they both result in the\nsame density matrix and therefore give identical predictions for all\nfuture measurements.\n\nSuch a coincidence demands an explanation. Something that one might\nexpect is the following: the description of the experiment in terms of\nquantum states is not the most fundamental, and there are in fact other\nstates (we'll write them as $\\lambda$), that comprise our quantum\nstates. In contextuality these are called *ontic states*, although they\nalso go by the name of *hidden variables*. When we prepare a state\n$\\vert 0 \\rangle$, $\\vert 1 \\rangle$, $\\vert + \\rangle$,\n$\\vert - \\rangle$, what is really going on is that we prepare a mixture\n$P_{\\vert 0 \\rangle}(\\lambda)$, $P_{\\vert 1 \\rangle}(\\lambda)$,\n$P_{\\vert + \\rangle}(\\lambda)$, $P_{\\vert - \\rangle}(\\lambda)$ over the\ntrue ontic states. One may imagine that the corresponding mixtures over\nthe $\\lambda$ s are the same for the $Z$ and $X$ basis preparation:\n\n$$\\frac{1}{2}P_{\\vert 0 \\rangle}(\\lambda)+\\frac{1}{2}P_{\\vert 1 \\rangle}(\\lambda)=\\frac{1}{2}P_{\\vert + \\rangle}(\\lambda)+\\frac{1}{2}P_{\\vert - \\rangle}(\\lambda).$$\n\nThis is a rather natural explanation of our coincidence: the two\nprocedures are indistinguishable because they actually correspond to the\nsame mixture over the fundamental states $\\lambda$. This sort of\nexplanation is called *non-contextual*, since the two mixtures do not\ndepend on the basis (that is, the context) in which the state is\nprepared. It turns out that if one tries to apply this logic to all the\nindistinguishabilities in quantum theory, one arrives at contradictions:\nit simply cannot be done. For this reason we say that quantum theory is\na *contextual* theory.\n\nIn the paper we frame generalized contextuality in the machine learning\nsetting, which allows us to define what we mean by a contextual learning\nmodel. In a nutshell, this definition demands that if a learning model\nis non-contextual, then any indistinguishabilities in the model should\nbe explained in a non-contextual way similar to the above. This results\nin constraints on the learning model, which limits their expressivity.\nSince quantum models are contextual, they can of course go beyond these\nconstraints, and understanding when and how they do this may shed light\non the non-classical features that separate quantum models from\nclassical ones.\n\nBelow we describe a specific learning problem that demonstrates this\napproach. As we will see, the corresponding indistinguishability relates\nto an *inductive bias* of the learning model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rock-paper-scissors game\n============================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learning problem we will consider involves three players (we\\'ll\ncall them players 0, 1 and 2) playing a variant of the\nrock-paper-scissors game with a referee. The game goes as follows. In\neach round, a player can choose to play either rock (R), paper (P) or\nscissors (S). Each player also has a 'special' action. For player 0 it\nis R, for player 1 it is P and for player 2 it is S. The actions of the\nplayers are then compared pairwise, with the following rules:\n\n-   If two players play different actions, then one player beats the\n    other following the usual rule (rock beats scissors, scissors beats\n    paper, paper beats rock).\n-   If two players play the same action, the one who plays their special\n    action beats the other. If neither plays their special action, it is\n    a draw.\n\nA referee then decides the winners and the losers of that round: the\nwinners receive $\\$1$ and the losers lose $\\$1$ (we will call this their\n*payoff* for that round).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/contextuality/rps.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naturally, the more players a given player beats, the higher the\nprobability that they get a positive payoff. In particular, if we denote\nthe payoff of player $k$ by $y_k=\\pm1$ then\n\n$$\\mathbb{E}(y_k) = \\frac{n^k_{\\text{win}}-n^k_{\\text{lose}}}{2},$$\n\nwhere $n^k_{\\text{win}}$, $n^k_{\\text{lose}}$ is the number of players\nthat player $k$ beats or loses to in that round. This ensures that a\nplayer is certain to get a positive (or negative) payoff if they beat\n(or lose) to everyone.\n\nTo make this concrete, we will construct three 3x3 matrices `A01`,\n`A02`, `A12` which determine the rules for each pair of players. `A01`\ncontains the expected payoff values of player 0 when playing against\nplayer 1. Using the rules of the game it looks as follows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/contextuality/rpstable.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The matrices `A02` and `A12` are defined similarly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import jax\nimport jax.numpy as jnp\nimport pennylane as qml\nimport numpy as np\njax.config.update(\"jax_platform_name\", \"cpu\")\nnp.random.seed(666) # seed used for random functions\n\nA01 = np.array([[1, -1, 1], [1, -1, -1], [-1, 1, 0]])  # rules for player 0 vs player 1\nA02 = np.array([[1, -1, 1], [1, 0, -1], [-1, 1, -1]])\nA12 = np.array([[0, -1, 1], [1, 1, -1], [-1, 1, -1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also define the matrices `A10`, `A20`, `A21`. Since switching the\nplayers corresponds to taking the transpose matrix and a positive payoff\nfor one player implies a negative for the other, these matrices are\ngiven by the negative of the transposed matrix:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "A10 = -A01.T  # rules for player 1 vs player 0\nA20 = -A02.T\nA21 = -A12.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the above game is an example of a *zero-sum game*: if player 1\nbeats player 2 then necessarily player 2 loses to player 1. This implies\n$\\sum_k n^k_{\\text{wins}}=\\sum_kn^k_{\\text{lose}}$ and so in every round\nwe have\n\n$$\\mathbb{E}(y_1)+\\mathbb{E}(y_2)+\\mathbb{E}(y_3)=0.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Constructing the dataset \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we construct a dataset based on the above game. Our data points\ncorrespond to probability distributions over possible actions: in the\nzero-sum game literature these are called *strategies*. For example, a\nstrategy for player k is a vector\n\n$$x_k=(P(a_k=R), P(a_k=P), P(a_k=S))$$\n\nwhere $a_k$ denotes player $k$'s action. We collect these into a\nstrategy matrix X\n\n$$\\begin{aligned}\nX = \\begin{pmatrix}\n    P(a_0=R) & P(a_0=P) & P(a_0=S) \\\\\n    P(a_1=R) & P(a_1=P) & P(a_1=S) \\\\\n    P(a_2=R) & P(a_2=P) & P(a_2=S) .\n    \\end{pmatrix}\n\\end{aligned}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's write a function to generate a set of strategy matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_strategy_matrices(N):\n    \"\"\"\n    Generates N strategy matrices, normalised by row\n    \"\"\"\n    X = np.random.rand(N, 3, 3)\n    for i in range(N):\n        norm = np.array(X[i].sum(axis=1))\n        for k in range(3):\n            X[i, k, :] = X[i, k, :] / norm[k]\n    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The labels in our dataset correspond to payoff values $y_k$ of the three\nplayers. Following the rules of probability we find that if the players\nuse strategies $x_0, x_1, x_2$ the expected values of\n$n_{\\text{wins}}^k - n_{\\text{lose}}^k$ are given by\n\n$$\\mathbb{E}[n_{\\text{wins}}^0 - n_{\\text{lose}}^0]  = x_0 \\cdot A_{01}\\cdot x_1^T+x_0 \\cdot A_{02}\\cdot x_2^T$$\n\n$$\\mathbb{E}[n_{\\text{wins}}^1 - n_{\\text{lose}}^1] = x_1 \\cdot A_{10}\\cdot x_0^T+x_1 \\cdot A_{12}\\cdot x_2^T$$\n\n$$\\mathbb{E}[n_{\\text{wins}}^2 - n_{\\text{lose}}^2] = x_2 \\cdot A_{20}\\cdot x_0^T+x_2 \\cdot A_{21}\\cdot x_1^T$$\n\nSince we have seen that\n$\\mathbb{E}(y_k) = \\frac{n^k_{\\text{win}}-n^k_{\\text{lose}}}{2}$ it\nfollows that the probability for player $k$ to receive a positive payoff\ngiven strategies $X$ is\n\n$$P(y_k=+1\\vert X) = \\frac{\\mathbb{E}(y_k\\vert X)+1}{2} =  \\frac{(\\mathbb{E}[n_{\\text{wins}}^k - n_{\\text{lose}}^k])/2+1}{2}$$\n\nPutting all this together we can write some code to generate the labels\nfor our data set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def payoff_probs(X):\n    \"\"\"\n    get the payoff probabilities for each player given a strategy matrix X\n    \"\"\"\n    n0 = X[0] @ A01 @ X[1] + X[0] @ A02 @ X[2]  # n0 = <n0_wins - n0_lose>\n    n1 = X[1] @ A10 @ X[0] + X[1] @ A12 @ X[2]\n    n2 = X[2] @ A20 @ X[0] + X[2] @ A21 @ X[1]\n    probs = (jnp.array([n0, n1, n2]) / 2 + 1) / 2\n    return probs\n\n\n# JAX vectorisation\nvpayoff_probs = jax.vmap(payoff_probs)\n\n\ndef generate_data(N):\n    X = get_strategy_matrices(N)  # strategies\n    P = vpayoff_probs(X)  # payoff probabilities\n    r = np.random.rand(*P.shape)\n    Y = np.where(P > r, 1, -1)  # sampled payoffs for data labels\n    return X, Y, P\n\n\nX, Y, P = generate_data(2000)\n\nprint(X[0])  # the first strategy matrix in our dataset\nprint(Y[0])  # the corresponding sampled payoff values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that since strategies are probabilistic mixtures of actions, our\ndata labels satisfy a zero-sum condition\n\n$$\\mathbb{E}(y_1\\vert X_i)+\\mathbb{E}(y_2\\vert X_i)+\\mathbb{E}(y_3\\vert X_i)=0.$$\n\nWe can verify this using the payoff probability matrix `P` that we used\nto sample the labels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "expvals = 2 * P - 1  # convert probs to expvals\nexpvals[:10].sum(axis=1)  # check first 10 entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learning problem\n====================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we are given a data set $\\{X_i,\\vec{y}_i\\}$ consisting of\nstrategy matrices and payoff values, however we don't know what the\nunderlying game is (that is, we don't know the players were playing the\nrock, paper scissors game described above). We do have one piece of\ninformation though: we know the game is zero-sum so that the data\ngeneration process satisfies\n\n$$\\mathbb{E}(y_0\\vert X_i)+\\mathbb{E}(y_1\\vert X_i)+\\mathbb{E}(y_2\\vert X_i)=0.$$\n\nCan we learn the rock, paper scissors game from this data? More\nprecisely, if we are given an unseen strategy matrix $X_{\\text{test}}$\nour task is to sample from the three distributions\n\n$$P(y_0\\vert X_{\\text{test}}), P(y_1\\vert X_{\\text{test}}), P(y_2\\vert X_{\\text{test}}).$$\n\nNote we are not asking to sample from the joint distribution\n$P(\\vec{y}\\vert X_{\\text{test}})$ but the three marginal distributions\nonly. This can be seen as an instance of multi-task learning, where a\nsingle task corresponds to sampling the payoff for one of the three\nplayers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building inductive bias into a quantum model\n============================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we describe a simple three qubit model to tackle this problem.\nSince we know that the data satisfies the zero-sum condition, we aim to\ncreate a quantum model that encodes this knowledge. That is, like the\ndata we want our model to satisfy\n\n$$\\mathbb{E}(y_0\\vert X_i)+\\mathbb{E}(y_1\\vert X_i)+\\mathbb{E}(y_2\\vert X_i)=0.$$\n\nIn machine learning, this is called encoding an *inductive bias* into\nthe model, and considerations like this are often crucial for good\ngeneralisation performance.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nSince the above holds for all $X_i$, it implies an indistinguishability\nof the model: if we look at one of the labels at random, we are equally\nlikely to see a positive or negative payoff regardless of $X_i$, and so\nthe $X_i$ are indistinguishable with respect to this observation. This\nimplies a corresponding constraint on non-contextual learning models,\nwhich limits their expressivity and may therefore hinder their\nperformance: see the paper for more details on how this looks in\npractice. Luckily for us quantum theory is a contextual theory, so these\nlimitations don't apply to our model!\n:::\n\nThe quantum model we consider has the following structure:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../demonstrations/contextuality/model.png){.align-center\nwidth=\"50.0%\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters $\\theta$ and $\\alpha$ are trainable parameters of the\nmodel, and we will use the three $Z$ measurements at the end of the\ncircuit to sample the three labels. Therefore, if we write the entire\ncircuit as $\\vert \\psi(\\alpha,\\theta,X)\\rangle$ the zero sum condition\nwill be satisfied if\n\n$$\\langle \\psi(\\alpha,\\theta,X) \\vert (Z_0+Z_1+Z_2) \\vert \\psi(\\alpha,\\theta,X) \\rangle = 0.$$\n\nLet's see how we can create a model class that satisfies this. For\nprecise details on the structure of the model, check out Figure 6 in the\npaper. We'll first look at the parameterised unitary $V_{\\alpha}$, that\nwe call the *input preparation unitary*. This prepares a state\n$V_\\alpha\\vert 0 \\rangle$ such that\n\n$$\\langle 0 \\vert V^\\dagger_\\alpha (Z_0+Z_1+Z_2) V_\\alpha\\vert 0 \\rangle = 0.$$\n\nAn example of such a circuit is the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def input_prep(alpha):\n    # This ensures the prepared state has <Z_0+Z_1+Z_2>=0\n    qml.Hadamard(wires=0)\n    qml.Hadamard(wires=1)\n    qml.Hadamard(wires=2)\n    qml.RY(alpha[0], wires=0)\n    qml.RY(alpha[0] + np.pi, wires=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second unitary is a *bias invariant layer*: it preserves the value\nof $\\langle Z_0+Z_1+Z_2 \\rangle$ for all input states into the layer. To\nachieve this, the generators of the unitaries in this layer must commute\nwith the operator $Z_0+Z_1+Z_2$. For example the operator\n$X\\otimes X + Y\\otimes Y + Z\\otimes Z$ (on any pair of qubits) commutes\nwith $Z_0+Z_1+Z_2$ and so a valid parameterised gate could be\n\n$$e^{i\\theta(X\\otimes X\\otimes\\mathbb{I} + Y\\otimes Y\\otimes\\mathbb{I} + Z\\otimes Z\\otimes\\mathbb{I})}.$$\n\nThis kind of reasoning is an example of geometric quantum machine\nlearning (check out and or our own\n[demo](https://pennylane.ai/qml/demos/tutorial_geometric_qml.html) for\nan awesome introduction to the subject). Below we construct the bias\ninvariant layer: note that all the generators commute with\n$Z_0+Z_1+Z_2$. The variables `blocks` and `layers` are model\nhyperparameters that we will fix as `blocks=1` and `layers=2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "blocks = 1\nlayers = 2\n\n\ndef swap_rot(weights, wires):\n    \"\"\"\n    bias-invariant unitary with swap matrix as generator\n    \"\"\"\n    qml.PauliRot(weights, \"XX\", wires=wires)\n    qml.PauliRot(weights, \"YY\", wires=wires)\n    qml.PauliRot(weights, \"ZZ\", wires=wires)\n\n\ndef param_unitary(weights):\n    \"\"\"\n    A bias-invariant unitary (U in the paper)\n    \"\"\"\n    for b in range(blocks):\n        for q in range(3):\n            qml.RZ(weights[b, q], wires=q)\n        qml.PauliRot(weights[b, 3], \"ZZ\", wires=[0, 1])\n        qml.PauliRot(weights[b, 4], \"ZZ\", wires=[0, 2])\n        qml.PauliRot(weights[b, 5], \"ZZ\", wires=[1, 2])\n        swap_rot(weights[b, 6], wires=[0, 1])\n        swap_rot(weights[b, 7], wires=[1, 2])\n        swap_rot(weights[b, 8], wires=[0, 2])\n\n\ndef data_encoding(x):\n    \"\"\"\n    S_x^1 in paper\n    \"\"\"\n    for q in range(3):\n        qml.RZ(x[q], wires=q)\n\n\ndef data_encoding_pairs(x):\n    \"\"\"\n    S_x^2 in paper\n    \"\"\"\n    qml.PauliRot(x[0] * x[1], \"ZZ\", wires=[0, 1])\n    qml.PauliRot(x[1] * x[2], \"ZZ\", wires=[1, 2])\n    qml.PauliRot(x[0] * x[2], \"ZZ\", wires=[0, 2])\n\n\ndef bias_inv_layer(weights, x):\n    \"\"\"\n    The full bias invariant layer.\n    \"\"\"\n    # data preprocessing\n    x1 = jnp.array([x[0, 0], x[1, 1], x[2, 2]])\n    x2 = jnp.array(([x[0, 1] - x[0, 2], x[1, 2] - x[1, 0], x[2, 0] - x[2, 1]]))\n    for l in range(0, 2 * layers, 2):\n        param_unitary(weights[l])\n        data_encoding(x1)\n        param_unitary(weights[l + 1])\n        data_encoding_pairs(x2)\n    param_unitary(weights[2 * layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our `input_prep` and `bias_inv_layer` functions we can now define\nour quantum model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=3)\n\n\n@qml.qnode(dev, interface=\"jax\")\ndef model(weights, x):\n    input_prep(weights[2 * layers + 1, 0])  # alpha is stored in the weights array\n    bias_inv_layer(weights, x)\n    return [qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2))]\n\n\n# jax vectorisation, we vectorise over the data input (the second argument)\nvmodel = jax.vmap(model, (None, 0))\nvmodel = jax.jit(vmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To investigate the effect of the encoded inductive bias, we will compare\nthis model to a generic model with the same data encoding and similar\nnumber of parameters (46 vs 45 parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generic_layer(weights, x):\n    # data preprocessing\n    x1 = jnp.array([x[0, 0], x[1, 1], x[2, 2]])\n    x2 = jnp.array(([x[0, 1] - x[0, 2], x[1, 2] - x[1, 0], x[2, 0] - x[2, 1]]))\n    for l in range(0, 2 * layers, 2):\n        qml.StronglyEntanglingLayers(weights[l], wires=range(3))\n        data_encoding(x1)\n        qml.StronglyEntanglingLayers(weights[l + 1], wires=range(3))\n        data_encoding_pairs(x2)\n    qml.StronglyEntanglingLayers(weights[2 * layers], wires=range(3))\n\n\ndev = qml.device(\"default.qubit\", wires=3)\n\n\n@qml.qnode(dev, interface=\"jax\")\ndef generic_model(weights, x):\n    generic_layer(weights, x)\n    return [qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1)), qml.expval(qml.PauliZ(2))]\n\n\nvmodel_generic = jax.vmap(generic_model, (None, 0))\nvmodel_generic = jax.jit(vmodel_generic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Warning**: Since we are using JAX it is important that our `model` and\n`generic model` functions are functionally pure (read more\n[here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)).\nThis means we cannot change the values of `blocks` or `layers` from\nhereon since these values have been cached for JIT compilation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and evaluation\n=======================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train the model we will minimise the negative log likelihood of the\nlabels given the data\n\n$$\\mathcal{L} = -\\frac{1}{3\\vert N \\vert}\\sum_{(X_i,\\vec{y}_i)} \\log(\\mathcal{P}_0(y_i^{(0)}\\vert X_i))+\\log(\\mathcal{P}_1(y_i^{(1)}\\vert X_i))+\\log(\\mathcal{P}_2(y_i^{(2)}\\vert X_i))$$\n\nHere $\\mathcal{P}_k$ is the probability distribution of the $k$ label\nfrom the model, $y_i^{(k)}$ is the kth element of the payoff vector\n$\\vec{y}_i$ in the dataset, and $N$ is the size of the training dataset.\nWe remark that training the negative log likelihood is in some sense\ncheating, since for large quantum circuits we don't know how to estimate\nit efficiently. As generative modeling in QML progresses, we can hope\nhowever that scalable methods that approximate this type of training may\nappear.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def likelihood(weights, X, Y, model):\n    \"\"\"\n    The cost function. Returns the negative log likelihood\n    \"\"\"\n    expvals = jnp.array(model(weights, X)).T\n    probs = (1 + Y * expvals) / 2  # get the relevant probabilites\n    probs = jnp.log(probs)\n    llh = jnp.sum(probs) / len(X) / 3\n    return -llh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For evaluation we will use the average KL divergence between the true\ndata distribution and the model distribution\n\n$$\\mathbb{E}_{P^\\text{data}(X)} \\left[\\frac{1}{3}\\sum_{k=1}^{3} D_{\\text{KL}}(P^\\text{data}_k(y\\vert X)\\vert\\vert \\mathcal{P}_k(y\\vert X)) \\right].$$\n\nTo estimate this we sample a test set of strategies, calculate their\npayoff probabilities, and estimate the above expectation via the sample\nmean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N_test = 10000\nX_test = get_strategy_matrices(N_test)\n\nprobs_test = np.zeros([N_test, 3, 2])\nprobs_test[:, :, 0] = vpayoff_probs(X_test)  # the true probabilities for the test set\nprobs_test[:, :, 1] = 1 - probs_test[:, :, 0]\nprobs_test = jnp.array(probs_test)\n\n\ndef kl_div(p, q):\n    \"\"\"\n    Get the KL divergence between two probability distribtuions\n    \"\"\"\n    p = jnp.vstack([p, jnp.ones(len(p)) * 10 ** (-8)])  # lower cutoff of prob values of 10e-8\n    p = jnp.max(p, axis=0)\n    return jnp.sum(q * jnp.log(q / p))  # forward kl div\n\n\ndef kl_marginals(probs, probs_test):\n    \"\"\"\n    get the mean KL divergence of the three marginal distributions\n    (the square brackets above)\n    \"\"\"\n    kl = 0\n    for t in range(3):\n        kl = kl + kl_div(probs[t, :], probs_test[t, :])\n    return kl / 3\n\n\n# vectorise the kl_marginals function. Makes estimating the average KL diverence of a model faster.\nvkl_marginals = jax.vmap(kl_marginals, (0, 0))\n\n\ndef get_av_test_kl(model, weights, probs_test, X_test):\n    \"\"\"\n    returns the average KL divergence for a test set X_test.\n    \"\"\"\n    N_test = len(X_test)\n    probs = np.zeros(probs_test.shape)\n    expvals = jnp.array(model(weights, X_test)).T\n    for t in range(3):\n        probs[:, t, 0] = (1 + expvals[:, t]) / 2\n        probs[:, t, 1] = (1 - expvals[:, t]) / 2\n    return np.sum(vkl_marginals(probs, probs_test)) / N_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To optimise the model we make use of the JAX optimization library optax.\nWe will use the adam gradient descent optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import optax\nfrom tqdm import tqdm\n\n\ndef optimise_model(model, nstep, lr, weights):\n    plot = [[], [], []]\n    optimizer = optax.adam(lr)\n    opt_state = optimizer.init(weights)\n    steps = tqdm(range(nstep))\n    for step in steps:\n        #         use optax to update parameters\n        llh, grads = jax.value_and_grad(likelihood)(weights, X, Y, model)\n        updates, opt_state = optimizer.update(grads, opt_state, weights)\n        weights = optax.apply_updates(weights, updates)\n\n        kl = get_av_test_kl(model, weights, probs_test, X_test)\n        steps.set_description(\n            \"Current divergence: %s\" % str(kl) + \" :::: \" + \"Current likelihood: %s\" % str(llh)\n        )\n        plot[0].append(step)\n        plot[1].append(float(llh))\n        plot[2].append(float(kl))\n    return weights, llh, kl, plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to generate a data set and optimize our models!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# generate data\nN = 2000  # number of data points\nX, Y, P = generate_data(N)\n\nnstep = 2000  # number of optimisation steps\n\nlr = 0.001  # initial learning rate\nweights_model = np.random.rand(2 * layers + 2, blocks, 9) * 2 * np.pi\nweights_generic = np.random.rand(2 * layers + 1, blocks, 3, 3) * 2 * np.pi\n\n# optimise the structured model\nweights_model, llh, kl, plot_model = optimise_model(vmodel, nstep, lr, weights_model)\n# optimise the generic model\nweights_generic, llh, kl, plot_genereic = optimise_model(vmodel_generic, nstep, lr, weights_generic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the average KL divergence and the negative log likelihood for\nboth models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.style.use('pennylane.drawer.plot')\n\n# subplots\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\nfig.tight_layout(pad=10.0)\n\n# KL divergence\nax1.plot(plot_model[0], plot_model[2], label=\"biased model\")\nax1.plot(plot_genereic[0], plot_genereic[2], label=\"generic model\")\n\nax1.set_yscale(\"log\")\nax1.set_ylabel(\"KL divergence (test)\")\nax1.set_xlabel(\"training step\")\nax1.legend()\n\n# negative log likelihood\nax2.plot(plot_model[0], plot_model[1])\nax2.plot(plot_genereic[0], plot_genereic[1])\n\nax2.set_yscale(\"log\")\nax2.set_ylabel(\"Negative log likelihood (train)\")\nax2.set_xlabel(\"training step\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the model that encodes the inductive bias achieves both a\nlower training error and generalisation error, as can be expected.\nIncorporating knowledge about the data into the model design is\ngenerally a very good idea!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n==========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this demo we have constructed a dataset whose structure is connected\nto generalized contextuality, and have shown how to encode this\nstructure as an inductive bias of a quantum model class. As is often the\ncase, we saw that this approach outperforms a generic model class that\ndoes not take this knowledge into account. As a general rule,\nconsiderations like this should be at the front of one\\'s mind when\nbuilding a quantum model for a specific task.\n\nThat is all for this demo. In our paper[^1], it is also shown how models\nof this kind can perform better than classical surrogate models[^2] at\nthis specific task, which further strengthens the claim that the\ninductive bias of the quantum model is useful. For more information and\nto read more about the link between contextuality and QML, check out the\nfull paper.\n\nReferences\n==========\n\nAbout the author\n================\n\n[^1]: J. Bowles, V. J. Wright, M. Farkas, N. Killoran, M. Schuld\n    \\\"Contextuality and inductive bias in quantum machine learning.\\\"\n    [arXiv:2302.01365](https://arxiv.org/abs/2302.01365), 2023.\n\n[^2]: F. J. Schreiber, J. Eiser, J. J. Meyer \\\"Classical surrogates for\n    quantum learning models.\\\"\n    [arXiv:2206.11740](https://arxiv.org/abs/2206.11740), 2022.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}