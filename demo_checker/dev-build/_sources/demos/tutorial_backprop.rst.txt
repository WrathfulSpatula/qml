
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "demos/tutorial_backprop.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_demos_tutorial_backprop.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_demos_tutorial_backprop.py:


Quantum gradients with backpropagation
======================================

.. meta::
    :property="og:description": Using backpropagation can speed up training of quantum circuits compared to the parameter-shift rule—if you are using a simulator.

    :property="og:image": https://pennylane.ai/qml/_images/sphx_glr_tutorial_backprop_002.png

.. related::

   tutorial_quantum_natural_gradient Quantum natural gradient

*Author: Josh Izaac — Posted: 11 August 2020. Last updated: 31 January 2021.*

In PennyLane, any quantum device, whether a hardware device or a simulator, can be
trained using the :doc:`parameter-shift rule </glossary/parameter_shift>` to compute quantum
gradients. Indeed, the parameter-shift rule is ideally suited to hardware devices, as it does
not require any knowledge about the internal workings of the device; it is sufficient to treat
the device as a 'black box', and to query it with different input values in order to determine the gradient.

When working with simulators, however, we *do* have access to the internal (classical)
computations being performed. This allows us to take advantage of other methods of computing the
gradient, such as backpropagation, which may be advantageous in certain regimes. In this tutorial,
we will compare and contrast the parameter-shift rule against backpropagation, using
the PennyLane :class:`default.qubit <pennylane.devices.default_qubit>`
device.

The parameter-shift rule
------------------------

The parameter-shift rule states that, given a variational quantum circuit :math:`U(\boldsymbol
\theta)` composed of parametrized Pauli rotations, and some measured observable :math:`\hat{B}`, the
derivative of the expectation value

.. math::

    \langle \hat{B} \rangle (\boldsymbol\theta) =
    \langle 0 \vert U(\boldsymbol\theta)^\dagger \hat{B} U(\boldsymbol\theta) \vert 0\rangle

with respect to the input circuit parameters :math:`\boldsymbol{\theta}` is given by

.. math::

   \nabla_{\theta_i}\langle \hat{B} \rangle(\boldsymbol\theta)
      =  \frac{1}{2}
            \left[
                \langle \hat{B} \rangle\left(\boldsymbol\theta + \frac{\pi}{2}\hat{\mathbf{e}}_i\right)
              - \langle \hat{B} \rangle\left(\boldsymbol\theta - \frac{\pi}{2}\hat{\mathbf{e}}_i\right)
            \right].

Thus, the gradient of the expectation value can be calculated by evaluating the same variational
quantum circuit, but with shifted parameter values (hence the name, parameter-shift rule!).

Let's have a go implementing the parameter-shift rule manually in PennyLane.

.. GENERATED FROM PYTHON SOURCE LINES 57-83

.. code-block:: default





    # set the random seed


    # create a device to execute the circuit on

























.. GENERATED FROM PYTHON SOURCE LINES 84-85

Let's test the variational circuit evaluation with some parameter input:

.. GENERATED FROM PYTHON SOURCE LINES 85-92

.. code-block:: default


    # initial parameters












.. GENERATED FROM PYTHON SOURCE LINES 93-94

We can also draw the executed quantum circuit:

.. GENERATED FROM PYTHON SOURCE LINES 100-103

Now that we have defined our variational circuit QNode, we can construct
a function that computes the gradient of the :math:`i\text{th}` parameter
using the parameter-shift rule.

.. GENERATED FROM PYTHON SOURCE LINES 103-117

.. code-block:: default












    # gradient with respect to the first parameter









.. GENERATED FROM PYTHON SOURCE LINES 118-120

In order to compute the gradient with respect to *all* parameters, we need
to loop over the index ``i``:

.. GENERATED FROM PYTHON SOURCE LINES 132-137

We can compare this to PennyLane's *built-in* quantum gradient support by using
the :func:`qml.grad <pennylane.grad>` function, which allows us to compute gradients
of hybrid quantum-classical cost functions. Remember, when we defined the
QNode, we specified that we wanted it to be differentiable using the parameter-shift
method (``diff_method="parameter-shift"``).

.. GENERATED FROM PYTHON SOURCE LINES 142-144

Alternatively, we can directly compute quantum gradients of QNodes using
PennyLane's built in :mod:`qml.gradients <pennylane.gradients>` module:

.. GENERATED FROM PYTHON SOURCE LINES 149-167

If you count the number of quantum evaluations, you will notice that we had to evaluate the circuit
``2*len(params)`` number of times in order to compute the quantum gradient with respect to all
parameters. While reasonably fast for a small number of parameters, as the number of parameters in
our quantum circuit grows, so does both

1. the circuit depth (and thus the time taken to evaluate each expectation value or 'forward' pass), and

2. the number of parameter-shift evaluations required.

Both of these factors increase the time taken to compute the gradient with
respect to all parameters.

Benchmarking
~~~~~~~~~~~~

Let's consider an example with a significantly larger number of parameters.
We'll make use of the :class:`~pennylane.StronglyEntanglingLayers` template
to make a more complicated QNode.

.. GENERATED FROM PYTHON SOURCE LINES 167-181

.. code-block:: default









    # initialize circuit parameters












.. GENERATED FROM PYTHON SOURCE LINES 182-184

This circuit has 180 parameters. Let's see how long it takes to perform a forward
pass of the circuit.

.. GENERATED FROM PYTHON SOURCE LINES 196-198

We can now estimate the time taken to compute the full gradient vector,
and see how this compares.

.. GENERATED FROM PYTHON SOURCE LINES 198-208

.. code-block:: default


    # create the gradient function















.. GENERATED FROM PYTHON SOURCE LINES 209-212

Based on the parameter-shift rule, we expect that the amount of time to compute the quantum
gradients should be approximately :math:`2p\Delta t_{f}` where :math:`p` is the number of
parameters and :math:`\Delta t_{f}` if the time taken for the forward pass. Let's verify this:

.. GENERATED FROM PYTHON SOURCE LINES 217-258

Backpropagation
---------------

An alternative to the parameter-shift rule for computing gradients is
`reverse-mode autodifferentiation <https://en.wikipedia.org/wiki/Reverse_accumulation>`__.
Unlike the parameter-shift method, which requires :math:`2p` circuit evaluations for
:math:`p` parameters, reverse-mode requires only a *single* forward pass of the
differentiable function to compute
the gradient of all variables, at the expense of increased memory usage.
During the forward pass, the results of all intermediate subexpressions are stored;
the computation is then traversed *in reverse*, with the gradient computed by repeatedly
applying the chain rule.
In most classical machine learning settings (where we are training scalar loss functions
consisting of a large number of parameters),
reverse-mode autodifferentiation is the
preferred method of autodifferentiation—the reduction in computational time enables larger and
more complex models to be successfully trained. The backpropagation algorithm is a particular
special-case of reverse-mode autodifferentiation, which has helped lead to the machine learning
explosion we see today.

In quantum machine learning, however, the inability to store and utilize the results of
*intermediate* quantum operations on hardware remains a barrier to using backprop;
while reverse-mode
autodifferentiation works fine for small quantum simulations, only the
parameter-shift rule can be used to compute gradients on quantum hardware directly. Nevertheless,
when training quantum models via classical simulation, it's useful to explore the regimes where
reverse-mode differentiation may be a better choice than the parameter-shift rule.

Benchmarking
~~~~~~~~~~~~

When creating a QNode, :doc:`PennyLane supports various methods of differentiation
<code/api/pennylane.qnode>`, including ``"parameter-shift"`` (which we used previously),
``"finite-diff"``, ``"reversible"``, and ``"backprop"``. While ``"parameter-shift"`` works with all devices
(simulator or hardware), ``"backprop"`` will only work for specific simulator devices that are
designed to support backpropagation.

One such device is :class:`default.qubit <pennylane.devices.DefaultQubit>`. It
has backends written using TensorFlow, JAX, and Autograd, so when used with the
TensorFlow, JAX, and Autograd interfaces respectively, supports backpropagation.
In this demo, we will use the default Autograd interface.

.. GENERATED FROM PYTHON SOURCE LINES 262-265

When defining the QNode, we specify ``diff_method="backprop"`` to ensure that
we are using backpropagation mode. Note that this is the *default differentiation
mode* for the ``default.qubit`` device.

.. GENERATED FROM PYTHON SOURCE LINES 265-277

.. code-block:: default








    # initialize circuit parameters











.. GENERATED FROM PYTHON SOURCE LINES 278-279

Let's see how long it takes to perform a forward pass of the circuit.

.. GENERATED FROM PYTHON SOURCE LINES 290-293

Comparing this to the forward pass from ``default.qubit``, we note that there is some potential
overhead from using backpropagation. We can now estimate the time required to perform a
gradient computation via backpropagation:

.. GENERATED FROM PYTHON SOURCE LINES 299-309

Unlike with the parameter-shift rule, the time taken to perform the backwards pass appears
of the order of a single forward pass! This can significantly speed up training of simulated
circuits with many parameters.

Time comparison
---------------

Let's compare the two differentiation approaches as the number of trainable parameters
in the variational circuit increases, by timing both the forward pass and the gradient
computation as the number of layers is allowed to increase.

.. GENERATED FROM PYTHON SOURCE LINES 317-322

We'll continue to use the same ansatz as before, but to reduce the time taken
to collect the data, we'll reduce the number and repetitions of timings per data
point. Below, we loop over a variational circuit depth ranging from 0 (no gates/
trainable parameters) to 20. Each layer will contain :math:`3N` parameters, where
:math:`N` is the number of wires (in this case, we have :math:`N=4`).

.. GENERATED FROM PYTHON SOURCE LINES 373-374

We now import matplotlib, and plot the results.

.. GENERATED FROM PYTHON SOURCE LINES 388-402

.. raw:: html

    <br>

We can see that the computational time for the parameter-shift rule increases with
increasing number of parameters, as expected, whereas the computational time
for backpropagation appears much more constant, with perhaps a minute linear increase
with :math:`p`. Note that the plots are not perfectly linear, with some 'bumpiness' or
noisiness. This is likely due to low-level operating system jitter, and
other environmental fluctuations—increasing the number of repeats can help smooth
out the plot.

For a better comparison, we can scale the time required for computing the quantum
gradients against the time taken for the corresponding forward pass:

.. GENERATED FROM PYTHON SOURCE LINES 402-428

.. code-block:: default










    # perform a least squares regression to determine the linear best fit/gradient
    # for the normalized time vs. number of parameters






















.. GENERATED FROM PYTHON SOURCE LINES 429-440

.. raw:: html

    <br>

We can now see clearly that there is constant overhead for backpropagation with
``default.qubit``, but the parameter-shift rule scales as :math:`\sim 2p`.


About the author
----------------
.. include:: ../_static/authors/josh_izaac.txt


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.001 seconds)


.. _sphx_glr_download_demos_tutorial_backprop.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_backprop.py <tutorial_backprop.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_backprop.ipynb <tutorial_backprop.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
