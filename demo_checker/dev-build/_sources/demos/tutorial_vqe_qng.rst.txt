
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "demos/tutorial_vqe_qng.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_demos_tutorial_vqe_qng.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_demos_tutorial_vqe_qng.py:


Accelerating VQEs with quantum natural gradient
===============================================

.. meta::
    :property="og:description": Accelerating variational quantum eigensolvers
        using quantum natural gradients in PennyLane.
    :property="og:image": https://pennylane.ai/qml/_images/qng_example.png

.. related::

   tutorial_vqe A brief overview of VQE
   tutorial_quantum_natural_gradient Quantum natural gradient

*Authors: Maggie Li, Lana Bozanic, Sukin Sim â€” Posted: 06 November 2020. Last updated: 29 August 2023.*

This tutorial showcases how one can apply quantum natural gradients (QNG) [#stokes2019]_ [#yamamoto2019]_
to accelerate the optimization step of the Variational Quantum Eigensolver (VQE) algorithm [#peruzzo2014]_.
We will implement two small examples: estimating the ground state energy of a single-qubit VQE
problem, which we can visualize using the Bloch sphere, and the hydrogen molecule.

Before going through this tutorial, we recommend that readers refer to the
:doc:`QNG tutorial </demos/tutorial_quantum_natural_gradient>` and
:doc:`VQE tutorial </demos/tutorial_vqe>` for overviews
of quantum natural gradient and the variational quantum eigensolver algorithm, respectively.
Let's get started!


Single-qubit VQE example
------------------------

The first step is to import the required libraries and packages:

.. GENERATED FROM PYTHON SOURCE LINES 34-39

.. code-block:: default


    import matplotlib.pyplot as plt
    from pennylane import numpy as np
    import pennylane as qml








.. GENERATED FROM PYTHON SOURCE LINES 40-43

For this simple example, we consider the following single-qubit Hamiltonian: :math:`\sigma_x + \sigma_z`.

We define the device:

.. GENERATED FROM PYTHON SOURCE LINES 43-47

.. code-block:: default


    dev = qml.device("default.qubit", wires=1)









.. GENERATED FROM PYTHON SOURCE LINES 48-50

For the variational ansatz, we use two single-qubit rotations, which the user may recognize
from a previous :doc:`tutorial </demos/tutorial_qubit_rotation>` on qubit rotations.

.. GENERATED FROM PYTHON SOURCE LINES 50-57

.. code-block:: default



    def circuit(params, wires=0):
        qml.RX(params[0], wires=wires)
        qml.RY(params[1], wires=wires)









.. GENERATED FROM PYTHON SOURCE LINES 58-61

We then define our cost function which supports the computation of
block-diagonal or diagonal approximations to the Fubini-Study metric tensor [#stokes2019]_. This tensor is a
crucial component for optimizing with quantum natural gradients.

.. GENERATED FROM PYTHON SOURCE LINES 61-72

.. code-block:: default


    coeffs = [1, 1]
    obs = [qml.PauliX(0), qml.PauliZ(0)]

    H = qml.Hamiltonian(coeffs, obs)

    @qml.qnode(dev, interface="autograd")
    def cost_fn(params):
        circuit(params)
        return qml.expval(H)








.. GENERATED FROM PYTHON SOURCE LINES 73-79

To analyze the performance of quantum natural gradient on VQE calculations,
we set up and execute optimizations using the ``GradientDescentOptimizer`` (which does not
utilize quantum gradients) and the ``QNGOptimizer`` that uses the block-diagonal approximation
to the metric tensor.

To perform a fair comparison, we fix the initial parameters for the two optimizers.

.. GENERATED FROM PYTHON SOURCE LINES 79-83

.. code-block:: default


    init_params = np.array([3.97507603, 3.00854038], requires_grad=True)









.. GENERATED FROM PYTHON SOURCE LINES 84-87

We will carry out each optimization over a maximum of 500 steps. As was done in the VQE
tutorial, we aim to reach a convergence tolerance of around :math:`10^{-6}`.
We use a step size of 0.01.

.. GENERATED FROM PYTHON SOURCE LINES 87-92

.. code-block:: default


    max_iterations = 500
    conv_tol = 1e-06
    step_size = 0.01








.. GENERATED FROM PYTHON SOURCE LINES 93-94

First, we carry out the VQE optimization using the standard gradient descent method.

.. GENERATED FROM PYTHON SOURCE LINES 94-127

.. code-block:: default


    opt = qml.GradientDescentOptimizer(stepsize=step_size)

    params = init_params

    gd_param_history = [params]
    gd_cost_history = []

    for n in range(max_iterations):

        # Take step
        params, prev_energy = opt.step_and_cost(cost_fn, params)
        gd_param_history.append(params)
        gd_cost_history.append(prev_energy)

        energy = cost_fn(params)

        # Calculate difference between new and old energies
        conv = np.abs(energy - prev_energy)

        if n % 20 == 0:
            print(
                "Iteration = {:},  Energy = {:.8f} Ha,  Convergence parameter = {"
                ":.8f} Ha".format(n, energy, conv)
            )

        if conv <= conv_tol:
            break

    print()
    print("Final value of the energy = {:.8f} Ha".format(energy))
    print("Number of iterations = ", n)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Iteration = 0,  Energy = 0.56743624 Ha,  Convergence parameter = 0.00973536 Ha
    Iteration = 20,  Energy = 0.38709233 Ha,  Convergence parameter = 0.00821261 Ha
    Iteration = 40,  Energy = 0.24420954 Ha,  Convergence parameter = 0.00616395 Ha
    Iteration = 60,  Energy = 0.14079686 Ha,  Convergence parameter = 0.00435028 Ha
    Iteration = 80,  Energy = 0.06758408 Ha,  Convergence parameter = 0.00314443 Ha
    Iteration = 100,  Energy = 0.01128048 Ha,  Convergence parameter = 0.00262544 Ha
    Iteration = 120,  Energy = -0.04175219 Ha,  Convergence parameter = 0.00278160 Ha
    Iteration = 140,  Energy = -0.10499504 Ha,  Convergence parameter = 0.00361450 Ha
    Iteration = 160,  Energy = -0.19195848 Ha,  Convergence parameter = 0.00511056 Ha
    Iteration = 180,  Energy = -0.31444953 Ha,  Convergence parameter = 0.00708743 Ha
    Iteration = 200,  Energy = -0.47706980 Ha,  Convergence parameter = 0.00900220 Ha
    Iteration = 220,  Energy = -0.66993027 Ha,  Convergence parameter = 0.01001574 Ha
    Iteration = 240,  Energy = -0.86789981 Ha,  Convergence parameter = 0.00955105 Ha
    Iteration = 260,  Energy = -1.04241317 Ha,  Convergence parameter = 0.00783834 Ha
    Iteration = 280,  Energy = -1.17653061 Ha,  Convergence parameter = 0.00567546 Ha
    Iteration = 300,  Energy = -1.26906291 Ha,  Convergence parameter = 0.00374812 Ha
    Iteration = 320,  Energy = -1.32823089 Ha,  Convergence parameter = 0.00232769 Ha
    Iteration = 340,  Energy = -1.36424033 Ha,  Convergence parameter = 0.00139097 Ha
    Iteration = 360,  Energy = -1.38549883 Ha,  Convergence parameter = 0.00081221 Ha
    Iteration = 380,  Energy = -1.39782393 Ha,  Convergence parameter = 0.00046788 Ha
    Iteration = 400,  Energy = -1.40489478 Ha,  Convergence parameter = 0.00026743 Ha
    Iteration = 420,  Energy = -1.40892679 Ha,  Convergence parameter = 0.00015217 Ha
    Iteration = 440,  Energy = -1.41121801 Ha,  Convergence parameter = 0.00008637 Ha
    Iteration = 460,  Energy = -1.41251745 Ha,  Convergence parameter = 0.00004895 Ha
    Iteration = 480,  Energy = -1.41325360 Ha,  Convergence parameter = 0.00002772 Ha

    Final value of the energy = -1.41365468 Ha
    Number of iterations =  499




.. GENERATED FROM PYTHON SOURCE LINES 128-129

We then repeat the process for the optimizer employing quantum natural gradients:

.. GENERATED FROM PYTHON SOURCE LINES 129-163

.. code-block:: default


    opt = qml.QNGOptimizer(stepsize=step_size, approx="block-diag")

    params = init_params

    qngd_param_history = [params]
    qngd_cost_history = []

    for n in range(max_iterations):

        # Take step
        params, prev_energy = opt.step_and_cost(cost_fn, params)
        qngd_param_history.append(params)
        qngd_cost_history.append(prev_energy)

        # Compute energy
        energy = cost_fn(params)

        # Calculate difference between new and old energies
        conv = np.abs(energy - prev_energy)

        if n % 20 == 0:
            print(
                "Iteration = {:},  Energy = {:.8f} Ha,  Convergence parameter = {"
                ":.8f} Ha".format(n, energy, conv)
            )

        if conv <= conv_tol:
            break

    print()
    print("Final value of the energy = {:.8f} Ha".format(energy))
    print("Number of iterations = ", n)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Iteration = 0,  Energy = 0.51052556 Ha,  Convergence parameter = 0.06664604 Ha
    Iteration = 20,  Energy = -0.90729965 Ha,  Convergence parameter = 0.05006082 Ha
    Iteration = 40,  Energy = -1.35504644 Ha,  Convergence parameter = 0.00713113 Ha
    Iteration = 60,  Energy = -1.40833787 Ha,  Convergence parameter = 0.00072399 Ha
    Iteration = 80,  Energy = -1.41364035 Ha,  Convergence parameter = 0.00007078 Ha
    Iteration = 100,  Energy = -1.41415774 Ha,  Convergence parameter = 0.00000689 Ha

    Final value of the energy = -1.41420585 Ha
    Number of iterations =  117




.. GENERATED FROM PYTHON SOURCE LINES 164-170

Visualizing the results
^^^^^^^^^^^^^^^^^^^^^^^

For single-qubit examples, we can visualize the optimization process in several ways.

For example, we can track the energy history:

.. GENERATED FROM PYTHON SOURCE LINES 170-180

.. code-block:: default


    plt.style.use("seaborn")
    plt.plot(gd_cost_history, "b", label="Gradient descent")
    plt.plot(qngd_cost_history, "g", label="Quantum natural gradient descent")

    plt.ylabel("Cost function value")
    plt.xlabel("Optimization steps")
    plt.legend()
    plt.show()




.. image-sg:: /demos/images/sphx_glr_tutorial_vqe_qng_001.png
   :alt: tutorial vqe qng
   :srcset: /demos/images/sphx_glr_tutorial_vqe_qng_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/qml/qml/demonstrations/tutorial_vqe_qng.py:171: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn")




.. GENERATED FROM PYTHON SOURCE LINES 181-184

Or we can visualize the optimization path in the parameter space using a contour plot.
Energies at different grid points have been pre-computed, and they can be downloaded by
clicking :download:`here<../demonstrations/vqe_qng/param_landscape.npy>`.

.. GENERATED FROM PYTHON SOURCE LINES 184-238

.. code-block:: default


    # Discretize the parameter space
    theta0 = np.linspace(0.0, 2.0 * np.pi, 100)
    theta1 = np.linspace(0.0, 2.0 * np.pi, 100)

    # Load energy value at each point in parameter space
    parameter_landscape = np.load("vqe_qng/param_landscape.npy")

    # Plot energy landscape
    fig, axes = plt.subplots(figsize=(6, 6))
    cmap = plt.cm.get_cmap("coolwarm")
    contour_plot = plt.contourf(theta0, theta1, parameter_landscape, cmap=cmap)
    plt.xlabel(r"$\theta_0$")
    plt.ylabel(r"$\theta_1$")

    # Plot optimization path for gradient descent. Plot every 10th point.
    gd_color = "g"
    plt.plot(
        np.array(gd_param_history)[::10, 0],
        np.array(gd_param_history)[::10, 1],
        ".",
        color=gd_color,
        linewidth=1,
        label="Gradient descent",
    )
    plt.plot(
        np.array(gd_param_history)[:, 0],
        np.array(gd_param_history)[:, 1],
        "-",
        color=gd_color,
        linewidth=1,
    )

    # Plot optimization path for quantum natural gradient descent. Plot every 10th point.
    qngd_color = "k"
    plt.plot(
        np.array(qngd_param_history)[::10, 0],
        np.array(qngd_param_history)[::10, 1],
        ".",
        color=qngd_color,
        linewidth=1,
        label="Quantum natural gradient descent",
    )
    plt.plot(
        np.array(qngd_param_history)[:, 0],
        np.array(qngd_param_history)[:, 1],
        "-",
        color=qngd_color,
        linewidth=1,
    )

    plt.legend()
    plt.show()




.. image-sg:: /demos/images/sphx_glr_tutorial_vqe_qng_002.png
   :alt: tutorial vqe qng
   :srcset: /demos/images/sphx_glr_tutorial_vqe_qng_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/qml/qml/demonstrations/tutorial_vqe_qng.py:194: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.
      cmap = plt.cm.get_cmap("coolwarm")




.. GENERATED FROM PYTHON SOURCE LINES 239-262

Here, the blue regions indicate states with lower energies, and the red regions indicate
states with higher energies. We can see that the ``QNGOptimizer`` takes a more direct
route to the minimum in larger strides compared to the path taken by the ``GradientDescentOptimizer``.

Lastly, we can visualize the same optimization paths on the Bloch sphere using routines
from `QuTiP <http://qutip.org/>`__. The result should look like the following:

.. figure:: /demonstrations/vqe_qng/opt_paths_bloch.png
    :width: 50%
    :align: center

where again the black markers and line indicate the path taken by the ``QNGOptimizer``,
and the green markers and line indicate the path taken by the ``GradientDescentOptimizer``.
Using this visualization method, we can clearly see how the path using the ``QNGOptimizer`` tightly
"hugs" the curvature of the Bloch sphere and takes the shorter path.

Now, we will move onto a more interesting example: estimating the ground state energy
of molecular hydrogen.

Hydrogen VQE Example
--------------------

To construct our system Hamiltonian, we can use `PennyLane Datasets <https://pennylane.ai/datasets>`__ to obtain the dataset for a :math:`\text{H}_2` molecule.

.. GENERATED FROM PYTHON SOURCE LINES 262-269

.. code-block:: default


    dataset = qml.data.load('qchem',molname="H2", bondlength=0.7)[0]
    hamiltonian, qubits = dataset.hamiltonian, len(dataset.hamiltonian.wires)

    print("Number of qubits = ", qubits)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Number of qubits =  4




.. GENERATED FROM PYTHON SOURCE LINES 270-274

For our ansatz, we use the circuit from the
`VQE tutorial <https://pennylane.ai/qml/demos/tutorial_vqe.html>`__
but expand out the arbitrary single-qubit rotations to elementary
gates (RZ-RY-RZ).

.. GENERATED FROM PYTHON SOURCE LINES 274-289

.. code-block:: default


    dev = qml.device("default.qubit", wires=qubits)
    hf_state = np.array([1, 1, 0, 0], requires_grad=False)

    def ansatz(params, wires=[0, 1, 2, 3]):
        qml.BasisState(hf_state, wires=wires)
        for i in wires:
            qml.RZ(params[3 * i], wires=i)
            qml.RY(params[3 * i + 1], wires=i)
            qml.RZ(params[3 * i + 2], wires=i)
        qml.CNOT(wires=[2, 3])
        qml.CNOT(wires=[2, 0])
        qml.CNOT(wires=[3, 1])









.. GENERATED FROM PYTHON SOURCE LINES 290-293

Note that the qubit register has been initialized to :math:`|1100\rangle`, which encodes for
the Hartree-Fock state of the hydrogen molecule described in the minimal basis.
Again, we define the cost function to be the following QNode that measures ``expval(H)``:

.. GENERATED FROM PYTHON SOURCE LINES 293-299

.. code-block:: default


    @qml.qnode(dev, interface="autograd")
    def cost(params):
        ansatz(params)
        return qml.expval(hamiltonian)








.. GENERATED FROM PYTHON SOURCE LINES 300-302

For this problem, we can compute the exact value of the
ground state energy via exact diagonalization. We provide the value below using the dataset.

.. GENERATED FROM PYTHON SOURCE LINES 302-306

.. code-block:: default


    exact_value = dataset.fci_energy # -1.1361895496530567









.. GENERATED FROM PYTHON SOURCE LINES 307-308

We now set up our optimizations runs.

.. GENERATED FROM PYTHON SOURCE LINES 308-315

.. code-block:: default


    np.random.seed(0)
    init_params = np.random.uniform(low=0, high=2 * np.pi, size=12, requires_grad=True)
    max_iterations = 500
    step_size = 0.5
    conv_tol = 1e-06








.. GENERATED FROM PYTHON SOURCE LINES 316-318

As was done with our previous VQE example, we run the standard gradient descent
optimizer.

.. GENERATED FROM PYTHON SOURCE LINES 318-354

.. code-block:: default


    opt = qml.GradientDescentOptimizer(step_size)

    params = init_params

    gd_cost = []

    for n in range(max_iterations):
        params, prev_energy = opt.step_and_cost(cost, params)
        gd_cost.append(prev_energy)

        energy = cost(params)
        conv = np.abs(energy - prev_energy)

        if n % 20 == 0:
            print(
                "Iteration = {:},  Energy = {:.8f} Ha".format(n, energy)
            )

        if conv <= conv_tol:
            break


    print()
    print("Final convergence parameter = {:.8f} Ha".format(conv))
    print("Number of iterations = ", n)
    print("Final value of the ground-state energy = {:.8f} Ha".format(energy))
    print(
        "Accuracy with respect to the FCI energy: {:.8f} Ha ({:.8f} kcal/mol)".format(
            np.abs(energy - exact_value), np.abs(energy - exact_value) * 627.503
        )
    )
    print()
    print("Final circuit parameters = \n", params)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Iteration = 0,  Energy = -0.09424484 Ha
    Iteration = 20,  Energy = -0.55156649 Ha
    Iteration = 40,  Energy = -1.12731550 Ha
    Iteration = 60,  Energy = -1.13583273 Ha
    Iteration = 80,  Energy = -1.13602376 Ha
    Iteration = 100,  Energy = -1.13611105 Ha
    Iteration = 120,  Energy = -1.13615247 Ha

    Final convergence parameter = 0.00000097 Ha
    Number of iterations =  130
    Final value of the ground-state energy = -1.13616408 Ha
    Accuracy with respect to the FCI energy: 0.00002547 Ha (0.01598211 kcal/mol)

    Final circuit parameters = 
     [3.44829694e+00 6.28318531e+00 3.78727399e+00 3.42360201e+00
     5.09255717e-08 4.05827240e+00 2.74944154e+00 6.07360246e+00
     6.24620668e+00 2.40923412e+00 6.28318531e+00 3.32314479e+00]




.. GENERATED FROM PYTHON SOURCE LINES 355-357

Next, we run the optimizer employing quantum natural gradients. We also need to make the
Hamiltonian coefficients non-differentiable by setting ``requires_grad=False``.

.. GENERATED FROM PYTHON SOURCE LINES 357-393

.. code-block:: default


    hamiltonian = qml.Hamiltonian(np.array(hamiltonian.coeffs, requires_grad=False), hamiltonian.ops)

    opt = qml.QNGOptimizer(step_size, lam=0.001, approx="block-diag")

    params = init_params
    prev_energy = cost(params)
    qngd_cost = []

    for n in range(max_iterations):
        params, prev_energy = opt.step_and_cost(cost, params)
        qngd_cost.append(prev_energy)

        energy = cost(params)
        conv = np.abs(energy - prev_energy)

        if n % 4 == 0:
            print(
                "Iteration = {:},  Energy = {:.8f} Ha".format(n, energy)
            )

        if conv <= conv_tol:
            break


    print("\nFinal convergence parameter = {:.8f} Ha".format(conv))
    print("Number of iterations = ", n)
    print("Final value of the ground-state energy = {:.8f} Ha".format(energy))
    print(
        "Accuracy with respect to the FCI energy: {:.8f} Ha ({:.8f} kcal/mol)".format(
            np.abs(energy - exact_value), np.abs(energy - exact_value) * 627.503
        )
    )
    print()
    print("Final circuit parameters = \n", params)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Iteration = 0,  Energy = -0.32164654 Ha
    Iteration = 4,  Energy = -0.46875045 Ha
    Iteration = 8,  Energy = -0.85090214 Ha
    Iteration = 12,  Energy = -1.13575345 Ha
    Iteration = 16,  Energy = -1.13618926 Ha

    Final convergence parameter = 0.00000022 Ha
    Number of iterations =  17
    Final value of the ground-state energy = -1.13618947 Ha
    Accuracy with respect to the FCI energy: 0.00000008 Ha (0.00004853 kcal/mol)

    Final circuit parameters = 
     [3.44829694e+00 6.28318510e+00 3.78727399e+00 3.42360201e+00
     4.03274702e-04 4.05827240e+00 2.74944154e+00 6.07375127e+00
     6.28401998e+00 2.40923412e+00 6.28318525e+00 3.32314479e+00]




.. GENERATED FROM PYTHON SOURCE LINES 394-400

Visualizing the results
^^^^^^^^^^^^^^^^^^^^^^^

To evaluate the performance of our two optimizers, we can compare: (a) the
number of steps it takes to reach our ground state estimate and (b) the quality of our ground
state estimate by comparing the final optimization energy to the exact value.

.. GENERATED FROM PYTHON SOURCE LINES 400-410

.. code-block:: default


    plt.style.use("seaborn")
    plt.plot(np.array(gd_cost) - exact_value, "g", label="Gradient descent")
    plt.plot(np.array(qngd_cost) - exact_value, "k", label="Quantum natural gradient descent")
    plt.yscale("log")
    plt.ylabel("Energy difference")
    plt.xlabel("Step")
    plt.legend()
    plt.show()




.. image-sg:: /demos/images/sphx_glr_tutorial_vqe_qng_003.png
   :alt: tutorial vqe qng
   :srcset: /demos/images/sphx_glr_tutorial_vqe_qng_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/qml/qml/demonstrations/tutorial_vqe_qng.py:401: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn")




.. GENERATED FROM PYTHON SOURCE LINES 411-415

We see that by employing quantum natural gradients, it takes fewer steps
to reach a ground state estimate and the optimized energy achieved by
the optimizer is lower than that obtained using vanilla gradient descent.


.. GENERATED FROM PYTHON SOURCE LINES 417-449

Robustness in parameter initialization
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

While results above show a more rapid convergence for quantum natural gradients,
what if we were just lucky, i.e., we started at a "good" point in parameter space?
How do we know this will be the case with high probability regardless of the
parameter initialization?

Using the same system Hamiltonian, ansatz, and device, we tested the robustness
of the ``QNGOptimizer`` by running 10 independent trials with random parameter initializations.
For this numerical test, our optimizer does not terminate based on energy improvement; we fix the number of
iterations to 200.
We show the result of this test below (after pre-computing), where we plot the mean and standard
deviation of the energies over optimization steps for quantum natural gradient and standard gradient descent.

.. figure:: ../demonstrations/vqe_qng/k_runs_.png
    :align: center
    :width: 60%
    :target: javascript:void(0)

We observe that quantum natural gradient on average converges faster for this system.

.. note::

    While using QNG may help accelerate the VQE algorithm in terms of optimization steps,
    each QNG step is more costly than its vanilla gradient descent counterpart due to
    a greater number of calls to the quantum computer that are needed to compute the Fubini-Study metric tensor.

While further benchmark studies are needed to better understand the advantages
of quantum natural gradient, preliminary studies such as this tutorial show the potentials
of the method. ðŸŽ‰


.. GENERATED FROM PYTHON SOURCE LINES 451-479

References
--------------

.. [#stokes2019]

    Stokes, James, *et al.*, "Quantum Natural Gradient".
    `arXiv preprint arXiv:1909.02108 (2019).
    <https://arxiv.org/abs/1909.02108>`__

.. [#yamamoto2019]

    Yamamoto, Naoki, "On the natural gradient for variational quantum eigensolver".
    `arXiv preprint arXiv:1909.05074 (2019).
    <https://arxiv.org/abs/1909.05074>`__

.. [#peruzzo2014]

    Alberto Peruzzo, Jarrod McClean *et al.*, "A variational eigenvalue solver on a photonic
    quantum processor". `Nature Communications 5, 4213 (2014).
    <https://www.nature.com/articles/ncomms5213?origin=ppub>`__


About the authors
-----------------
.. include:: ../_static/authors/maggie_li.txt

.. include:: ../_static/authors/lana_bozanic.txt

.. include:: ../_static/authors/sukin_sim.txt


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  16.515 seconds)


.. _sphx_glr_download_demos_tutorial_vqe_qng.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_vqe_qng.py <tutorial_vqe_qng.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_vqe_qng.ipynb <tutorial_vqe_qng.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
