
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "demos/tutorial_variational_classifier.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_demos_tutorial_variational_classifier.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_demos_tutorial_variational_classifier.py:


.. role:: html(raw)
   :format: html

.. _variational_classifier:

Variational classifier
======================

.. meta::
    :property="og:description": Using PennyLane to implement quantum circuits that can be trained from labelled data to
        classify new data samples.
    :property="og:image": https://pennylane.ai/qml/_images/classifier_output_59_0.png

.. related::

   tutorial_data_reuploading_classifier Data-reuploading classifier
   tutorial_multiclass_classification Multiclass margin classifier
   ensemble_multi_qpu Ensemble classification with Rigetti and Qiskit devices

*Author: Maria Schuld — Posted: 11 October 2019. Last updated: 19 January 2021.*

In this tutorial, we show how to use PennyLane to implement variational
quantum classifiers - quantum circuits that can be trained from labelled
data to classify new data samples. The architecture is inspired by
`Farhi and Neven (2018) <https://arxiv.org/abs/1802.06002>`__ as well as
`Schuld et al. (2018) <https://arxiv.org/abs/1804.00633>`__.

.. GENERATED FROM PYTHON SOURCE LINES 31-56

We will first show that the variational quantum classifier can reproduce
the parity function

.. math::

    f: x \in \{0,1\}^{\otimes n} \rightarrow y =
    \begin{cases} 1 \text{  if uneven number of ones in } x \\ 0
    \text{ otherwise} \end{cases}.

This optimization example demonstrates how to encode binary inputs into
the initial state of the variational circuit, which is simply a
computational basis state.

We then show how to encode real vectors as amplitude vectors (*amplitude
encoding*) and train the model to recognize the first two classes of
flowers in the Iris dataset.

1. Fitting the parity function
------------------------------

Imports
~~~~~~~

As before, we import PennyLane, the PennyLane-provided version of NumPy,
and an optimizer.

.. GENERATED FROM PYTHON SOURCE LINES 63-67

Quantum and classical nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~

We create a quantum device with four “wires” (or qubits).

.. GENERATED FROM PYTHON SOURCE LINES 71-77

Variational classifiers usually define a “layer” or “block”, which is an
elementary circuit architecture that gets repeated to build the
variational circuit.

Our circuit layer consists of an arbitrary rotation on every qubit, as
well as CNOTs that entangle each qubit with its neighbour.

.. GENERATED FROM PYTHON SOURCE LINES 93-104

We also need a way to encode data inputs :math:`x` into the circuit, so
that the measured output depends on the inputs. In this first example,
the inputs are bitstrings, which we encode into the state of the qubits.
The quantum state :math:`\psi` after
state preparation is a computational basis state that has 1s where
:math:`x` has 1s, for example

.. math::  x = 0101 \rightarrow |\psi \rangle = |0101 \rangle .

We use the :class:`~pennylane.BasisState` function provided by PennyLane, which expects
``x`` to be a list of zeros and ones, i.e. ``[0,1,0,1]``.

.. GENERATED FROM PYTHON SOURCE LINES 111-114

Now we define the quantum node as a state preparation routine, followed
by a repetition of the layer structure. Borrowing from machine learning,
we call the parameters ``weights``.

.. GENERATED FROM PYTHON SOURCE LINES 128-138

Different from previous examples, the quantum node takes the data as a
keyword argument ``x`` (with the default value ``None``). Keyword
arguments of a quantum node are considered as fixed when calculating a
gradient; they are never trained.

If we want to add a “classical” bias parameter, the variational quantum
classifier also needs some post-processing. We define the final model by
a classical node that uses the first variable, and feeds the remainder
into the quantum node. Before this, we reshape the list of remaining
variables for easy use in the quantum node.

.. GENERATED FROM PYTHON SOURCE LINES 145-151

Cost
~~~~

In supervised learning, the cost function is usually the sum of a loss
function and a regularizer. We use the standard square loss that
measures the distance between target labels and model predictions.

.. GENERATED FROM PYTHON SOURCE LINES 163-165

To monitor how many inputs the current classifier predicted correctly,
we also define the accuracy given target labels and model predictions.

.. GENERATED FROM PYTHON SOURCE LINES 179-181

For learning tasks, the cost depends on the data - here the features and
labels considered in the iteration of the optimization routine.

.. GENERATED FROM PYTHON SOURCE LINES 189-200

Optimization
~~~~~~~~~~~~

Let’s now load and preprocess some data.

.. note::

    The parity dataset can be downloaded
    :html:`<a href="https://raw.githubusercontent.com/XanaduAI/qml/master/demonstrations/variational_classifier/data/parity.txt"
    download=parity.txt target="_blank">here</a>` and
    should be placed in the subfolder ``variational_classifier/data``.

.. GENERATED FROM PYTHON SOURCE LINES 212-215

We initialize the variables randomly (but fix a seed for
reproducibility). The first variable in the list is used as a bias,
while the rest is fed into the gates of the variational circuit.

.. GENERATED FROM PYTHON SOURCE LINES 225-226

Next we create an optimizer and choose a batch size…

.. GENERATED FROM PYTHON SOURCE LINES 231-235

…and train the optimizer. We track the accuracy - the share of correctly
classified data samples. For this we compute the outputs of the
variational classifier and turn them into predictions in
:math:`\{-1,1\}` by taking the sign of the output.

.. GENERATED FROM PYTHON SOURCE LINES 258-266

2. Iris classification
----------------------

Quantum and classical nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To encode real-valued vectors into the amplitudes of a quantum state, we
use a 2-qubit simulator.

.. GENERATED FROM PYTHON SOURCE LINES 270-284

State preparation is not as simple as when we represent a bitstring with
a basis state. Every input x has to be translated into a set of angles
which can get fed into a small routine for state preparation. To
simplify things a bit, we will work with data from the positive
subspace, so that we can ignore signs (which would require another
cascade of rotations around the z axis).

The circuit is coded according to the scheme in `Möttönen, et al.
(2004) <https://arxiv.org/abs/quant-ph/0407010>`__, or—as presented
for positive vectors only—in `Schuld and Petruccione
(2018) <https://link.springer.com/book/10.1007/978-3-319-96424-9>`__. We
had to also decompose controlled Y-axis rotations into more basic
circuits following `Nielsen and Chuang
(2010) <http://www.michaelnielsen.org/qcqi/>`__.

.. GENERATED FROM PYTHON SOURCE LINES 315-316

Let’s test if this routine actually works.

.. GENERATED FROM PYTHON SOURCE LINES 337-344

Note that the ``default.qubit`` simulator provides a shortcut to
``statepreparation`` with the command
``qml.QubitStateVector(x, wires=[0, 1])``. However, some devices may not
support an arbitrary state-preparation routine.

Since we are working with only 2 qubits now, we need to update the layer
function as well.

.. GENERATED FROM PYTHON SOURCE LINES 353-356

The variational classifier model and its cost remain essentially the
same, but we have to reload them with the new state preparation and
layer functions.

.. GENERATED FROM PYTHON SOURCE LINES 378-392

Data
~~~~

We then load the Iris data set. There is a bit of preprocessing to do in
order to encode the inputs into the amplitudes of a quantum state. In
the last preprocessing step, we translate the inputs x to rotation
angles using the ``get_angles`` function we defined above.

.. note::

    The Iris dataset can be downloaded
    :html:`<a href="https://raw.githubusercontent.com/XanaduAI/qml/master/demonstrations/variational_classifier/data/iris_classes1and2_scaled.txt"
    download=parity.txt target="_blank">here</a>` and should be placed
    in the subfolder ``variational_classifer/data``.

.. GENERATED FROM PYTHON SOURCE LINES 392-413

.. code-block:: default






    # pad the vectors to size 2^2 with constant values




    # normalize each input




    # angles for state preparation are new features












.. GENERATED FROM PYTHON SOURCE LINES 414-420

These angles are our new features, which is why we have renamed X to
“features” above. Let’s plot the stages of preprocessing and play around
with the dimensions (dim1, dim2). Some of them still separate the
classes well, while others are less informative.

*Note: To run the following code you need the matplotlib library.*

.. GENERATED FROM PYTHON SOURCE LINES 455-458

This time we want to generalize from the data samples. To monitor the
generalization performance, the data is split into training and
validation set.

.. GENERATED FROM PYTHON SOURCE LINES 458-472

.. code-block:: default











    # We need these later for plotting










.. GENERATED FROM PYTHON SOURCE LINES 473-477

Optimization
~~~~~~~~~~~~

First we initialize the variables.

.. GENERATED FROM PYTHON SOURCE LINES 485-486

Again we optimize the cost. This may take a little patience.

.. GENERATED FROM PYTHON SOURCE LINES 486-515

.. code-block:: default





    # train the variational classifier































.. GENERATED FROM PYTHON SOURCE LINES 516-518

We can plot the continuous output of the variational classifier for the
first two dimensions of the Iris data set.

.. GENERATED FROM PYTHON SOURCE LINES 518-583

.. code-block:: default





    # make data for decision regions



    # preprocess grid points like data inputs above










    # plot decision regions








    # plot data











































.. GENERATED FROM PYTHON SOURCE LINES 584-586

About the author
----------------
.. include:: ../_static/authors/maria_schuld.txt


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.001 seconds)


.. _sphx_glr_download_demos_tutorial_variational_classifier.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tutorial_variational_classifier.py <tutorial_variational_classifier.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tutorial_variational_classifier.ipynb <tutorial_variational_classifier.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
