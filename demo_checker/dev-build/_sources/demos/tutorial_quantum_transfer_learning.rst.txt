
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "demos/tutorial_quantum_transfer_learning.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_demos_tutorial_quantum_transfer_learning.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_demos_tutorial_quantum_transfer_learning.py:


.. _quantum_transfer_learning:

Quantum transfer learning
=========================

.. meta::
    :property="og:description": Combine PyTorch and PennyLane to train a hybrid quantum-classical image
        classifier using transfer learning.
    :property="og:image": https://pennylane.ai/qml/_images/transfer_images.png

*Author: Andrea Mari â€” Posted: 19 December 2019. Last updated: 28 January 2021.*

In this tutorial we apply a machine learning method, known as *transfer learning*, to an
image classifier based on a hybrid classical-quantum network.

This example follows the general structure of the PyTorch
`tutorial on transfer learning <https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_
by Sasank Chilamkurthy, with the crucial difference of using a quantum circuit to perform the
final classification task.

More details on this topic can be found in the research paper [1] (`Mari et al. (2019) <https://arxiv.org/abs/1912.08278>`_).


Introduction
------------

Transfer learning is a well-established technique for training artificial neural networks (see e.g., Ref. [2]),
which is based on the general intuition that if a pre-trained network is good at solving a
given problem, then, with just a bit of additional training, it can be used to also solve a different
but related problem.

As discussed in Ref. [1], this idea can be formalized in terms of two abstract networks :math:`A`
and :math:`B`, independently from their quantum or classical physical nature.

|


.. figure:: ../demonstrations/quantum_transfer_learning/transfer_learning_general.png
   :scale: 45%
   :alt: transfer_general
   :align: center

|

As sketched in the above figure, one can give the following **general definition of the
transfer learning method**:

1. Take a network :math:`A` that has been pre-trained on a dataset :math:`D_A` and for a given
   task :math:`T_A`.

2. Remove some of the final layers. In this way, the resulting truncated network :math:`A'`
   can be used as a feature extractor.

3. Connect a new trainable network :math:`B` at the end of the pre-trained network :math:`A'`.

4. Keep the weights of :math:`A'` constant, and train the final block :math:`B` with a
   new dataset :math:`D_B` and/or for a new task of interest :math:`T_B`.

When dealing with hybrid systems, depending on the physical nature (classical or quantum) of the
networks :math:`A` and :math:`B`, one can have different implementations of transfer learning as

summarized in following table:

|

.. rst-class:: docstable

+-----------+-----------+-----------------------------------------------------+
| Network A | Network B | Transfer learning scheme                            |
+===========+===========+=====================================================+
| Classical | Classical | CC - Standard classical method. See e.g., Ref. [2]. |
+-----------+-----------+-----------------------------------------------------+
| Classical | Quantum   | CQ - **Hybrid model presented in this tutorial.**   |
+-----------+-----------+-----------------------------------------------------+
| Quantum   | Classical | QC - Model studied in Ref. [1].                     |
+-----------+-----------+-----------------------------------------------------+
| Quantum   | Quantum   | QQ - Model studied in Ref. [1].                     |
+-----------+-----------+-----------------------------------------------------+

Classical-to-quantum transfer learning
--------------------------------------

We focus on the CQ transfer learning scheme discussed in the previous section and we give a specific example.

1. As pre-trained network :math:`A` we use **ResNet18**, a deep residual neural network introduced by
   Microsoft in Ref. [3], which is pre-trained on the *ImageNet* dataset.

2. After removing its final layer we obtain :math:`A'`, a pre-processing block which maps any
   input high-resolution image into 512 abstract features.

3. Such features are classified by a 4-qubit "dressed quantum circuit" :math:`B`, i.e., a
   variational quantum circuit sandwiched between two classical layers.

4. The hybrid model is trained, keeping :math:`A'` constant, on the *Hymenoptera* dataset
   (a small subclass of ImageNet) containing images of *ants* and *bees*.

A graphical representation of the full data processing pipeline is given in the figure below.

.. figure:: ../demonstrations/quantum_transfer_learning/transfer_learning_c2q.png
   :scale: 55%
   :alt: transfer_c2q
   :align: center

.. GENERATED FROM PYTHON SOURCE LINES 108-118

General setup
------------------------

.. note::

   To use the PyTorch interface in PennyLane, you must first
   `install PyTorch <https://pytorch.org/get-started/locally/#start-locally>`_.

In addition to *PennyLane*, we will also need some standard *PyTorch* libraries and the
plotting library *matplotlib*.

.. GENERATED FROM PYTHON SOURCE LINES 118-149

.. code-block:: default


    # Some parts of this code are based on the Python script:
    # https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py
    # License: BSD





    # PyTorch







    # Pennylane






    # Plotting


    # OpenMP: number of parallel threads.










.. GENERATED FROM PYTHON SOURCE LINES 150-156

Setting of the main hyper-parameters of the model
------------------------------------------------------------

.. note::
  To reproduce the results of Ref. [1], ``num_epochs`` should be set to ``30`` which may take a long time.
  We suggest to first try with ``num_epochs=1`` and, if everything runs smoothly, increase it to a larger value.

.. GENERATED FROM PYTHON SOURCE LINES 168-169

We initialize a PennyLane device with a ``default.qubit`` backend.

.. GENERATED FROM PYTHON SOURCE LINES 173-174

We configure PyTorch to use CUDA only if available. Otherwise the CPU is used.

.. GENERATED FROM PYTHON SOURCE LINES 178-191

Dataset loading
------------------------------------------------------------

.. note::
    The dataset containing images of *ants* and *bees* can be downloaded
    `here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_ and
    should be extracted in the subfolder ``../_data/hymenoptera_data``.

This is a very small dataset (roughly 250 images), too small for training from scratch a
classical or quantum model, however it is enough when using *transfer learning* approach.

The PyTorch packages ``torchvision`` and ``torch.utils.data`` are used for loading the dataset
and performing standard preliminary image operations: resize, center, crop, normalize, *etc.*

.. GENERATED FROM PYTHON SOURCE LINES 191-244

.. code-block:: default


































    # Initialize dataloader





    # function to plot images




















.. GENERATED FROM PYTHON SOURCE LINES 245-246

Let us show a batch of the test data, just to have an idea of the classification problem.

.. GENERATED FROM PYTHON SOURCE LINES 246-261

.. code-block:: default


    # Get a batch of training data


    # Make a grid from batch

















.. GENERATED FROM PYTHON SOURCE LINES 262-265

Variational quantum circuit
------------------------------------
We first define some quantum layers that will compose the quantum circuit.

.. GENERATED FROM PYTHON SOURCE LINES 294-308

Now we define the quantum circuit through the PennyLane `qnode` decorator .

The structure is that of a typical variational quantum circuit:

* **Embedding layer:** All qubits are first initialized in a balanced superposition
  of *up* and *down* states, then they are rotated according to the input parameters
  (local embedding).

* **Variational layers:** A sequence of trainable rotation layers and constant
  entangling layers is applied.

* **Measurement layer:** For each qubit, the local expectation value of the :math:`Z`
  operator is measured. This produces a classical output vector, suitable for
  additional post-processing.

.. GENERATED FROM PYTHON SOURCE LINES 336-352

Dressed quantum circuit
------------------------

We can now define a custom ``torch.nn.Module`` representing a *dressed* quantum circuit.

This is a concatenation of:

* A classical pre-processing layer (``nn.Linear``).
* A classical activation function (``torch.tanh``).
* A constant ``np.pi/2.0`` scaling.
* The previously defined quantum circuit (``quantum_net``).
* A classical post-processing layer (``nn.Linear``).

The input of the module is a batch of vectors with 512 real parameters (features) and
the output is a batch of vectors with two real outputs (associated with the two classes
of images: *ants* and *bees*).

.. GENERATED FROM PYTHON SOURCE LINES 392-405

Hybrid classical-quantum model
------------------------------------

We are finally ready to build our full hybrid classical-quantum network.
We follow the *transfer learning* approach:

1. First load the classical pre-trained network *ResNet18* from the ``torchvision.models`` zoo.
2. Freeze all the weights since they should not be trained.
3. Replace the last fully connected layer with our trainable dressed quantum circuit (``DressedQuantumNet``).

.. note::
  The *ResNet18* model is automatically downloaded by PyTorch and it may take several minutes (only the first time).


.. GENERATED FROM PYTHON SOURCE LINES 405-417

.. code-block:: default







    # Notice that model_hybrid.fc is the last layer of ResNet18


    # Use CUDA or CPU according to the "device" object.









.. GENERATED FROM PYTHON SOURCE LINES 418-425

Training and results
------------------------

Before training the network we need to specify the *loss* function.

We use, as usual in classification problem, the *cross-entropy* which is
directly available within ``torch.nn``.

.. GENERATED FROM PYTHON SOURCE LINES 430-432

We also initialize the *Adam optimizer* which is called at each training step
in order to update the weights of the model.

.. GENERATED FROM PYTHON SOURCE LINES 437-439

We schedule to reduce the learning rate by a factor of ``gamma_lr_scheduler``
every 10 epochs.

.. GENERATED FROM PYTHON SOURCE LINES 446-449

What follows is a training function that will be called later.
This function should return a trained model that can be used to make predictions
(classifications).

.. GENERATED FROM PYTHON SOURCE LINES 549-550

We are ready to perform the actual training process.

.. GENERATED FROM PYTHON SOURCE LINES 556-558

Visualizing the model predictions
------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 560-561

We first define a visualization function for a batch of test data.

.. GENERATED FROM PYTHON SOURCE LINES 584-587

Finally, we can run the previous function to see a batch of images
with the corresponding predictions.


.. GENERATED FROM PYTHON SOURCE LINES 591-610

References
------------

[1] Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, and Nathan Killoran.
*Transfer learning in hybrid classical-quantum neural networks*. arXiv:1912.08278 (2019).

[2] Rajat Raina, Alexis Battle, Honglak  Lee,  Benjamin Packer, and Andrew Y Ng.
*Self-taught learning:  transfer learning from unlabeled data*.
Proceedings of the 24th International  Conference  on  Machine  Learning*, 759â€“766 (2007).

[3] Kaiming He, Xiangyu Zhang, Shaoqing ren and Jian Sun. *Deep residual learning for image recognition*.
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770-778 (2016).

[4] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Carsten Blank, Keri McKiernan, and Nathan Killoran.
*PennyLane: Automatic differentiation of hybrid quantum-classical computations*. arXiv:1811.04968 (2018).


About the author
----------------
.. include:: ../_static/authors/andrea_mari.txt


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.001 seconds)


.. _sphx_glr_download_demos_tutorial_quantum_transfer_learning.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: tutorial_quantum_transfer_learning.py <tutorial_quantum_transfer_learning.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: tutorial_quantum_transfer_learning.ipynb <tutorial_quantum_transfer_learning.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
