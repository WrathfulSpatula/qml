
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "demos/tutorial_contextuality.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_demos_tutorial_contextuality.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_demos_tutorial_contextuality.py:


Contextuality and inductive bias in QML
============================================================

.. meta::
    :property="og:description": Train a tailored quantum model on a contextuality-inspired dataset
    :property="og:image": https://pennylane.ai/qml/_images/contextuality_thumbnail.png

.. related::
    tutorial_geometric_qml

*Author: Joseph Bowles — Posted: 21 March 2023*

.. GENERATED FROM PYTHON SOURCE LINES 18-50

What machine learning problems are quantum computers likely to excel
at?

In the article *Contextuality and inductive bias in quantum machine
learning* [#paper]_ by Joseph Bowles,
Victoria J Wright, Máté Farkas, Nathan Killoran and Maria Schuld, we
look to contextuality for answers to this question.

Contextuality is a nonclassical phenomenon exhibited by quantum
systems, and it is necessary for computational advantage relative to
classical machines. To be a little more specific, we focus on the
framework of *generalized
contextuality* [#contextuality]_, 
which was introduced by Robert Spekkens in 2004. We find learning
problems for which contextuality plays a key role, and these problems
may therefore be good areas where quantum machine learning algorithms
shine. In this demo we will:

-  Describe a specific example of a contextuality-relevant problem that is based on the
   well-known rock-paper-scissors game, and
-  Construct and train a quantum model that is tailored to the
   symmetries of the problem.

Throughout the demo we will make use of JAX to vectorise and just-in-time compile
certain functions, which will speed things up. For more information on how to
combine JAX and PennyLane, see the PennyLane
`documentation <https://docs.pennylane.ai/en/stable/introduction/interfaces/jax.html>`__.

.. figure:: ../demonstrations/contextuality/socialthumbnail_large_Contextuality.png
   :align: center
   :width: 50%


.. GENERATED FROM PYTHON SOURCE LINES 52-55

Generalized contextuality
-------------------------


.. GENERATED FROM PYTHON SOURCE LINES 58-114

Suppose we want to prepare the maximally mixed state of a single qubit, 
with :math:`\rho = \frac{1}{2}\mathbb{I}`. Although this corresponds to a
single density matrix, there are many ways we could prepare the state.
For example, we could mix the states :math:`\vert 0 \rangle` or
:math:`\vert 1 \rangle` with equal probability. Alternatively, we could
use the :math:`X` basis, and mix the states :math:`\vert + \rangle` or
:math:`\vert - \rangle`. Even though this may not strike us as particularly
strange, a remarkable coincidence is in fact going on here: an
experimentalist can perform two physically distinct procedures (namely,
preparing :math:`\rho` in the :math:`Z` or :math:`X` basis), however it
is impossible to distinguish which procedure was performed, since they
both result in the same density matrix and therefore give identical
predictions for all future measurements.

Such a coincidence demands an explanation. Something that one might expect
is the following: the description of the experiment in terms of quantum
states is not the most fundamental, and there are in fact other states
(we’ll write them as :math:`\lambda`), that comprise our quantum states.
In contextuality these are called *ontic states*, although they also go
by the name of *hidden variables*. When we prepare a state
:math:`\vert 0 \rangle`, :math:`\vert 1 \rangle`,
:math:`\vert + \rangle`, :math:`\vert - \rangle`, what is really going
on is that we prepare a mixture :math:`P_{\vert 0 \rangle}(\lambda)`,
:math:`P_{\vert 1 \rangle}(\lambda)`,
:math:`P_{\vert + \rangle}(\lambda)`,
:math:`P_{\vert - \rangle}(\lambda)` over the true ontic states. One may
imagine that the corresponding mixtures over the :math:`\lambda` s are
the same for the :math:`Z` and :math:`X` basis preparation:

.. math:: \frac{1}{2}P_{\vert 0 \rangle}(\lambda)+\frac{1}{2}P_{\vert 1 \rangle}(\lambda)=\frac{1}{2}P_{\vert + \rangle}(\lambda)+\frac{1}{2}P_{\vert - \rangle}(\lambda).

This is a rather natural explanation of our coincidence: the two
procedures are indistinguishable because they actually correspond to the
same mixture over the fundamental states :math:`\lambda`. This sort of
explanation is called *non-contextual*, since the two mixtures do not
depend on the basis (that is, the context) in which the state is
prepared. It turns out that if one tries to apply this logic to all the
indistinguishabilities in quantum theory, one arrives at contradictions:
it simply cannot be done. For this reason we say that quantum theory is
a *contextual* theory.

In the paper we frame generalized contextuality in the machine learning
setting, which allows us to define what we mean by a contextual learning
model. In a nutshell, this definition demands that if a learning model
is non-contextual, then any indistinguishabilities in the model should
be explained in a non-contextual way similar to the above. This results
in constraints on the learning model, which limits their expressivity.
Since quantum models are contextual, they can of course go beyond these
constraints, and understanding when and how they do this may shed light
on the non-classical features that separate quantum models from
classical ones.

Below we describe a specific learning problem that demonstrates this
approach. As we will see, the corresponding indistinguishability relates
to an *inductive bias* of the learning model.


.. GENERATED FROM PYTHON SOURCE LINES 117-120

The rock-paper-scissors game
------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 123-143

The learning problem we will consider involves three players
(we'll call them players 0, 1 and 2) playing a
variant of the rock-paper-scissors game with a referee.
The game goes as follows. In each round, a player can choose to play
either rock (R), paper (P) or scissors (S). Each player also has a
‘special’ action. For player 0 it is R, for player 1 it is P and for
player 2 it is S. The actions of the players are then compared pairwise,
with the following rules:

-  If two players play different actions, then one player beats the
   other following the usual rule (rock beats scissors, scissors beats
   paper, paper beats rock).
-  If two players play the same action, the one who plays their special
   action beats the other. If neither plays their special action, it is
   a draw.

A referee then decides the winners and the losers of that round: the
winners receive :math:`\$1` and the losers lose :math:`\$1` (we will
call this their *payoff* for that round).


.. GENERATED FROM PYTHON SOURCE LINES 145-148

.. figure:: ../demonstrations/contextuality/rps.png
   :align: center
   :width: 50%

.. GENERATED FROM PYTHON SOURCE LINES 150-166

Naturally, the more players a given player beats, the higher the
probability that they get a positive payoff. In particular, if we denote
the payoff of player :math:`k` by :math:`y_k=\pm1` then

.. math:: \mathbb{E}(y_k) = \frac{n^k_{\text{win}}-n^k_{\text{lose}}}{2},

where :math:`n^k_{\text{win}}`, :math:`n^k_{\text{lose}}` is the number
of players that player :math:`k` beats or loses to in that round. This
ensures that a player is certain to get a positive (or negative) payoff
if they beat (or lose) to everyone.

To make this concrete, we will construct three 3x3 matrices ``A01``,
``A02``, ``A12`` which determine the rules for each pair of players.
``A01`` contains the expected payoff values of player 0 when playing
against player 1. Using the rules of the game it looks as follows.


.. GENERATED FROM PYTHON SOURCE LINES 168-171

.. figure:: ../demonstrations/contextuality/rpstable.png
   :align: center
   :width: 50%

.. GENERATED FROM PYTHON SOURCE LINES 174-176

The matrices ``A02`` and ``A12`` are defined similarly.


.. GENERATED FROM PYTHON SOURCE LINES 190-196

We can also define the matrices ``A10``, ``A20``, ``A21``. Since
switching the players corresponds to taking the transpose matrix and
a positive payoff for one player implies a negative for the other,
these matrices are given by
the negative of the transposed matrix:


.. GENERATED FROM PYTHON SOURCE LINES 202-209

Note that the above game is an example of a *zero-sum game*: if player 1 beats
player 2 then necessarily player 2 loses to player 1. This implies
:math:`\sum_k n^k_{\text{wins}}=\sum_kn^k_{\text{lose}}` and so in every
round we have

.. math:: \mathbb{E}(y_1)+\mathbb{E}(y_2)+\mathbb{E}(y_3)=0.


.. GENERATED FROM PYTHON SOURCE LINES 212-215

Constructing the dataset
------------


.. GENERATED FROM PYTHON SOURCE LINES 218-240

Here we construct a dataset based on the above game. Our data points
correspond to probability
distributions over possible actions: in the  zero-sum game literature
these are called *strategies*.
For example, a strategy for player k is a
vector

.. math:: x_k=(P(a_k=R), P(a_k=P), P(a_k=S))

where :math:`a_k` denotes player :math:`k`\ ’s action. We collect these
into a strategy matrix X

.. math::

   X = \begin{pmatrix}
       P(a_0=R) & P(a_0=P) & P(a_0=S) \\
       P(a_1=R) & P(a_1=P) & P(a_1=S) \\
       P(a_2=R) & P(a_2=P) & P(a_2=S) .
       \end{pmatrix}




.. GENERATED FROM PYTHON SOURCE LINES 243-246

Let’s write a function
to generate a set of strategy matrices.


.. GENERATED FROM PYTHON SOURCE LINES 261-283

The labels in our dataset correspond to payoff values :math:`y_k` of the
three players. Following the rules of probability we find that if the
players use strategies :math:`x_0, x_1, x_2` the expected values of
:math:`n_{\text{wins}}^k - n_{\text{lose}}^k` are given
by

.. math:: \mathbb{E}[n_{\text{wins}}^0 - n_{\text{lose}}^0]  = x_0 \cdot A_{01}\cdot x_1^T+x_0 \cdot A_{02}\cdot x_2^T

.. math:: \mathbb{E}[n_{\text{wins}}^1 - n_{\text{lose}}^1] = x_1 \cdot A_{10}\cdot x_0^T+x_1 \cdot A_{12}\cdot x_2^T

.. math:: \mathbb{E}[n_{\text{wins}}^2 - n_{\text{lose}}^2] = x_2 \cdot A_{20}\cdot x_0^T+x_2 \cdot A_{21}\cdot x_1^T

Since we have seen that
:math:`\mathbb{E}(y_k) = \frac{n^k_{\text{win}}-n^k_{\text{lose}}}{2}`
it follows that the probability for player :math:`k` to receive a
positive payoff given strategies :math:`X` is

.. math:: P(y_k=+1\vert X) = \frac{\mathbb{E}(y_k\vert X)+1}{2} =  \frac{(\mathbb{E}[n_{\text{wins}}^k - n_{\text{lose}}^k])/2+1}{2}

Putting all this together we can write some code to generate the labels
for our data set.


.. GENERATED FROM PYTHON SOURCE LINES 283-313

.. code-block:: default














    # JAX vectorisation























.. GENERATED FROM PYTHON SOURCE LINES 314-322

Note that since strategies are probabilistic mixtures of actions, our
data labels satisfy a zero-sum condition

.. math:: \mathbb{E}(y_1\vert X_i)+\mathbb{E}(y_2\vert X_i)+\mathbb{E}(y_3\vert X_i)=0.

We can verify this using the payoff probability matrix ``P`` that we
used to sample the labels:


.. GENERATED FROM PYTHON SOURCE LINES 328-331

The learning problem
--------------------


.. GENERATED FROM PYTHON SOURCE LINES 334-356

Suppose we are given a data set :math:`\{X_i,\vec{y}_i\}` consisting of
strategy matrices and payoff values, however we don’t know what the
underlying game is (that is, we don’t know the players were playing the
rock, paper scissors game described above). We do have one piece of
information though: we know the game is zero-sum so that the data
generation process satisfies

.. math:: \mathbb{E}(y_0\vert X_i)+\mathbb{E}(y_1\vert X_i)+\mathbb{E}(y_2\vert X_i)=0.

Can we learn the rock, paper scissors game from this data? More
precisely, if we are given an unseen strategy matrix
:math:`X_{\text{test}}` our task is to sample from the three
distributions

.. math:: P(y_0\vert X_{\text{test}}), P(y_1\vert X_{\text{test}}), P(y_2\vert X_{\text{test}}).

Note we are not asking to sample from the joint distribution
:math:`P(\vec{y}\vert X_{\text{test}})` but the three marginal
distributions only. This can be seen as an instance of multi-task
learning, where a single task corresponds to sampling the payoff for one
of the three players.


.. GENERATED FROM PYTHON SOURCE LINES 359-362

Building inductive bias into a quantum model
--------------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 365-389

Here we describe a simple three qubit model to tackle this problem.
Since we know that the data satisfies the zero-sum condition, we aim to
create a quantum model that encodes this knowledge. That is, like
the data we want our model to satisfy

.. math:: \mathbb{E}(y_0\vert X_i)+\mathbb{E}(y_1\vert X_i)+\mathbb{E}(y_2\vert X_i)=0.

In machine learning, this is called encoding an *inductive
bias* into the model, and considerations like this are often crucial for
good generalisation performance.

.. note::
  Since the above holds for all :math:`X_i`, it implies an
  indistinguishability of the model: if we look at one of the labels at
  random, we are equally likely to see a positive or negative payoff
  regardless of :math:`X_i`, and so the :math:`X_i` are indistinguishable
  with respect to this observation. This implies a corresponding constraint
  on non-contextual learning models, which limits their expressivity and
  may therefore hinder their performance: see the paper for more details
  on how this looks in practice. Luckily for us quantum theory is a
  contextual theory, so these limitations don’t apply to our model!

The quantum model we consider has the following structure:


.. GENERATED FROM PYTHON SOURCE LINES 391-394

.. figure:: ../demonstrations/contextuality/model.png
   :align: center
   :width: 50%

.. GENERATED FROM PYTHON SOURCE LINES 397-416

The parameters :math:`\theta` and :math:`\alpha` are trainable
parameters of the model, and we will use the three :math:`Z`
measurements at the end of the circuit to sample the three labels.
Therefore, if we write the entire circuit as
:math:`\vert \psi(\alpha,\theta,X)\rangle` the zero sum condition will
be satisfied if

.. math:: \langle \psi(\alpha,\theta,X) \vert (Z_0+Z_1+Z_2) \vert \psi(\alpha,\theta,X) \rangle = 0.

Let’s see how we can create a model class that satisfies this. For
precise details on the structure of the model, check out Figure 6 in the
paper. We’ll first look at the parameterised unitary :math:`V_{\alpha}`,
that we call the *input preparation unitary*. This prepares a state
:math:`V_\alpha\vert 0 \rangle` such that

.. math:: \langle 0 \vert V^\dagger_\alpha (Z_0+Z_1+Z_2) V_\alpha\vert 0 \rangle = 0.

An example of such a circuit is the following.


.. GENERATED FROM PYTHON SOURCE LINES 428-446

The second unitary is a *bias invariant layer*: it preserves the value
of :math:`\langle Z_0+Z_1+Z_2 \rangle` for all input states into the
layer. To achieve this, the generators of the unitaries in this layer
must commute with the operator :math:`Z_0+Z_1+Z_2`. For example the
operator :math:`X\otimes X + Y\otimes Y + Z\otimes Z` (on any pair of
qubits) commutes with :math:`Z_0+Z_1+Z_2` and so a valid parameterised
gate could be

.. math:: e^{i\theta(X\otimes X\otimes\mathbb{I} + Y\otimes Y\otimes\mathbb{I} + Z\otimes Z\otimes\mathbb{I})}.

This kind of reasoning is an example of geometric quantum machine
learning (check out [#reptheory]_ and [#equivariant]_ or our own
`demo <https://pennylane.ai/qml/demos/tutorial_geometric_qml.html>`__ for an awesome introduction to the subject).
Below we construct the
bias invariant layer: note that all the generators commute with
:math:`Z_0+Z_1+Z_2`. The variables ``blocks`` and ``layers`` are model
hyperparameters that we will fix as ``blocks=1`` and ``layers=2``.


.. GENERATED FROM PYTHON SOURCE LINES 508-511

With our ``input_prep`` and ``bias_inv_layer`` functions we can now
define our quantum model.


.. GENERATED FROM PYTHON SOURCE LINES 511-527

.. code-block:: default












    # jax vectorisation, we vectorise over the data input (the second argument)











.. GENERATED FROM PYTHON SOURCE LINES 528-532

To investigate the effect of the encoded inductive bias, we will compare
this model to a generic model with the same data encoding and similar
number of parameters (46 vs 45 parameters).


.. GENERATED FROM PYTHON SOURCE LINES 560-566

**Warning**: Since we are using JAX it is important that our ``model``
and ``generic model`` functions are functionally pure (read more
`here <https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html>`__).
This means we cannot change the values of ``blocks`` or ``layers`` from
hereon since these values have been cached for JIT compilation.


.. GENERATED FROM PYTHON SOURCE LINES 569-572

Training and evaluation
-----------------------


.. GENERATED FROM PYTHON SOURCE LINES 575-589

To train the model we will minimise the negative log likelihood of the
labels given the data

.. math:: \mathcal{L} = -\frac{1}{3\vert N \vert}\sum_{(X_i,\vec{y}_i)} \log(\mathcal{P}_0(y_i^{(0)}\vert X_i))+\log(\mathcal{P}_1(y_i^{(1)}\vert X_i))+\log(\mathcal{P}_2(y_i^{(2)}\vert X_i))

Here :math:`\mathcal{P}_k` is the probability distribution of the
:math:`k` label from the model, :math:`y_i^{(k)}` is the kth element
of the payoff vector :math:`\vec{y}_i` in the dataset, and :math:`N` is
the size of the training dataset. We remark that
training the negative log likelihood is in some sense cheating, since
for large quantum circuits we don’t know how to estimate it efficiently.
As generative modeling in QML progresses, we can hope however that
scalable methods that approximate this type of training may appear.


.. GENERATED FROM PYTHON SOURCE LINES 603-612

For evaluation we will use the average KL divergence between the true
data distribution and the model distribution

.. math:: \mathbb{E}_{P^\text{data}(X)} \left[\frac{1}{3}\sum_{k=1}^{3} D_{\text{KL}}(P^\text{data}_k(y\vert X)\vert\vert \mathcal{P}_k(y\vert X)) \right].

To estimate this we sample a test set of strategies, calculate their
payoff probabilities, and estimate the above expectation via the sample
mean.


.. GENERATED FROM PYTHON SOURCE LINES 612-659

.. code-block:: default































    # vectorise the kl_marginals function. Makes estimating the average KL diverence of a model faster.























.. GENERATED FROM PYTHON SOURCE LINES 660-663

To optimise the model we make use of the JAX optimization library optax.
We will use the adam gradient descent optimizer.


.. GENERATED FROM PYTHON SOURCE LINES 690-692

We are now ready to generate a data set and optimize our models!


.. GENERATED FROM PYTHON SOURCE LINES 692-709

.. code-block:: default


    # generate data









    # optimise the structured model

    # optimise the generic model










.. GENERATED FROM PYTHON SOURCE LINES 710-713

Let’s plot the average KL divergence and the negative log likelihood for
both models.


.. GENERATED FROM PYTHON SOURCE LINES 713-741

.. code-block:: default






    # subplots



    # KL divergence








    # negative log likelihood
















.. GENERATED FROM PYTHON SOURCE LINES 742-747

We see that the model that encodes the inductive bias achieves both a
lower training error and generalisation error, as can be expected.
Incorporating knowledge about the data into the model design is
generally a very good idea!


.. GENERATED FROM PYTHON SOURCE LINES 750-753

Conclusion
----------


.. GENERATED FROM PYTHON SOURCE LINES 755-808

In this demo we have constructed a dataset whose structure is
connected to generalized contextuality, and have shown how to encode
this structure as an inductive bias of a quantum model class. As is
often the case, we saw that this approach outperforms a generic model
class that does not take this knowledge into account. As a general rule,
considerations like this should be at the front of one's mind when
building a quantum model for a specific task.

That is all for this demo. In our paper [#paper]_, it is also shown how models of
this kind can perform better than classical surrogate
models [#surrogates]_ at this specific task,
which further strengthens the claim that the inductive bias of the
quantum model is useful. For more information and to read more about the
link between contextuality and QML, check out the full paper.


References
----------

.. [#paper]

    J. Bowles, V. J. Wright, M. Farkas, N. Killoran, M. Schuld
    "Contextuality and inductive bias in quantum machine learning."
    `arXiv:2302.01365 <https://arxiv.org/abs/2302.01365>`__, 2023.

.. [#contextuality]

    R. W. Spekkens
    "Contextuality for preparations, transformations, and unsharp measurements."
    `Phys. Rev. A 71, 052108 <https://journals.aps.org/pra/abstract/10.1103/PhysRevA.71.052108>`__, 2005.

.. [#reptheory]

    M. Ragone, P. Braccia, Q. T. Nguyen, L. Schatzki, P. J. Coles, F. Sauvage, M. Larocca, M. Cerezo
    "Representation Theory for Geometric Quantum Machine Learning."
    `arXiv:2210.07980 <https://arxiv.org/abs/2210.07980>`__, 2023.

.. [#equivariant]

    Q. T. Nguyen, L. Schatzki, P. Braccia, M. Ragone, P. J. Coles, F. Sauvage, M. Larocca, M. Cerezo
    "Theory for Equivariant Quantum Neural Networks."
    `arXiv:2210.08566 <https://arxiv.org/abs/2210.08566>`__, 2022.

.. [#surrogates]

    F. J. Schreiber, J. Eiser, J. J. Meyer
    "Classical surrogates for quantum learning models."
    `arXiv:2206.11740 <https://arxiv.org/abs/2206.11740>`__, 2022.


About the author
----------------
.. include:: ../_static/authors/joseph_bowles.txt


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.001 seconds)


.. _sphx_glr_download_demos_tutorial_contextuality.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tutorial_contextuality.py <tutorial_contextuality.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tutorial_contextuality.ipynb <tutorial_contextuality.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
