
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta content="Introduction to the Quantum natural SPSA optimizer, which reduces the number of quantum measurements in the optimization." property="og:description" />
<meta content="https://pennylane.ai/qml/_images/qnspsa_cover.png" property="og:image" />

  <link rel="icon" type="image/x-icon" href="../_static/favicon.ico">
  <link rel="shortcut icon" type="image/x-icon" href="../_static/favicon.ico">
  


  <meta property="og:title" content="Quantum natural SPSA optimizer &#8212; PennyLane">
  <meta property="og:url" content="https://pennylane.ai/qml/demos/qnspsa.html">
  <meta property="og:type" content="website">
  <meta name="twitter:card" content="summary_large_image">

  
  
  <meta content="Introduction to the Quantum natural SPSA optimizer, which reduces the number of quantum measurements in the optimization." property="og:description" />
  

  <!-- Google Fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&display=swap">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css">
  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/css/bootstrap.min.css">
  <!-- Material Design Bootstrap -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.5.14/css/mdb.min.css">
  <!-- NanoScroller -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.nanoscroller/0.8.7/css/nanoscroller.min.css">
  <!-- Syntax Highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/tomorrow-night.min.css">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
       TeX: {
         Macros: {
           pr : ['|\#1\\rangle\\langle\#1|',1],
           ket: ['\\left| \#1\\right\\rangle',1],
           bra: ['\\left\\langle \#1\\right|',1],
           xket: ['\\left| \#1\\right\\rangle_x',1],
           xbra: ['\\left\\langle \#1\\right|_x',1],
           braket: ['\\langle \#1 \\rangle',1],
           braketD: ['\\langle \#1 \\mid \#2 \\rangle',2],
           braketT: ['\\langle \#1 \\mid \#2 \\mid \#3 \\rangle',3],
           ketbra: ['| #1 \\rangle \\langle #2 |',2],
           hc: ['\\text{h.c.}',0],
           cc: ['\\text{c.c.}',0],
           h: ['\\hat',0],
           nn: ['\\nonumber',0],
           di: ['\\frac{d}{d \#1}',1],
           uu: ['\\mathcal{U}',0],
           inn: ['\\text{in}',0],
           out: ['\\text{out}',0],
           vac: ['\\text{vac}',0],
           I: ['\\hat{\\mathbf{1}}',0],
           x: ['\\hat{x}',0],
           p: ['\\hat{p}',0],
           a: ['\\hat{a}',0],
           ad: ['\\hat{a}^\\dagger',0],
           n: ['\\hat{n}',0],
           nbar: ['\\overline{n}',0],
           sech: ['\\mathrm{sech~}',0],
           tanh: ['\\mathrm{tanh~}',0],
           re: ['\\text{Re}',0],
           im: ['\\text{Im}',0],
           tr: ['\\mathrm{Tr} #1',1],
           sign: ['\\text{sign}',0],
           overlr: ['\\overset\\leftrightarrow{\#1}',1],
           overl: ['\\overset\leftarrow{\#1}',1],
           overr: ['\\overset\rightarrow{\#1}',1],
           avg: ['\\left< \#1 \\right>',1],
           slashed: ['\\cancel{\#1}',1],
           bold: ['\\boldsymbol{\#1}',1],
           d: ['\\mathrm d',0],
           expect: ["\\langle #1 \\rangle",1],
           pde: ["\\frac{\\partial}{\\partial \#1}",1],
           R: ["\\mathbb{R}",0],
           C: ["\\mathbb{C}",0],
           Ad: ["\\text{Ad}",0],
           Var: ["\\text{Var}",0],
           bx: ["\\mathbf{x}", 0],
           bm: ["\\boldsymbol{\#1}",1],
           haf: ["\\mathrm{haf}",0],
           lhaf: ["\\mathrm{lhaf}",0]
         }
       }
     });
     </script>

  <!-- Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-130507810-1"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-130507810-1');
      </script>
  
    <title>Quantum natural SPSA optimizer &#8212; PennyLane  documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/xanadu.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/light-slider.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/hubs.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <link rel="canonical" href="https://pennylane.ai/qml/demos/qnspsa.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Differentiating quantum error mitigation transforms" href="tutorial_diffable-mitigation.html" />
    <link rel="prev" title="QAOA for MaxCut" href="tutorial_qaoa_maxcut.html" /> 
  </head><body><nav class="navbar navbar-expand-lg navbar-light white sticky-top">

<!-- Logo and Title -->









  



  <a class="navbar-brand nav-link" href="https://pennylane.ai">
    
  <img class="pr-1" src=" ../_static/logo.png" width="28px"></img>
  
    <img id="navbar-wordmark" src="../_static/pennylane.svg"></img>
  
  </a>


  <!-- [Mobile] Collapse Button -->
  <div class="row right">
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#basicExampleNav"
      aria-controls="basicExampleNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
  </div>

  <!-- [Mobile] Collapsible Content -->
  <div class="collapse navbar-collapse" id="basicExampleNav">

    <!-- Links on the Left -->
    <ul class="navbar-nav mr-auto">
      
        
          
            <li class="nav-item active">
              <a class="nav-link" href="https://pennylane.ai/qml/">
                
  
    Learn
  

              </a>
              <span class="sr-only">(current)</span>
            </li>
          

        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/qml/demonstrations.html">
                
  
    Demos
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/install.html">
                
  
    Install
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/plugins.html">
                
  
    Plugins
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://docs.pennylane.ai">
                
  
    Documentation
  

            </a>
          </li>
        
      
        
          <li class="nav-item">
            <a class="nav-link" href="https://pennylane.ai/blog/">
                
  
    Blog
  

            </a>
          </li>
        
      
    </ul>

    <!-- Links on the Right -->
    <ul class="navbar-nav ml-auto nav-flex-icons">
      
        <li class="nav-item">
          <a class="nav-link" href="https://pennylane.ai/faq.html">
            <i class="fas fa-question pr-1"></i> FAQ
          </a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://discuss.pennylane.ai/">
            <i class="fab fa-discourse pr-1"></i> Support
          </a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/PennyLaneAI/pennylane">
            <i class="fab fa-github pr-1"></i> GitHub
          </a>
        </li>
      

    </ul>
  </div>

</nav>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="tutorial_diffable-mitigation.html" title="Differentiating quantum error mitigation transforms"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tutorial_qaoa_maxcut.html" title="QAOA for MaxCut"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../quantum-computing.html" >Quantum Computing</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="../demos_optimization.html" accesskey="U">Optimization</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Quantum natural SPSA optimizer</a></li> 
      </ul>
    </div>
    <div class="container-wrapper">
        <div id="content">
          <div id="right-column">
            
            

            <div class="document clearer body">
              
    <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-demos-qnspsa-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="quantum-natural-spsa-optimizer">
<span id="sphx-glr-demos-qnspsa-py"></span><h1>Quantum natural SPSA optimizer<a class="headerlink" href="#quantum-natural-spsa-optimizer" title="Permalink to this headline">¶</a></h1>
<p><script type="text/javascript">
    var related_tutorials = ["tutorial_spsa.html"];
    var related_tutorials_titles = ['Simultaneous perturbation stochastic approximation (SPSA) optimizer'];
</script></p>
<p><em>Author: Yiheng Duan — Posted: 18 July 2022. Last updated: 05 September 2022.</em></p>
<p>In this tutorial, we show how we can implement the
<a class="reference external" href="https://quantum-journal.org/papers/q-2021-10-20-567/">quantum natural simultaneous perturbation stochastic approximation (QN-SPSA) optimizer</a>
from Gacon et al. <a class="footnote-reference brackets" href="#gacon2021" id="id1">1</a>  using PennyLane.</p>
<p>Variational quantum algorithms (VQAs) are in close analogy to their counterparts
in classical machine learning.  They both build a closed optimization loop and utilize an
optimizer to iterate on the parameters. However, out-of-the-box classical gradient-based
optimizers, such as gradient descent, are often unsatisfying for VQAs, as quantum measurements
are notoriously expensive and gradient measurements for quantum circuits scale poorly
with the system size.</p>
<p>In <a class="footnote-reference brackets" href="#gacon2021" id="id2">1</a>, Gacon et al. propose QN-SPSA, which is tailored for quantum
algorithms. In each optimization step, QN-SPSA executes only 2 quantum circuits to
estimate the gradient, and another 4 for the Fubini-Study metric tensor, independent of the
problem size. This preferred scaling makes it a promising candidate for optimization tasks
for noisy intermediate-scale quantum (NISQ) devices.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In quantum machine learning (QML) and variational quantum algorithms
(VQA), an optimizer does the following two tasks:</p>
<ul class="simple">
<li><p>It estimates the gradient of the cost function or other relevant
metrics at the current step.</p></li>
<li><p>Based on the metrics, it decides the parameters for the next iteration
to reduce the cost.</p></li>
</ul>
<p>A simple example of such an optimizer is the vanilla gradient descent
(GD), whose update rule is written as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(t + 1)} = \mathbf{x}^{(t)} - \eta \nabla f(\mathbf{x}^{(t)}) \label{eq:vanilla}\tag{1},\]</div>
<p>where <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> is the loss function with input parameter
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, while <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate. The superscript
<span class="math notranslate nohighlight">\(t\)</span> stands for the <span class="math notranslate nohighlight">\(t\)</span>-th iteration step in the optimization.
Here the gradient <span class="math notranslate nohighlight">\(\nabla f\)</span> is estimated dimension by dimension,
requiring <span class="math notranslate nohighlight">\(O(d)\)</span> quantum measurements (<span class="math notranslate nohighlight">\(d\)</span> being the
dimension of the parameter space). As quantum measurements are
expensive, this scaling makes GD impractical for complicated high-dimensional
circuits.</p>
<p>To address this unsatisfying scaling, the <a class="reference internal" href="tutorial_spsa.html"><span class="doc">simultaneous perturbation
stochastic approximation (SPSA) optimizer</span></a>
replaces this dimensionwise gradient estimation with a stochastic one <a class="footnote-reference brackets" href="#spsa" id="id3">2</a>.
In SPSA, a random direction <span class="math notranslate nohighlight">\(\mathbf{h} \in \mathcal{U}(\{-1, 1\}^d)\)</span>
in the parameter space is sampled, where <span class="math notranslate nohighlight">\(\mathcal{U}(\{-1, 1\}^d)\)</span> is a
<span class="math notranslate nohighlight">\(d\)</span>-dimensional discrete uniform distribution.  The gradient
component along this sampled direction is then measured with a finite difference
approach, with a perturbation step size <span class="math notranslate nohighlight">\(\epsilon\)</span>:</p>
<div class="math notranslate nohighlight">
\[  |{\nabla}_{\mathbf{h}}f(\mathbf{x})| \equiv
\mathbf{h}\cdot {\nabla}f(\mathbf{x}) \simeq \frac{1}{2\epsilon}\big(f(\mathbf{x} + \epsilon \mathbf{h}) - f(\mathbf{x} - \epsilon \mathbf{h})\big)\label{eq:finite_diff}\tag{2}.\]</div>
<p>A stochastic gradient estimator
<span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\nabla}}f(\mathbf{x}, \mathbf{h})_{SPSA}\)</span> is
then constructed:</p>
<div class="math notranslate nohighlight">
\[\widehat{\nabla f}(\mathbf{x}, \mathbf{h})_{SPSA} = | {\nabla}_{\mathbf{h}}f(\mathbf{x})|\mathbf{h}\label{eq:spsaGrad}\tag{3}.\]</div>
<p>With the estimator, SPSA gives the following update rule:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(t + 1)} = \mathbf{x}^{(t)} - \eta \widehat{\nabla f}(\mathbf{x}^{(t)}, \mathbf{h}^{(t)})_{SPSA} \label{eq:spsa}\tag{4},\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{h}^{(t)}\)</span> is sampled at each step. Although this
stochastic approach cannot provide a stepwise unbiased gradient
estimation, SPSA is proved to be especially effective when accumulated
over multiple optimization steps.</p>
<p>On the other hand, <a class="reference internal" href="tutorial_quantum_natural_gradient.html"><span class="doc">quantum natural gradient descent (QNG)</span></a> <a class="footnote-reference brackets" href="#stokes2020" id="id4">3</a>
is a variant of gradient descent. It introduces the Fubini-Study metric tensor
<span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span>  into the optimization to account for the
structure of the non-Euclidean parameter space <a class="footnote-reference brackets" href="#fs" id="id5">4</a>. The
<span class="math notranslate nohighlight">\(d \times d\)</span> metric tensor is defined as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{g}_{ij}(\mathbf{x}) = -\frac{1}{2} \frac{\partial}{\partial \mathbf{x}_i} \frac{\partial}{\partial \mathbf{x}_j} F(\mathbf{x}', \mathbf{x})\biggr\rvert_{\mathbf{x}'=\mathbf{x}},\label{eq:fsTensor}\tag{5}\]</div>
<p>where
<span class="math notranslate nohighlight">\(F(\mathbf{x}', \mathbf{x}) = \bigr\rvert\langle \phi(\mathbf{x}') | \phi(\mathbf{x}) \rangle \bigr\rvert ^ 2\)</span>,
and <span class="math notranslate nohighlight">\(\phi(\mathbf{x})\)</span> is the parameterized ansatz with input
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. With the metric tensor, the update rule is rewritten
as:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(t + 1)} = \mathbf{x}^{(t)} - \eta \boldsymbol{g}^{-1}(\mathbf{x}^{(t)}) \nabla f(\mathbf{x}^{(t)}) \label{eq:qn}\tag{6}.\]</div>
<p>While the introduction of the metric tensor helps to find better minima
and allows for faster convergence  <a class="footnote-reference brackets" href="#stokes2020" id="id6">3</a> <a class="footnote-reference brackets" href="#yamamoto2019" id="id7">5</a>,
the algorithm is not as scalable due to the number of measurements
required to estimate <span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span>.</p>
<p>QN-SPSA manages to combine the merits of QNG and SPSA by estimating
both the gradient and the metric tensor stochastically. The gradient is
estimated in the same fashion as the SPSA algorithm, while the
Fubini-Study metric is computed by a second-order process with another
two stochastic perturbations:</p>
<div class="math notranslate nohighlight">
\[\widehat{\boldsymbol{g}}(\mathbf{x}, \mathbf{h}_1, \mathbf{h}_2)_{SPSA} = \frac{\delta F }{8 \epsilon^2}\Big(\mathbf{h}_1 \mathbf{h}_2^\intercal + \mathbf{h}_2 \mathbf{h}_1^\intercal\Big) \label{eq:fs_qnspsa}\tag{7},\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\delta F = F(\mathbf{x, \mathbf{x} + \epsilon \mathbf{h}_1} + \epsilon \mathbf{h}_2) - F (\mathbf{x, \mathbf{x} + \epsilon \mathbf{h}_1}) - F(\mathbf{x, \mathbf{x} - \epsilon \mathbf{h}_1} + \epsilon \mathbf{h}_2) + F(\mathbf{x, \mathbf{x} - \epsilon \mathbf{h}_1})\label{eq:deltaf}\tag{8},\]</div>
<p>and <span class="math notranslate nohighlight">\(\mathbf{h}_1, \mathbf{h}_2 \in \mathcal{U}(\{-1, 1\}^d)\)</span> are
two randomly sampled directions.</p>
<p>With equation (7), QN-SPSA provides the update rule</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(t + 1)} = \mathbf{x}^{(t)} - \eta \widehat{\boldsymbol{g}}^{-1}(\mathbf{x}^{(t)}, \mathbf{h}_1^{(t)}, \mathbf{h}_2^{(t)})_{SPSA} \widehat{\nabla f}(\mathbf{x}^{(t)}, \mathbf{h}^{(t)})_{SPSA} \label{eq:qnspsa}\tag{9}.\]</div>
<p>In each optimization step <span class="math notranslate nohighlight">\(t\)</span>, one will need to randomly sample 3
perturbation directions
<span class="math notranslate nohighlight">\(\mathbf{h}^{(t)}, \mathbf{h}_1^{(t)}, \mathbf{h}_2^{(t)}\)</span>. Equation (9)
is then applied to compute the parameters for the <span class="math notranslate nohighlight">\((t + 1)\)</span>-th
step accordingly. This <span class="math notranslate nohighlight">\(O(1)\)</span> update rule fits into NISQ devices
well.</p>
</div>
<div class="section" id="numerical-stability">
<h2>Numerical stability<a class="headerlink" href="#numerical-stability" title="Permalink to this headline">¶</a></h2>
<p>The QN-SPSA update rule given in equation (9) is highly stochastic and may
not behave well numerically. In practice, a few tricks are applied to
ensure the method’s numerical stability <a class="footnote-reference brackets" href="#gacon2021" id="id8">1</a>:</p>
<dl>
<dt>Averaging on the Fubini-Study metric tensor</dt><dd><p>A running average is taken on the metric tensor estimated from equation (7)
at each step <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[\bar{\boldsymbol{g}}^{(t)}(\mathbf{x}) = \frac{1}{t + 1} \Big(\sum_{i=1}^{t}\widehat{\boldsymbol{g}}(\mathbf{x}, \mathbf{h}_1^{(i)}, \mathbf{h}_2^{(i)})_{SPSA} + \boldsymbol{g}^{(0)}\Big)\label{eq:tensorRunningAvg}\tag{10} ,\]</div>
<p>where the initial guess <span class="math notranslate nohighlight">\(\boldsymbol{g}^{(0)}\)</span> is set to be the
identity matrix.</p>
</dd>
<dt>Fubini-Study metric tensor regularization</dt><dd><p>To ensure the positive semidefiniteness of the metric tensor near
a minimum, the running average in equation (10) is regularized:</p>
<div class="math notranslate nohighlight">
\[\bar{\boldsymbol{g}}^{(t)}_{reg}(\mathbf{x}) = \sqrt{\bar{\boldsymbol{g}}^{(t)}(\mathbf{x}) \bar{\boldsymbol{g}}^{(t)}(\mathbf{x})} + \beta \mathbb{I}\label{eq:tensor_reg}\tag{11},\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is the regularization coefficient. We can consider <span class="math notranslate nohighlight">\(\beta\)</span>
as a hyperparameter and choose a suitable value by trial and error. If
<span class="math notranslate nohighlight">\(\beta\)</span> is too small, it cannot protect the positive semidefiniteness
of <span class="math notranslate nohighlight">\(\bar{\boldsymbol{g}}_{reg}\)</span>. If <span class="math notranslate nohighlight">\(\beta\)</span> is too large, it will wipe out
the information from the Fubini-Study metric tensor, reducing QN-SPSA to the first
order SPSA.</p>
<p>With equation (11), the QN-SPSA update rule we implement in code reads</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{(t + 1)} = \mathbf{x}^{(t)} - \eta (\bar{\boldsymbol{g}}^{(t)}_{reg})^{-1}(\mathbf{x}^{(t)}) \widehat{\nabla f}(\mathbf{x}^{(t)}, \mathbf{h}^{(t)})_{SPSA} \label{eq:qnspsa_reg}\tag{12}.\]</div>
</dd>
<dt>Blocking condition on the parameter update</dt><dd><p>A blocking condition is applied onto the parameter update. The optimizer
only accepts updates that lead to a loss value no larger than the one
before update, plus a tolerance. Reference <a class="footnote-reference brackets" href="#spall1997" id="id9">6</a> suggests choosing a tolerance
that is twice the standard deviation of the loss.</p>
</dd>
</dl>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>We are now going to implement the QN-SPSA optimizer with all the tricks for numerical stability
included, and test it with a toy optimization problem.</p>
<p>Let’s first set up the toy example to optimize. We use a <a class="reference external" href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.qaoa.cost.maxcut.html">QAOA max cut
problem</a>  as our testing ground.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize a graph for the max cut problem</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">from</span> <span class="nn">pennylane</span> <span class="kn">import</span> <span class="n">qaoa</span>

<span class="n">nodes</span> <span class="o">=</span> <span class="n">n_qubits</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">edges</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">121</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">gnm_random_graph</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
<span class="n">cost_h</span><span class="p">,</span> <span class="n">mixer_h</span> <span class="o">=</span> <span class="n">qaoa</span><span class="o">.</span><span class="n">maxcut</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="n">depth</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># define device to be the PennyLane lightning local simulator</span>
<span class="n">dev</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.device.html#pennylane.device" title="pennylane.device" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">device</span></a><span class="p">(</span><span class="s2">&quot;lightning.qubit&quot;</span><span class="p">,</span> <span class="n">wires</span><span class="o">=</span><span class="n">n_qubits</span><span class="p">,</span> <span class="n">shots</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">qaoa_layer</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="n">qaoa</span><span class="o">.</span><span class="n">cost_layer</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">cost_h</span><span class="p">)</span>
    <span class="n">qaoa</span><span class="o">.</span><span class="n">mixer_layer</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">mixer_h</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">qaoa_circuit</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">n_qubits</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
    <span class="c1"># initialize all qubits into +X eigenstate.</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_qubits</span><span class="p">):</span>
        <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.Hadamard.html#pennylane.Hadamard" title="pennylane.Hadamard" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">Hadamard</span></a><span class="p">(</span><span class="n">wires</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
    <span class="n">gammas</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># stack building blocks for depth times.</span>
    <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.layer.html#pennylane.layer" title="pennylane.layer" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">layer</span></a><span class="p">(</span><span class="n">qaoa_layer</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">gammas</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>


<span class="c1"># define ansatz and loss function</span>
<span class="nd">@qml</span><span class="o">.</span><span class="n">qnode</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">qaoa_circuit</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">n_qubits</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.expval.html#pennylane.expval" title="pennylane.expval" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">expval</span></a><span class="p">(</span><span class="n">cost_h</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s confirm this circuit works. We generate a set of random parameters
as input, and check if the QNode <code class="docutils literal notranslate"><span class="pre">cost_function</span></code> for the circuit can be
executed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pennylane</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># initialize a random parameter tensor with shape (2, depth), scaled</span>
<span class="c1"># to [-pi, pi)</span>
<span class="n">params_curr</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input parameter shape:&quot;</span><span class="p">,</span> <span class="n">params_curr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss value:&quot;</span><span class="p">,</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_curr</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Input parameter shape: (2, 2)
Loss value: -1.5099999999999998
</pre></div>
</div>
<p>With the problem set up, we will for now focus on implementing a
single-step update of the QN-SPSA. Given the current parameters
<code class="docutils literal notranslate"><span class="pre">params_curr</span></code>, we would like to compute the parameters for the next
step <code class="docutils literal notranslate"><span class="pre">params_next</span></code>. We first define a few necessary
hyperparameters and global variables.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># step index</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># random seed for sampling the perturbation directions</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># perturbation size for the finite difference calculation</span>
<span class="n">finite_diff_step</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># regularization coefficient for the metric tensor</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="mf">1e-3</span>

<span class="c1"># learning rate</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># initialize the metric tensor to be an identity matrix</span>
<span class="n">params_number</span> <span class="o">=</span> <span class="n">params_curr</span><span class="o">.</span><span class="n">size</span>
<span class="n">metric_tensor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">params_number</span><span class="p">)</span>
</pre></div>
</div>
<p>As both the gradient estimator and the metric tensor estimator involve
getting random perturbation directions, we first implement a sampling
function that we call <code class="docutils literal notranslate"><span class="pre">get_perturbation_direction</span></code>. The function takes
the input parameter to the circuit ansatz, and returns a direction tensor
of the same shape. The direction tensor is sampled from a discrete uniform
distribution <span class="math notranslate nohighlight">\(\mathcal{U}(\{-1, 1\}^d)\)</span> using <code class="docutils literal notranslate"><span class="pre">random.choices</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_perturbation_direction</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">param_number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">params</span><span class="o">.</span><span class="n">size</span>
    <span class="n">sample_list</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="n">param_number</span><span class="p">)</span>
    <span class="n">direction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">direction</span>


<span class="nb">print</span><span class="p">(</span><span class="n">get_perturbation_direction</span><span class="p">(</span><span class="n">params_curr</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[-1  1]
[ 1 -1]]
</pre></div>
</div>
<p>With this function at our disposal, we implement the gradient estimator <code class="docutils literal notranslate"><span class="pre">get_grad</span></code>
following equation (2):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_grad</span><span class="p">(</span><span class="n">params_curr</span><span class="p">):</span>
    <span class="n">grad_dir</span> <span class="o">=</span> <span class="n">get_perturbation_direction</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>
    <span class="c1"># apply the perturbation</span>
    <span class="n">params_forward</span> <span class="o">=</span> <span class="n">params_curr</span> <span class="o">+</span> <span class="n">finite_diff_step</span> <span class="o">*</span> <span class="n">grad_dir</span>
    <span class="n">params_backward</span> <span class="o">=</span> <span class="n">params_curr</span> <span class="o">-</span> <span class="n">finite_diff_step</span> <span class="o">*</span> <span class="n">grad_dir</span>
    <span class="c1"># measured stochastic gradient</span>
    <span class="n">loss_forward</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_forward</span><span class="p">)</span>
    <span class="n">loss_backward</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_backward</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_forward</span> <span class="o">-</span> <span class="n">loss_backward</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">finite_diff_step</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad_dir</span>
    <span class="k">return</span> <span class="n">grad</span>


<span class="n">grad</span> <span class="o">=</span> <span class="n">get_grad</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Estimated SPSA gradient:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Estimated SPSA gradient:
[[-3.05 -3.05]
[ 3.05  3.05]]
</pre></div>
</div>
<p>To estimate the raw stochastic metric tensor
<span class="math notranslate nohighlight">\(\widehat{\boldsymbol{g}}(\mathbf{x}, \mathbf{h}_1, \mathbf{h}_2)_{SPSA}\)</span>
from equation (7), we will first need to measure the state overlap
<span class="math notranslate nohighlight">\(F(\mathbf{x}_1, \mathbf{x}_2) = \bigr\rvert\langle \phi(\mathbf{x}_1) | \phi(\mathbf{x}_2) \rangle \bigr\rvert ^ 2\)</span>.
We denote the unitary transformation forming the ansatz with <span class="math notranslate nohighlight">\(U\)</span>;
that is,
<span class="math notranslate nohighlight">\(\rvert\phi(\mathbf{x})\rangle = U(\mathbf{x}) \rvert0\rangle\)</span>.
Applying the adjoint operation <span class="math notranslate nohighlight">\(U^{\dagger}(\mathbf{x}_2)\)</span> on to
the ansatz state <span class="math notranslate nohighlight">\(\rvert\phi(\mathbf{x}_1)\rangle\)</span> followed with a
measurement in the computational basis then does the trick. The state
overlap equals the probability of a <span class="math notranslate nohighlight">\(\rvert00...0\rangle\)</span>
measurement outcome. Note that this circuit measuring the state overlap
doubles the circuit depth of the ansatz, and therefore has longer
execution time and experiences more accumulated noise from the device.
The function <code class="docutils literal notranslate"><span class="pre">get_state_overlap</span></code> returns a state overlap value between
0 (minimum overlap) and 1 (perfect overlap).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">copy</span>


<span class="k">def</span> <span class="nf">get_operations</span><span class="p">(</span><span class="n">qnode</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">qnode</span><span class="o">.</span><span class="n">construct</span><span class="p">([</span><span class="n">params</span><span class="p">],</span> <span class="p">{})</span>
    <span class="k">return</span> <span class="n">qnode</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">operations</span>


<span class="k">def</span> <span class="nf">get_overlap_tape</span><span class="p">(</span><span class="n">qnode</span><span class="p">,</span> <span class="n">params1</span><span class="p">,</span> <span class="n">params2</span><span class="p">):</span>
    <span class="n">op_forward</span> <span class="o">=</span> <span class="n">get_operations</span><span class="p">(</span><span class="n">qnode</span><span class="p">,</span> <span class="n">params1</span><span class="p">)</span>
    <span class="n">op_inv</span> <span class="o">=</span> <span class="n">get_operations</span><span class="p">(</span><span class="n">qnode</span><span class="p">,</span> <span class="n">params2</span><span class="p">)</span>

    <span class="k">with</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.tape.QuantumTape.html#pennylane.tape.QuantumTape" title="pennylane.tape.QuantumTape" class="sphx-glr-backref-module-pennylane-tape sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">QuantumTape</span></a><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">op_forward</span><span class="p">:</span>
            <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.apply.html#pennylane.apply" title="pennylane.apply" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">apply</span></a><span class="p">(</span><span class="n">op</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">op_inv</span><span class="p">):</span>
            <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.adjoint.html#pennylane.adjoint" title="pennylane.adjoint" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">adjoint</span></a><span class="p">(</span><span class="n">copy</span><span class="p">(</span><span class="n">op</span><span class="p">))</span>
        <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.probs.html#pennylane.probs" title="pennylane.probs" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">probs</span></a><span class="p">(</span><span class="n">wires</span><span class="o">=</span><span class="n">qnode</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">wires</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tape</span>


<span class="k">def</span> <span class="nf">get_state_overlap</span><span class="p">(</span><span class="n">tape</span><span class="p">):</span>
    <span class="k">return</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.execute.html#pennylane.execute" title="pennylane.execute" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">execute</span></a><span class="p">([</span><span class="n">tape</span><span class="p">],</span> <span class="n">dev</span><span class="p">,</span> <span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s do a quick sanity check on the state overlap calculation. From the following
cell, we can see that the overlap of a state with itself is 1, while the number
for two states from random inputs can vary between 0 and 1. This means <code class="docutils literal notranslate"><span class="pre">get_state_overlap</span></code>
function works!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tape</span> <span class="o">=</span> <span class="n">get_overlap_tape</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Perfect overlap: &quot;</span><span class="p">,</span> <span class="n">get_state_overlap</span><span class="p">(</span><span class="n">tape</span><span class="p">))</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">get_overlap_tape</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Random state overlap: &quot;</span><span class="p">,</span> <span class="n">get_state_overlap</span><span class="p">(</span><span class="n">tape</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Perfect overlap:  1.0
Random state overlap:  0.599
</pre></div>
</div>
<p>Now that we have confirmed our implementation of the state overlap, we can
proceed to compute the raw stochastic metric tensor
<span class="math notranslate nohighlight">\(\widehat{\boldsymbol{g}}(\mathbf{x}, \mathbf{h}_1, \mathbf{h}_2)_{SPSA}\)</span>.
With the function <code class="docutils literal notranslate"><span class="pre">get_raw_tensor_metric</span></code>, we sample two perturbations with
<code class="docutils literal notranslate"><span class="pre">get_perturbation_direction</span></code> independently and estimate the raw metric
tensor with equations (8) and (7).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_raw_tensor_metric</span><span class="p">(</span><span class="n">params_curr</span><span class="p">):</span>

    <span class="n">dir1</span> <span class="o">=</span> <span class="n">get_perturbation_direction</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>
    <span class="n">dir2</span> <span class="o">=</span> <span class="n">get_perturbation_direction</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>
    <span class="n">perturb1</span> <span class="o">=</span> <span class="n">dir1</span> <span class="o">*</span> <span class="n">finite_diff_step</span>
    <span class="n">perturb2</span> <span class="o">=</span> <span class="n">dir2</span> <span class="o">*</span> <span class="n">finite_diff_step</span>
    <span class="n">dir_vec1</span> <span class="o">=</span> <span class="n">dir1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dir_vec2</span> <span class="o">=</span> <span class="n">dir2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">tapes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">get_overlap_tape</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="n">params_curr</span> <span class="o">+</span> <span class="n">perturb1</span> <span class="o">+</span> <span class="n">perturb2</span><span class="p">),</span>
        <span class="n">get_overlap_tape</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="n">params_curr</span> <span class="o">+</span> <span class="n">perturb1</span><span class="p">),</span>
        <span class="n">get_overlap_tape</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="n">params_curr</span> <span class="o">-</span> <span class="n">perturb1</span> <span class="o">+</span> <span class="n">perturb2</span><span class="p">),</span>
        <span class="n">get_overlap_tape</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="n">params_curr</span> <span class="o">-</span> <span class="n">perturb1</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">tensor_finite_diff</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">get_state_overlap</span><span class="p">(</span><span class="n">tapes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="o">-</span> <span class="n">get_state_overlap</span><span class="p">(</span><span class="n">tapes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="o">-</span> <span class="n">get_state_overlap</span><span class="p">(</span><span class="n">tapes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="o">+</span> <span class="n">get_state_overlap</span><span class="p">(</span><span class="n">tapes</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="p">)</span>

    <span class="n">metric_tensor_raw</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">dir_vec1</span><span class="p">,</span> <span class="n">dir_vec2</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">dir_vec2</span><span class="p">,</span> <span class="n">dir_vec1</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="o">*</span> <span class="n">tensor_finite_diff</span>
        <span class="o">/</span> <span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">finite_diff_step</span> <span class="o">*</span> <span class="n">finite_diff_step</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">metric_tensor_raw</span>


<span class="n">metric_tensor_raw</span> <span class="o">=</span> <span class="n">get_raw_tensor_metric</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Raw estimated metric tensor:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">metric_tensor_raw</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Raw estimated metric tensor:
[[ 2.5  0.  -2.5  2.5]
[ 0.  -2.5  0.   0. ]
[-2.5  0.   2.5 -2.5]
[ 2.5  0.  -2.5  2.5]]
</pre></div>
</div>
<p>Now, let’s apply the running average in equation (10), and the regularization in equation (11):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">sqrtm</span>

<span class="n">metric_tensor_avg</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">metric_tensor_raw</span> <span class="o">+</span> <span class="n">k</span> <span class="o">/</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">metric_tensor</span>
<span class="n">tensor_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">metric_tensor_avg</span><span class="p">,</span> <span class="n">metric_tensor_avg</span><span class="p">)))</span>
<span class="c1"># update metric tensor</span>
<span class="n">metric_tensor</span> <span class="o">=</span> <span class="p">((</span><span class="n">tensor_reg</span> <span class="o">+</span> <span class="n">regularization</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">metric_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span> <span class="o">/</span> <span class="p">(</span>
    <span class="mi">1</span> <span class="o">+</span> <span class="n">regularization</span>
<span class="p">)</span>
<span class="c1"># update step index</span>
<span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Updated metric tensor after the step:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">metric_tensor</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Updated metric tensor after the step:
[[ 1.74925075  0.         -1.24875125  1.24875125]
[ 0.          0.75024975  0.          0.        ]
[-1.24875125  0.          1.74925075 -1.24875125]
[ 1.24875125  0.         -1.24875125  1.74925075]]
</pre></div>
</div>
<p>Equation (12) requires computing the inverse of the metric tensor. A
numerically more stable approach is to solve the equivalent linear
equation for <span class="math notranslate nohighlight">\(\mathbf{x}^{(t + 1)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\bar{\boldsymbol{g}}^{(t)}_{reg}(\mathbf{x}^{(t)})\big( \mathbf{x}^{(t)} - \mathbf{x}^{(t + 1)}\big) =  \eta  \widehat{\nabla f}(\mathbf{x}^{(t)}, \mathbf{h}^{(t)})_{SPSA} \label{eq:lin_solver}\tag{13}.\]</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_next_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
    <span class="n">grad_vec</span><span class="p">,</span> <span class="n">params_vec</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">new_params_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span>
        <span class="n">metric_tensor</span><span class="p">,</span>
        <span class="p">(</span><span class="o">-</span><span class="n">lr</span> <span class="o">*</span> <span class="n">grad_vec</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">metric_tensor</span><span class="p">,</span> <span class="n">params_vec</span><span class="p">)),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_params_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="n">params_next</span> <span class="o">=</span> <span class="n">get_next_params</span><span class="p">(</span><span class="n">params_curr</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Next parameters:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">params_next</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Next parameters:
[[-1.03117138 -0.54824992]
[-2.03318839  0.80331292]]
</pre></div>
</div>
<p>Now, it is the time to apply the blocking condition. Let’s first try the
proposal in <a class="footnote-reference brackets" href="#spall1997" id="id10">6</a> to use twice the sample standard deviation of the loss
at the current step as the tolerance. To collect such a sample, we need to
repeat the QNode execution for the loss <code class="docutils literal notranslate"><span class="pre">cost_function</span></code> for, say, 10 times.
The straightforward implementation goes as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss_next</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_next</span><span class="p">)</span>

<span class="n">repeats</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">loss_curr_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">repeats</span><span class="p">):</span>
    <span class="n">loss_curr_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>

<span class="n">tol</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">loss_curr_list</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">loss_curr</span> <span class="o">=</span> <span class="n">loss_curr_list</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># block updates that lead to significant increase</span>
<span class="c1"># of the loss value</span>
<span class="k">if</span> <span class="n">loss_curr</span> <span class="o">+</span> <span class="n">tol</span> <span class="o">&lt;</span> <span class="n">loss_next</span><span class="p">:</span>
    <span class="n">params_next</span> <span class="o">=</span> <span class="n">params_curr</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Next parameters after blocking:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">params_next</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Next parameters after blocking:
[[-1.03117138 -0.54824992]
[-2.03318839  0.80331292]]
</pre></div>
</div>
<p>As quantum measurements are generally expensive, computing the tolerance
this way adds significant overhead to the QN-SPSA optimizer. To be
specific, in each step of the optimization, QN-SPSA only requires
executing 2 circuits for the gradient, and 4 for the metric tensor. Yet
in the approach above, there are an additional 10 (from the repeat number) + 1
circuits required to apply the blocking.</p>
<p>To address this issue, we propose to define the tolerance as the
standard deviation of the loss values of the past <span class="math notranslate nohighlight">\(N\)</span> steps
instead, where <span class="math notranslate nohighlight">\(N\)</span> is a hyperparameter we choose.
The intuition here is that when the optimizer is working in a
fast-descending regime, the blocking condition is unlikely to be
triggered, as new loss values are often smaller than the previous ones.
On the other hand, when the optimizer is working in a rather flat energy
landscape, losses from the past <span class="math notranslate nohighlight">\(N\)</span> steps could be very similar to
the current loss value. In this regime, the tolerance defined from both
approaches should be close.</p>
<p>The implementation of this new tolerance is shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># define number of steps to track</span>
<span class="n">history_length</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># track the past losses in an array</span>
<span class="n">last_n_steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">history_length</span><span class="p">)</span>


<span class="c1"># stepwise update</span>
<span class="n">loss_curr</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_curr</span><span class="p">)</span>
<span class="n">loss_next</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">params_next</span><span class="p">)</span>

<span class="c1"># k has been updated above</span>
<span class="n">ind</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="n">history_length</span>
<span class="n">last_n_steps</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_curr</span>

<span class="n">tol</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">last_n_steps</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="n">history_length</span> <span class="k">else</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">last_n_steps</span><span class="p">[:</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="k">if</span> <span class="n">loss_curr</span> <span class="o">+</span> <span class="n">tol</span> <span class="o">&lt;</span> <span class="n">loss_next</span><span class="p">:</span>
    <span class="n">params_next</span> <span class="o">=</span> <span class="n">params_curr</span>
</pre></div>
</div>
<p>The efficacy of this new tolerance definition is confirmed by
reproducing the experiment on QN-SPSA in Fig. 1(b) from reference <a class="footnote-reference brackets" href="#gacon2021" id="id11">1</a>. In the
following figure, we show the performance of the optimizer with the two
tolerance definitions for an 11-qubit system. The shaded areas are the
profiles of 25 trials of the experiment. One can confirm the
past-<span class="math notranslate nohighlight">\(N\)</span>-step (<span class="math notranslate nohighlight">\(N=5\)</span> for the plot) standard deviation works
just as well. With the new choice of the tolerance, for each step, the
QN-SPSA will only need to execute 2 (for gradient) + 4 (for metric tensor) +
2 (for the current and the next-step loss) = 8 circuits. In practice, we measure
a 50% reduction in the stepwise optimization time.</p>
<p>The test is done with <a class="reference external" href="https://docs.aws.amazon.com/braket/latest/developerguide/braket-jobs.html">Amazon Braket Hybrid Jobs</a>, as it is a handy tool to
scale up experiments systematically. We will show how to do that towards
the end of the tutorial.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/qnspsa_new_tol.png"><img alt="../_images/qnspsa_new_tol.png" src="../_images/qnspsa_new_tol.png" style="width: 80%;" /></a>
</div>
<p>Similarly, with Hybrid Jobs, we can confirm that blocking is
necessary for this second-order SPSA optimizer, though it does not make
much difference for SPSA. Here, the envelope of the QN-SPSA curves
without blocking is not plotted since it is too noisy to visualize. SPSA
is implemented by replacing the metric tensor with an identity
matrix.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/qnspsa_blocking.png"><img alt="../_images/qnspsa_blocking.png" src="../_images/qnspsa_blocking.png" style="width: 80%;" /></a>
</div>
</div>
<div class="section" id="efficiency-improvement">
<h2>Efficiency improvement<a class="headerlink" href="#efficiency-improvement" title="Permalink to this headline">¶</a></h2>
<p>Let’s do a deep dive on how to further improve the execution efficiency
of the code. In the code example above, we compute gradient, metric
tensor, and the loss values through individual calls on the
<code class="docutils literal notranslate"><span class="pre">QNode.__call__()</span></code> function (in this example, <code class="docutils literal notranslate"><span class="pre">cost_function()</span></code>). In
a handwavy argument, each <code class="docutils literal notranslate"><span class="pre">QNode.__call__()</span></code> does the following two
things: (1) it constructs a tape with the given parameters, and (2)
calls <code class="docutils literal notranslate"><span class="pre">qml.execute()</span></code> to execute the single tape.</p>
<p>However, in this use case, the better practice is to group the tapes and
call one <code class="docutils literal notranslate"><span class="pre">qml.execute()</span></code> on all the tapes. This practice utilizes the
batch execution feature from PennyLane, and has a few potential
advantages. Some simulators provide parallelization support, so that the
grouped tapes can be executed simutaneously. As an example, utilizing
the <a class="reference external" href="https://docs.aws.amazon.com/braket/latest/developerguide/braket-batching-tasks.html?tag=local002">task
batching</a>
feature from the Braket SV1 simulator, we are able to reduce the
optimization time by 75% for large circuits. For quantum hardware,
sending tapes in batches could also enable further efficiency
improvement in circuit compilation.</p>
<p>With this rewriting, the complete optimizer class is provided in the
following cell.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pennylane</span> <span class="k">as</span> <span class="nn">qml</span>
<span class="kn">from</span> <span class="nn">pennylane</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">sqrtm</span>
<span class="kn">import</span> <span class="nn">warnings</span>


<span class="k">class</span> <span class="nc">QNSPSA</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantum natural SPSA optimizer. Refer to https://quantum-journal.org/papers/q-2021-10-20-567/</span>
<span class="sd">    for a detailed description of the methodology. When disable_metric_tensor</span>
<span class="sd">    is set to be True, the metric tensor estimation is disabled, and QNSPSA is</span>
<span class="sd">    reduced to be a SPSA optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        stepsize (float): The learn rate.</span>
<span class="sd">        regularization (float): Regularitzation term to the Fubini-Study</span>
<span class="sd">            metric tensor for numerical stability.</span>
<span class="sd">        finite_diff_step (float): step size to compute the finite difference</span>
<span class="sd">            gradient and the Fubini-Study metric tensor.</span>
<span class="sd">        resamplings (int): The number of samples to average for each parameter</span>
<span class="sd">            update.</span>
<span class="sd">        blocking (boolean): When set to be True, the optimizer only accepts</span>
<span class="sd">            updates that lead to a loss value no larger than the loss value</span>
<span class="sd">            before update, plus a tolerance. The tolerance is set with the</span>
<span class="sd">            parameter history_length.</span>
<span class="sd">        history_length (int): When blocking is True, the tolerance is set to be</span>
<span class="sd">            the average of the cost values in the last history_length steps.</span>
<span class="sd">        disable_metric_tensor (boolean): When set to be True, the optimizer is</span>
<span class="sd">            reduced to be a (1st-order) SPSA optimizer.</span>
<span class="sd">        seed (int): Seed for the random sampling.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">stepsize</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">regularization</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">finite_diff_step</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
        <span class="n">resamplings</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">history_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">disable_metric_tensor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span> <span class="o">=</span> <span class="n">stepsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">=</span> <span class="n">regularization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span> <span class="o">=</span> <span class="n">finite_diff_step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resamplings</span> <span class="o">=</span> <span class="n">resamplings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocking</span> <span class="o">=</span> <span class="n">blocking</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_n_steps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">history_length</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history_length</span> <span class="o">=</span> <span class="n">history_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disable_metric_tensor</span> <span class="o">=</span> <span class="n">disable_metric_tensor</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update trainable arguments with one step of the optimizer.</span>

<span class="sd">        .. warning::</span>
<span class="sd">            When blocking is set to be True, use step_and_cost instead, as loss</span>
<span class="sd">            measurements are required for the updates for the case.</span>

<span class="sd">        Args:</span>
<span class="sd">            cost (qml.QNode): the QNode wrapper for the objective function for</span>
<span class="sd">            optimization</span>
<span class="sd">            params (np.array): Parameter before update.</span>

<span class="sd">        Returns:</span>
<span class="sd">            np.array: The new variable values after step-wise update.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocking</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;step_and_cost() instead of step() is called when &quot;</span>
                <span class="s2">&quot;blocking is turned on, as the step-wise loss value &quot;</span>
                <span class="s2">&quot;is required by the algorithm.&quot;</span><span class="p">,</span>
                <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_and_cost</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_metric_tensor</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__step_core_first_order</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__step_core</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step_and_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update trainable parameters with one step of the optimizer and return</span>
<span class="sd">        the corresponding objective function value after the step.</span>

<span class="sd">        Args:</span>
<span class="sd">            cost (qml.QNode): the QNode wrapper for the objective function for</span>
<span class="sd">                optimization</span>
<span class="sd">            params (np.array): Parameter before update.</span>

<span class="sd">        Returns:</span>
<span class="sd">            tuple[np.array, float]: the updated parameter and the objective</span>
<span class="sd">                function output before the step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params_next</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__step_core_first_order</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_metric_tensor</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">__step_core</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocking</span><span class="p">:</span>
            <span class="n">loss_curr</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">params_next</span><span class="p">,</span> <span class="n">loss_curr</span>
        <span class="n">params_next</span><span class="p">,</span> <span class="n">loss_curr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__apply_blocking</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">params_next</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">params_next</span><span class="p">,</span> <span class="n">loss_curr</span>

    <span class="k">def</span> <span class="nf">__step_core</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># Core function that returns the next parameters before applying blocking.</span>
        <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">tensor_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">params</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">resamplings</span><span class="p">):</span>
            <span class="n">grad_tapes</span><span class="p">,</span> <span class="n">grad_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_spsa_grad_tapes</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
            <span class="n">metric_tapes</span><span class="p">,</span> <span class="n">tensor_dirs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_tensor_tapes</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
            <span class="n">raw_results</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.execute.html#pennylane.execute" title="pennylane.execute" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">execute</span></a><span class="p">(</span><span class="n">grad_tapes</span> <span class="o">+</span> <span class="n">metric_tapes</span><span class="p">,</span> <span class="n">cost</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__post_process_grad</span><span class="p">(</span><span class="n">raw_results</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">grad_dir</span><span class="p">)</span>
            <span class="n">metric_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__post_process_tensor</span><span class="p">(</span><span class="n">raw_results</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">tensor_dirs</span><span class="p">)</span>
            <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">grad_avg</span> <span class="o">*</span> <span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">grad</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">tensor_avg</span> <span class="o">=</span> <span class="n">tensor_avg</span> <span class="o">*</span> <span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">metric_tensor</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">__update_tensor</span><span class="p">(</span><span class="n">tensor_avg</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_next_params</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grad_avg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__step_core_first_order</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># Reduced core function that returns the next parameters with SPSA rule.</span>
        <span class="c1"># Blocking not applied.</span>
        <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">resamplings</span><span class="p">):</span>
            <span class="n">grad_tapes</span><span class="p">,</span> <span class="n">grad_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_spsa_grad_tapes</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
            <span class="n">raw_results</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.execute.html#pennylane.execute" title="pennylane.execute" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">execute</span></a><span class="p">(</span><span class="n">grad_tapes</span><span class="p">,</span> <span class="n">cost</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__post_process_grad</span><span class="p">(</span><span class="n">raw_results</span><span class="p">,</span> <span class="n">grad_dir</span><span class="p">)</span>
            <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">grad_avg</span> <span class="o">*</span> <span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">grad</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">params</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span> <span class="o">*</span> <span class="n">grad_avg</span>

    <span class="k">def</span> <span class="nf">__post_process_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_raw_results</span><span class="p">,</span> <span class="n">grad_dir</span><span class="p">):</span>
        <span class="c1"># With the quantum measurement results from the 2 gradient tapes,</span>
        <span class="c1"># compute the estimated gradient. Returned gradient is a tensor</span>
        <span class="c1"># of the same shape with the input parameter tensor.</span>
        <span class="n">loss_forward</span><span class="p">,</span> <span class="n">loss_backward</span> <span class="o">=</span> <span class="n">grad_raw_results</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_forward</span> <span class="o">-</span> <span class="n">loss_backward</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad_dir</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">def</span> <span class="nf">__post_process_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_raw_results</span><span class="p">,</span> <span class="n">tensor_dirs</span><span class="p">):</span>
        <span class="c1"># With the quantum measurement results from the 4 metric tensor tapes,</span>
        <span class="c1"># compute the estimated raw metric tensor. Returned raw metric tensor</span>
        <span class="c1"># is a tensor of shape (d x d), d being the dimension of the input parameter</span>
        <span class="c1"># to the ansatz.</span>
        <span class="n">tensor_finite_diff</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tensor_raw_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">-</span> <span class="n">tensor_raw_results</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">-</span> <span class="n">tensor_raw_results</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">tensor_raw_results</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">metric_tensor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="o">-</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">tensor_dirs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tensor_dirs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">tensor_dirs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tensor_dirs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="o">*</span> <span class="n">tensor_finite_diff</span>
            <span class="o">/</span> <span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">metric_tensor</span>

    <span class="k">def</span> <span class="nf">__get_next_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
        <span class="n">grad_vec</span><span class="p">,</span> <span class="n">params_vec</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">new_params_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span><span class="p">,</span>
            <span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stepsize</span> <span class="o">*</span> <span class="n">grad_vec</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span><span class="p">,</span> <span class="n">params_vec</span><span class="p">)),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">new_params_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__get_perturbation_direction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="n">param_number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">params</span><span class="o">.</span><span class="n">size</span>
        <span class="n">sample_list</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="n">param_number</span><span class="p">)</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_list</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">direction</span>

    <span class="k">def</span> <span class="nf">__get_spsa_grad_tapes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># Returns the 2 tapes along with the sampled direction that will be</span>
        <span class="c1"># used to estimate the gradient per optimization step. The sampled</span>
        <span class="c1"># direction is of the shape of the input parameter.</span>
        <span class="n">direction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_perturbation_direction</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">cost</span><span class="o">.</span><span class="n">construct</span><span class="p">([</span><span class="n">params</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span> <span class="o">*</span> <span class="n">direction</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">tape_forward</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_operations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">cost</span><span class="o">.</span><span class="n">construct</span><span class="p">([</span><span class="n">params</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span> <span class="o">*</span> <span class="n">direction</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">tape_backward</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_operations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">tape_forward</span><span class="p">,</span> <span class="n">tape_backward</span><span class="p">],</span> <span class="n">direction</span>

    <span class="k">def</span> <span class="nf">__update_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_raw</span><span class="p">):</span>
        <span class="n">tensor_avg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_tensor_moving_avg</span><span class="p">(</span><span class="n">tensor_raw</span><span class="p">)</span>
        <span class="n">tensor_regularized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__regularize_tensor</span><span class="p">(</span><span class="n">tensor_avg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span> <span class="o">=</span> <span class="n">tensor_regularized</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">__get_tensor_tapes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># Returns the 4 tapes along with the 2 sampled directions that will be</span>
        <span class="c1"># used to estimate the raw metric tensor per optimization step. The sampled</span>
        <span class="c1"># directions are 1d vectors of the length of the input parameter dimension.</span>
        <span class="n">dir1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_perturbation_direction</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">dir2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_perturbation_direction</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">perturb1</span> <span class="o">=</span> <span class="n">dir1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span>
        <span class="n">perturb2</span> <span class="o">=</span> <span class="n">dir2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">finite_diff_step</span>
        <span class="n">dir_vecs</span> <span class="o">=</span> <span class="n">dir1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dir2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">tapes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__get_overlap_tape</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">params</span> <span class="o">+</span> <span class="n">perturb1</span> <span class="o">+</span> <span class="n">perturb2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__get_overlap_tape</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">params</span> <span class="o">+</span> <span class="n">perturb1</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__get_overlap_tape</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">params</span> <span class="o">-</span> <span class="n">perturb1</span> <span class="o">+</span> <span class="n">perturb2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__get_overlap_tape</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">params</span> <span class="o">-</span> <span class="n">perturb1</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">tapes</span><span class="p">,</span> <span class="n">dir_vecs</span>

    <span class="k">def</span> <span class="nf">__get_overlap_tape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params1</span><span class="p">,</span> <span class="n">params2</span><span class="p">):</span>
        <span class="n">op_forward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_operations</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params1</span><span class="p">)</span>
        <span class="n">op_inv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__get_operations</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">params2</span><span class="p">)</span>

        <span class="k">with</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.tape.QuantumTape.html#pennylane.tape.QuantumTape" title="pennylane.tape.QuantumTape" class="sphx-glr-backref-module-pennylane-tape sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">qml</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">QuantumTape</span></a><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="n">op_forward</span><span class="p">:</span>
                <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.apply.html#pennylane.apply" title="pennylane.apply" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">apply</span></a><span class="p">(</span><span class="n">op</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">op</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">op_inv</span><span class="p">):</span>
                <span class="n">op</span><span class="o">.</span><span class="n">adjoint</span><span class="p">()</span>
            <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.probs.html#pennylane.probs" title="pennylane.probs" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">probs</span></a><span class="p">(</span><span class="n">wires</span><span class="o">=</span><span class="n">cost</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">wires</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tape</span>

    <span class="k">def</span> <span class="nf">__get_operations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
        <span class="c1"># Given a QNode, returns the list of operations before the measurement.</span>
        <span class="n">cost</span><span class="o">.</span><span class="n">construct</span><span class="p">([</span><span class="n">params</span><span class="p">],</span> <span class="p">{})</span>
        <span class="k">return</span> <span class="n">cost</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">operations</span>

    <span class="k">def</span> <span class="nf">__get_tensor_moving_avg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric_tensor</span><span class="p">):</span>
        <span class="c1"># For numerical stability: averaging on the Fubini-Study metric tensor.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">metric_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric_tensor</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">metric_tensor</span>

    <span class="k">def</span> <span class="nf">__regularize_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric_tensor</span><span class="p">):</span>
        <span class="c1"># For numerical stability: Fubini-Study metric tensor regularization.</span>
        <span class="n">tensor_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">sqrtm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">metric_tensor</span><span class="p">,</span> <span class="n">metric_tensor</span><span class="p">)))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">tensor_reg</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">metric_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">reg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__apply_blocking</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">params_curr</span><span class="p">,</span> <span class="n">params_next</span><span class="p">):</span>
        <span class="c1"># For numerical stability: apply the blocking condition on the parameter update.</span>
        <span class="n">cost</span><span class="o">.</span><span class="n">construct</span><span class="p">([</span><span class="n">params_curr</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">tape_loss_curr</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_operations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">cost</span><span class="o">.</span><span class="n">construct</span><span class="p">([</span><span class="n">params_next</span><span class="p">],</span> <span class="p">{})</span>
        <span class="n">tape_loss_next</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="n">tape</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">copy_operations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">loss_curr</span><span class="p">,</span> <span class="n">loss_next</span> <span class="o">=</span> <a href="https://docs.pennylane.ai/en/stable/code/api/pennylane.execute.html#pennylane.execute" title="pennylane.execute" class="sphx-glr-backref-module-pennylane sphx-glr-backref-type-py-function"><span class="n">qml</span><span class="o">.</span><span class="n">execute</span></a><span class="p">([</span><span class="n">tape_loss_curr</span><span class="p">,</span> <span class="n">tape_loss_next</span><span class="p">],</span> <span class="n">cost</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="c1"># self.k has been updated earlier.</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">history_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_n_steps</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_curr</span>

        <span class="n">tol</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_n_steps</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">history_length</span>
            <span class="k">else</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_n_steps</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">loss_curr</span> <span class="o">+</span> <span class="n">tol</span> <span class="o">&lt;</span> <span class="n">loss_next</span><span class="p">:</span>
            <span class="n">params_next</span> <span class="o">=</span> <span class="n">params_curr</span>
        <span class="k">return</span> <span class="n">params_next</span><span class="p">,</span> <span class="n">loss_curr</span>
</pre></div>
</div>
<p>Let’s see how it performs on our QAOA example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">opt</span> <span class="o">=</span> <span class="n">QNSPSA</span><span class="p">(</span><span class="n">stepsize</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">)</span>
<span class="n">params_init</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">params_init</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">step_and_cost</span><span class="p">(</span><span class="n">cost_function</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">40</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: cost = </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Step 0: cost = -2.0730
Step 40: cost = -2.5390
Step 80: cost = -2.7240
Step 120: cost = -2.6760
Step 160: cost = -2.7540
Step 200: cost = -2.7060
Step 240: cost = -2.8110
Step 280: cost = -2.7930
</pre></div>
</div>
<p>The optimizer performs reasonably well: the loss drops over optimization
steps and converges finally. We then reproduce the benchmarking test
between the gradient descent, quantum natural gradient descent, SPSA and
QN-SPSA in Fig. 1(b) of reference <a class="footnote-reference brackets" href="#gacon2021" id="id12">1</a> with the following job. You
can find a more detailed version of the example in this
<a class="reference external" href="https://github.com/aws/amazon-braket-examples/blob/main/examples/hybrid_jobs/6_QNSPSA_optimizer_with_embedded_simulator/qnspsa_with_embedded_simulator.ipynb">notebook</a>,
with its dependencies in the <cite>source_scripts</cite>
<a class="reference external" href="https://github.com/aws/amazon-braket-examples/blob/main/examples/hybrid_jobs/6_QNSPSA_optimizer_with_embedded_simulator/source_scripts/">folder</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order for the remainder of this demo to work, you will need to have done 3 things:</p>
<ol class="arabic simple">
<li><p>Copied the <cite>source_scripts</cite> folder (linked above) to your working directory</p></li>
<li><p>Authenticated with AWS locally</p></li>
<li><p>Granted yourself the appropriate permissions as described in this <a class="reference external" href="https://docs.aws.amazon.com/braket/latest/developerguide/braket-enable-overview.html">AWS Braket setup document</a></p></li>
</ol>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsSession</span><span class="p">,</span> <span class="n">AwsQuantumJob</span>
<span class="kn">from</span> <span class="nn">braket.jobs.config</span> <span class="kn">import</span> <span class="n">InstanceConfig</span>
<span class="kn">from</span> <span class="nn">braket.jobs.image_uris</span> <span class="kn">import</span> <span class="n">Framework</span><span class="p">,</span> <span class="n">retrieve_image</span>
<span class="kn">import</span> <span class="nn">boto3</span>

<span class="n">region_name</span> <span class="o">=</span> <span class="n">AwsSession</span><span class="p">()</span><span class="o">.</span><span class="n">region</span>
<span class="n">image_uri</span> <span class="o">=</span> <span class="n">retrieve_image</span><span class="p">(</span><span class="n">Framework</span><span class="o">.</span><span class="n">BASE</span><span class="p">,</span> <span class="n">region_name</span><span class="p">)</span>

<span class="n">n_qubits</span> <span class="o">=</span> <span class="mi">11</span>

<span class="n">hyperparameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;n_qubits&quot;</span><span class="p">:</span> <span class="n">n_qubits</span><span class="p">,</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;shots&quot;</span><span class="p">:</span> <span class="mi">8192</span><span class="p">,</span>
    <span class="s2">&quot;max_iter&quot;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s2">&quot;learn_rate&quot;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span>
    <span class="s2">&quot;spsa_repeats&quot;</span><span class="p">:</span> <span class="mi">25</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">job_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ref-paper-benchmark-qubit-</span><span class="si">{</span><span class="n">n_qubits</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">instance_config</span> <span class="o">=</span> <span class="n">InstanceConfig</span><span class="p">(</span><span class="n">instanceType</span><span class="o">=</span><span class="s2">&quot;ml.m5.large&quot;</span><span class="p">,</span> <span class="n">volumeSizeInGb</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">instanceCount</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">job</span> <span class="o">=</span> <span class="n">AwsQuantumJob</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;local:pennylane/lightning.qubit&quot;</span><span class="p">,</span>
    <span class="n">source_module</span><span class="o">=</span><span class="s2">&quot;source_scripts&quot;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s2">&quot;source_scripts.benchmark_ref_paper_converge_speed&quot;</span><span class="p">,</span>
    <span class="n">job_name</span><span class="o">=</span><span class="n">job_name</span><span class="p">,</span>
    <span class="n">hyperparameters</span><span class="o">=</span><span class="n">hyperparameters</span><span class="p">,</span>
    <span class="n">instance_config</span><span class="o">=</span><span class="n">instance_config</span><span class="p">,</span>
    <span class="n">image_uri</span><span class="o">=</span><span class="n">image_uri</span><span class="p">,</span>
    <span class="n">wait_until_complete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Visualizing the job results, we get the following plot. The results are
well aligned with the observations from Gacon et al. <a class="footnote-reference brackets" href="#gacon2021" id="id13">1</a>. The
stepwise optimization times for GD, QNG, SPSA and QN-SPSA are measured to
be 0.43s, 0.75s, 0.03s and 0.20s. In this example, the average behavior of
SPSA matches the one from GD. QNG performs the best among the 4 candidates,
but it requires the most circuit executions and shots per step. In
particular, for QPUs, this is a severe disadvantage of this method.
From the more shot-frugal options, QN-SPSA demonstrates the fastest
convergence and best final accuracy, making it a promising candidate
for variational algorithms.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/qnspsa_braket.png"><img alt="../_images/qnspsa_braket.png" src="../_images/qnspsa_braket.png" style="width: 80%;" /></a>
</div>
<p>To sum up, in this tutorial, we showed step-by-step how we can implement
the QN-SPSA optimizer with PennyLane, along with a few tricks to further
improve the optimizer’s performance. We also demonstrated how one can
scale up the benchmarking experiments with Amazon Braket Hybrid Jobs.</p>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="gacon2021"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>,<a href="#id8">3</a>,<a href="#id11">4</a>,<a href="#id12">5</a>,<a href="#id13">6</a>)</span></dt>
<dd><p>Gacon, J., Zoufal, C., Carleo, G., &amp; Woerner, S. (2021).
<em>Simultaneous perturbation stochastic approximation of the quantum
fisher information</em>.
<a class="reference external" href="https://quantum-journal.org/papers/q-2021-10-20-567/">Quantum, 5, 567</a>.</p>
</dd>
<dt class="label" id="spsa"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Simultaneous perturbation stochastic approximation (2022).
Wikipedia.
<a class="reference external" href="https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation">https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation</a></p>
</dd>
<dt class="label" id="stokes2020"><span class="brackets">3</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Stokes, J., Izaac, J., Killoran, N., &amp; Carleo, G. (2020). <em>Quantum
natural gradient</em>. <a class="reference external" href="https://quantum-journal.org/papers/q-2020-05-25-269/">Quantum, 4, 269</a>.</p>
</dd>
<dt class="label" id="fs"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>Fubini–Study metric (2022). Wikipedia.
<a class="reference external" href="https://en.wikipedia.org/wiki/Fubini%E2%80%93Study_metric">https://en.wikipedia.org/wiki/Fubini%E2%80%93Study_metric</a></p>
</dd>
<dt class="label" id="yamamoto2019"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p>Yamamoto, N. (2019). <em>On the natural gradient for variational
quantum eigensolver</em>. <a class="reference external" href="https://arxiv.org/abs/1909.05074">arXiv preprint arXiv:1909.05074</a>.</p>
</dd>
<dt class="label" id="spall1997"><span class="brackets">6</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Spall, J. C. (1997). <em>Accelerated second-order stochastic
optimization using only function measurements</em>. <a class="reference external" href="https://ieeexplore.ieee.org/document/657661">In Proceedings of the
36th IEEE Conference on Decision and Control (Vol. 2, pp. 1417-1424).
IEEE</a>.</p>
</dd>
</dl>
</div>
<div class="section" id="about-the-author">
<h2>About the author<a class="headerlink" href="#about-the-author" title="Permalink to this headline">¶</a></h2>
<div class="bio" >
    <div class="photo" >
        <img class="photo__img" src="../_static/authors/yiheng_duan.jpeg" alt="Yiheng Duan" >
    </div>
    <div class="bio-text">
        <h4 class="bio-text__author-name">Yiheng Duan</h4>
        <p class="bio-text__author-description">Yiheng is an Applied Scientist at AWS Braket. Yiheng works at the intersection of quantum computing and machine learning and can be reached via yiheng@amazon.com.</p>
    </div>
</div><p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-demos-qnspsa-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/59ed829fd1a3f483666e3f2acbd5f534/qnspsa.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">qnspsa.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/979b3a91cf435ec9b9b5f7cc3cd166bf/qnspsa.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">qnspsa.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


    <script type="text/javascript">
        // This script ensures that the active navbar entry switches
        // from 'QML' to 'Demos' for any webpage within the demos/ directory,
        // or for any of the demonstration landing pages
        // (e.g., demos_optimization).
        var pagename = document.location.href.match(/[^\/]+$/)[0];
        var dir = document.URL.substr(0,document.URL.lastIndexOf('/')).match(/[^\/]+$/)[0];

        if (pagename.includes("demos") || pagename.includes("demonstrations") || dir.includes("demos")) {

            $(".nav-item.active").removeClass("active");
            var demos_link = $('.navbar-nav a').filter(function(index) { return $(this).text() === "Demos"; })[0]
            $(demos_link).parent().addClass("active");
        }
    </script>

              <div id="bottom-dl" class="xanadu-call-to-action-links">
                <div id="tutorial-type">demos/qnspsa</div>
                <div class="download-python-link">
                  <i class="fab fa-python"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Python script</div>
                </div>
                <div class="download-notebook-link">
                  <i class="fas fa-download"></i>&nbsp;
                  <div class="call-to-action-desktop-view">Download Notebook</div>
                </div>
                <div class="github-view-link">
                  <i class="fab fa-github"></i>&nbsp;
                  <div class="call-to-action-desktop-view">View on GitHub</div>
                </div>
              </div>

            </div>
            
          </div>
        
<div class="localtoc-container nano has-scrollbar">
  <div class="nano-content">
    <div id="localtoc">
        
          <h3>Contents</h3>
          <!-- Display the ToC for the current document if it is not empty. -->
          <ul class='current'>
<li class='current'><a class="reference internal" href="#">Quantum natural SPSA optimizer</a><ul class='current'>
<li class='current'><a class="reference internal" href="#introduction">Introduction</a></li>
<li class='current'><a class="reference internal" href="#numerical-stability">Numerical stability</a></li>
<li class='current'><a class="reference internal" href="#implementation">Implementation</a></li>
<li class='current'><a class="reference internal" href="#efficiency-improvement">Efficiency improvement</a></li>
<li class='current'><a class="reference internal" href="#references">References</a></li>
<li class='current'><a class="reference internal" href="#about-the-author">About the author</a></li>
</ul>
</li>
</ul>

        
    </div>

    <div class="xanadu-call-to-action-links">
        <h3>Downloads</h3>
        <div id="tutorial-type">demos/qnspsa</div>
        <div class="download-python-link">
            <i class="fab fa-python"></i>&nbsp;
            <div class="call-to-action-desktop-view">Download Python script</div>
        </div>
        <div class="download-notebook-link">
            <i class="fas fa-download"></i>&nbsp;
            <div class="call-to-action-desktop-view">Download Notebook</div>
        </div>
        <div class="github-view-link">
            <i class="fab fa-github"></i>&nbsp;
            <div class="call-to-action-desktop-view">View on GitHub</div>
        </div>
    </div>
    <div id="related-tutorials" class="mt-4">
      <h3> Related</h3>
    </div>
  </div>
</div>


    
          <div class="up-button">
            
              
                <a href="../demos_optimization.html"><i class="fas fa-angle-double-left"></i></a>
              
            
          </div>

          <div class="clearfix"></div>
        </div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="tutorial_diffable-mitigation.html" title="Differentiating quantum error mitigation transforms"
             >next</a> |</li>
        <li class="right" >
          <a href="tutorial_qaoa_maxcut.html" title="QAOA for MaxCut"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">PennyLane  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../quantum-computing.html" >Quantum Computing</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../demonstrations.html" >Demos</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="../demos_optimization.html" >Optimization</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Quantum natural SPSA optimizer</a></li> 
      </ul>
    </div>
  <script type="text/javascript">
    $("#mobile-toggle").click(function () {
      $("#left-column").slideToggle("slow");
    });
  </script>

  <!-- jQuery -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>
  <!-- MathJax -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- Bootstrap core JavaScript -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.3.1/js/bootstrap.min.js"></script>
  <!-- MDB core JavaScript -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.8.10/js/mdb.min.js"></script>
  <!-- NanoScroller -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery.nanoscroller/0.8.7/javascripts/jquery.nanoscroller.min.js"></script>
  <!-- Syntax Highlighting -->
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
  <script type="text/javascript">hljs.initHighlightingOnLoad();</script>

  <script type="text/javascript">
    $("a.reference.internal").each(function(){
      var link = $(this).attr("href");

      var hash = link.split("#")[1];
      var page = link.split("#")[0].split("/").slice(-1)[0].replace(".html", "");

      if (hash == page) {
        $(this).attr("href", link.split("#")[0]);
      }
    });

    $(".document > .section").removeClass("section");
    $("h1 ~ .section").removeClass("section");
    $(".localtoc-container .nano-content").css("height", $("#content").height());
    $(".localtoc-container").css("height", $("#content").height());
    $(".nano").nanoScroller();
  </script>

  <script type="text/javascript">
      $(window).scroll(function(){
        var scrollBottom = $(document).height() - $(window).height() - $(window).scrollTop();
        if (scrollBottom < 342) {
          $(".localtoc-container").css("height", "calc(100% - " + (342 - scrollBottom) + "px)");
          $(".localtoc-container .nano-content").css("height", "calc(100% - 119px)");
        }
      });
  </script>

  <script type="text/javascript">
    if ($(".current").length) {
      var target = $(".current")[0]
      var rect = target.getBoundingClientRect();
      if (rect.bottom > window.innerHeight) {
          $(".nano").nanoScroller({ scrollTo: $(".current") });
      } else {
          $(".nano").nanoScroller({ scrollTop: 0 });
      }
    }
    $(document).ready(function () {
        $(".css-transitions-only-after-page-load").each(function (index, element) {
            setTimeout(function () { $(element).removeClass("css-transitions-only-after-page-load") }, 10);
        });
        if (window.location.hash) {
          var target = $("[id='" + window.location.hash.substr(1) + "']");
          if (target.closest(".collapse").length) {
            target.closest(".collapse").addClass("show");
            target.closest(".collapse").prev().find(".rotate").addClass("up");
          }
        }
    });
  </script>

    <script type="text/javascript">
    var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
    if (downloadNote.length >= 1) {
      var tutorialUrlArray = $("#tutorial-type").text().split('/');

      if (tutorialUrlArray[0] == "demos") {
        tutorialUrlArray[0] = "demonstrations";
      }

      var githubLink = "https://github.com/" + "PennyLaneAI/qml" + "/blob/master/" + tutorialUrlArray.join("/") + ".py",
          pythonLink = $(".sphx-glr-download .reference.download")[0].href,
          notebookLink = $(".sphx-glr-download .reference.download")[1].href;

      $(".download-python-link").wrap("<a href=" + pythonLink + " data-behavior='call-to-action-event' data-response='Download Python script' download target='_blank'/>");
      $(".download-notebook-link").wrap("<a href=" + notebookLink + " data-behavior='call-to-action-event' data-response='Download Notebook' download target='_blank'/>");
      $(".github-view-link").wrap("<a href=" + githubLink + " data-behavior='call-to-action-event' data-response='View on Github' target='_blank'/>");
      $("#right-column").addClass("page-shadow");
    } else {
      $(".xanadu-call-to-action-links").hide();
      $("#bottom-dl").attr('style','display: none !important');
    }
    </script>

    <script type="text/javascript">
      function makeUL(urls, text) {
          var list = document.createElement('ul');

          for (var i = 0; i < urls.length; i++) {
              var item = document.createElement('li');
              var a = document.createElement('a');
              var linkText = document.createTextNode(text[i]);
              a.appendChild(linkText);
              a.href = urls[i];
              item.appendChild(a);
              list.appendChild(item);
          }
          return list;
      }

      if (typeof related_tutorials !== 'undefined') {
          document.getElementById('related-tutorials').appendChild(makeUL(related_tutorials, related_tutorials_titles));
          $("#related-tutorials ul li a").append(' <i class="fas fa-angle-double-right" style="font-size: smaller;"></i>')
          $("#related-tutorials").show();

    } else {
          $("#related-tutorials").hide();
    }
    </script>

  <!-- Account for MathJax when navigating to anchor tags. -->
  <script type="text/javascript">
    function scrollToElement(e) {
      // Scrolls to the given element, taking into account the navbar.
      MathJax.Hub.Queue(function() {
        // The following MUST be done asynchronously to take effect.
        setTimeout(function() {
          const navbar = document.querySelector("nav.navbar");
          const navbarHeight = navbar ? navbar.offsetHeight : 0;
          const scrollToY = e.offsetTop + e.offsetParent.offsetTop - navbarHeight;
          window.scrollTo(0, scrollToY);
        }, 0);
      });
    }

    function scrollToFragment(fragment) {
      // Scrolls to the position of the given URL fragment (which includes the "#").
      const elementID = fragment.replace(".", "\\.");
      if (elementID !== "") {
        const element = document.querySelector(elementID);
        if (element !== null) {
          scrollToElement(element);
        }
      }
    }

    $(document).ready(() => {
      scrollToFragment(window.location.hash);
      window.addEventListener("popstate", (_) => scrollToFragment(document.location.hash), false);
    });
  </script>

  <!-- Hide the rendering of :orphan: metadata. -->
  <script type="text/javascript">
    $(document).ready(() => {
      const elements = document.getElementsByClassName("field-odd");
      for (const element of elements) {
          if (element.innerHTML.trim() === "orphan") {
            element.style.display = "none";
          }
      }
    });
  </script>

  <script type="text/javascript">
    jQuery.noConflict(true);
  </script>

  

<footer class="page-footer text-md-left pt-4">

  <hr class="pb-0 mb-0">
  <div class="container-fluid">
    <div class="row justify-content-md-center">

      
      <!-- About -->
      <div class="col-md-4">
        <h5 class="mb-1 footer-heading">PennyLane</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <p>        PennyLane is an open-source software framework for quantum
        machine learning, quantum chemistry, and quantum computing, 
        with the ability to run on all hardware.
        Maintained with ❤️ by Xanadu.
        </p>
      </div>
      

      <!-- Links -->
      
      <div class="col-md-2 col-4">
        <h5 class="mb-1 footer-heading">PennyLane</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <ul class="list-unstyled">
          
          <li><a href="https://pennylane.ai/">Home</a></li>
          
          <li><a href="https://pennylane.ai/qml">Learn</a></li>
          
          <li><a href="https://pennylane.ai/qml/demonstrations.html">Demonstrations</a></li>
          
          <li><a href="https://docs.pennylane.ai/">Documentation</a></li>
          
          <li><a href="https://github.com/PennyLaneAI/pennylane">GitHub</a></li>
          
          <li><a href="https://twitter.com/pennylaneai">Twitter</a></li>
          
          <li><a href="https://pennylane.ai/blog">Blog</a></li>
          
        </ul>
      </div>
      
      <div class="col-md-2 col-4">
        <h5 class="mb-1 footer-heading">Xanadu</h5>
        <hr width=100px class="d-inline-block mt-0 mb-1 accent-4">
        <ul class="list-unstyled">
          
          <li><a href="https://xanadu.ai/">Home</a></li>
          
          <li><a href="https://xanadu.ai/about/">About</a></li>
          
          <li><a href="https://xanadu.ai/photonics">Hardware</a></li>
          
          <li><a href="https://xanadu.ai/careers/">Careers</a></li>
          
          <li><a href="https://cloud.xanadu.ai">Cloud</a></li>
          
          <li><a href="https://discuss.pennylane.ai/">Forum</a></li>
          
          <li><a href="https://xanadu.ai/blog">Blog</a></li>
          
        </ul>
      </div>
      

    </div>
  </div>
  <hr>

  <!-- Social -->
  <div class="social-section text-center">
      <ul class="list-unstyled list-inline mb-0">
          
          <li class="list-inline-item"><a class="btn-git" href="https://twitter.com/PennyLaneAI"><i class="fab fa-twitter"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://github.com/PennyLaneAI/pennylane"><i class="fab fa-github"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://linkedin.com/company/xanaduai/"><i class="fab fa-linkedin-in"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://discuss.pennylane.ai"><i class="fab fa-discourse"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://xanadu-quantum.slack.com/join/shared_invite/zt-nkwn25v9-H4hituCb_PUj4idG0MhSug#/shared-invite/email"><i class="fab fa-slack"> </i></a></li>
          
          <li class="list-inline-item"><a class="btn-git" href="https://pennylane.ai/blog/"><i class="fas fa-rss"> </i></a></li>
          
      </ul>
      
        
          <a href="https://xanadu.us17.list-manage.com/subscribe?u=725f07a1d1a4337416c3129fd&id=294b062630" style="font-size: initial;">
            Stay updated with our newsletter
          </a>
        
      
  </div>

  <!-- Copyright -->
  <div class="footer-copyright py-3 mt-0 text-center">
      <div class="container-fluid">
            Copyright &copy; 2022, Xanadu Quantum Technologies, Inc.

        
          <br>
          TensorFlow, the TensorFlow logo, and any related marks are trademarks of Google Inc.
        
      </div>
  </div>
</footer>
  </body>
</html>